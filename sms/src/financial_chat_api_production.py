"""
Financial Chat API - Production Version
100% Aligned with Production Pipeline Requirements

Features:
âœ… IST â†” UTC timezone conversion
âœ… BSON Date handling (no string conversion)
âœ… debitAmount calculation in all pipelines
âœ… 9 exact fallback templates as specified
âœ… Versioned caching system
âœ… Parallel execution (4-6 concurrent queries)
âœ… GroundingContext and post-fact validation
âœ… Real schema: user_financial_transactions collection
âœ… Robust fallback hierarchy: LLM â†’ Template â†’ Emergency
"""

import os
import json
import logging
import asyncio
import hashlib
import time
import ssl
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Any, Optional

import pytz
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from pymongo import MongoClient
from dotenv import load_dotenv

# Disable SSL warnings for Atlas connections
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Load environment variables
load_dotenv()

# -----------------------------------------------------------------------------
# Configuration
# -----------------------------------------------------------------------------
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Timezone configuration
IST = pytz.timezone("Asia/Kolkata")
UTC = pytz.timezone("UTC")

# Cache configuration
cache_store = {}
CACHE_TTL_SUBQUERIES = 1800  # 30 minutes
CACHE_TTL_PIPELINES = 7200   # 2 hours

# -----------------------------------------------------------------------------
# FastAPI App
# -----------------------------------------------------------------------------
app = FastAPI(
    title="Financial Chat API - Production",
    description="Production-ready natural language interface for financial data queries",
    version="2.0.0",
)

# -----------------------------------------------------------------------------
# AI Model Interface
# -----------------------------------------------------------------------------
from .ai_model_interface import ai_interface
from .ai_model_config import ai_config

# Ensure we have the latest configuration
ai_interface._refresh_config()

ai_status = ai_interface.get_status()
groq_available = ai_status["available"]

def call_ai_model(messages, model=None, temperature=0.1, max_retries=1):
    # Always refresh config before AI calls to ensure latest priority
    ai_interface._refresh_config()
    return ai_interface.call_ai_model(messages, model, temperature, max_retries=max_retries)

# -----------------------------------------------------------------------------
# MongoDB Configuration - PRODUCTION SCHEMA with Atlas SSL Support
# -----------------------------------------------------------------------------
MONGODB_URI = os.getenv("MONGODB_URI", "mongodb+srv://divyamverma:geMnO2HtgXwOrLsW@cluster0.gzbouvi.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0")
MONGODB_DB = os.getenv("MONGODB_DB", "pluto_money")
MONGODB_COLLECTION = "user_financial_transactions"  # FIXED: Correct collection name

def create_mongodb_client(uri: str) -> MongoClient:
    """
    Create MongoDB client with comprehensive SSL configuration for Atlas
    Handles multiple SSL scenarios and fallbacks
    """
    logger.info(f"ðŸ”Œ Connecting to MongoDB: {uri[:50]}...")
    
    # Check if this is an Atlas connection
    is_atlas = uri.startswith("mongodb+srv://") or "mongodb.net" in uri
    
    if is_atlas:
        logger.info("ðŸ” Detected MongoDB Atlas - configuring SSL...")
        
        # Try multiple SSL configurations in order of preference
        ssl_configs = [
            {
                "name": "Standard Atlas SSL",
                "config": {
                    "tls": True,
                    "serverSelectionTimeoutMS": 30000,
                    "connectTimeoutMS": 20000,
                    "socketTimeoutMS": 20000,
                    "maxPoolSize": 50,
                    "retryWrites": True,
                    "w": "majority"
                }
            },
            {
                "name": "Allow Invalid Certificates",
                "config": {
                    "tls": True,
                    "tlsAllowInvalidCertificates": True,
                    "serverSelectionTimeoutMS": 30000,
                    "connectTimeoutMS": 20000,
                    "socketTimeoutMS": 20000,
                    "maxPoolSize": 50,
                    "retryWrites": True,
                    "w": "majority"
                }
            },
            {
                "name": "Insecure TLS Mode",
                "config": {
                    "tls": True,
                    "tlsInsecure": True,
                    "serverSelectionTimeoutMS": 30000,
                    "connectTimeoutMS": 20000,
                    "socketTimeoutMS": 20000,
                    "maxPoolSize": 50,
                    "retryWrites": True,
                    "w": "majority"
                }
            },
            {
                "name": "Disable OCSP Check",
                "config": {
                    "tls": True,
                    "tlsDisableOCSPEndpointCheck": True,
                    "serverSelectionTimeoutMS": 30000,
                    "connectTimeoutMS": 20000,
                    "socketTimeoutMS": 20000,
                    "retryWrites": True
                }
            },
            {
                "name": "Basic TLS Only",
                "config": {
                    "tls": True,
                    "serverSelectionTimeoutMS": 45000,
                    "connectTimeoutMS": 30000,
                    "socketTimeoutMS": 30000
                }
            }
        ]
        
        # Try each SSL configuration
        for ssl_config in ssl_configs:
            try:
                logger.info(f"ðŸ”„ Trying {ssl_config['name']}...")
                client = MongoClient(uri, **ssl_config['config'])
                
                # Test the connection with a simple ping
                client.admin.command('ping')
                logger.info(f"âœ… {ssl_config['name']} successful!")
                
                # Additional connection test
                db_names = client.list_database_names()
                logger.info(f"ðŸ“Š Connected to Atlas. Available databases: {len(db_names)}")
                
                return client
                
            except Exception as e:
                logger.warning(f"âš ï¸ {ssl_config['name']} failed: {str(e)[:100]}...")
                continue
        
        # If all Atlas configurations failed
        raise Exception("âŒ All MongoDB Atlas SSL configurations failed")
        
    else:
        # Local MongoDB connection (no SSL)
        logger.info("ðŸ”§ Configuring local MongoDB connection...")
        try:
            client = MongoClient(
                uri,
                serverSelectionTimeoutMS=5000,
                connectTimeoutMS=5000
            )
            
            # Test the connection
            client.admin.command('ping')
            db_names = client.list_database_names()
            logger.info(f"âœ… Local MongoDB connected. Databases: {len(db_names)}")
            
            return client
            
        except Exception as e:
            logger.error(f"âŒ Local MongoDB connection failed: {e}")
            raise e

# Initialize MongoDB connection
client = create_mongodb_client(MONGODB_URI)
db = client[MONGODB_DB]
financial_collection = db[MONGODB_COLLECTION]
logger.info(f"âœ… MongoDB connected to {MONGODB_DB}.{MONGODB_COLLECTION}")

# -----------------------------------------------------------------------------
# Request/Response Models
# -----------------------------------------------------------------------------
class ChatRequest(BaseModel):
    user_id: str
    query: str
    context: Optional[Dict] = None

class ChatResponse(BaseModel):
    user_id: str
    query: str
    response: str
    sub_queries: List[str]
    data_points: int
    processing_time: float
    timestamp: str
    grounding_context: Dict

# -----------------------------------------------------------------------------
# Production Financial Chat Analyzer
# -----------------------------------------------------------------------------
class ProductionFinancialAnalyzer:
    """
    Production-ready analyzer implementing 100% of pipeline requirements
    """

    def __init__(self):
        self.groq_available = groq_available
        self.db = db

    # -------------------------- Cache Management ------------------------------
    
    def _generate_cache_key(self, prefix: str, user_id: str, query_hash: str, version: str = "v2") -> str:
        """Generate versioned cache key to avoid legacy collisions"""
        return f"{user_id}_{prefix}_{query_hash}_IST_{version}"
    
    def _get_cache(self, key: str) -> Optional[Dict]:
        """Get from cache if not expired"""
        if key in cache_store:
            data, timestamp = cache_store[key]
            if time.time() - timestamp < CACHE_TTL_SUBQUERIES:
                logger.debug(f"Cache hit: {key}")
                return data
            else:
                del cache_store[key]
        return None
    
    def _set_cache(self, key: str, data: Dict, ttl: int = CACHE_TTL_SUBQUERIES):
        """Set cache with TTL"""
        cache_store[key] = (data, time.time())
        logger.debug(f"Cache set: {key}")
    
    def _hash_query(self, query: str, intent: Dict) -> str:
        """Create hash for query + intent for cache keys"""
        content = f"{query}_{json.dumps(intent, sort_keys=True)}"
        return hashlib.md5(content.encode()).hexdigest()[:12]

    # -------------------------- IST â†” UTC Timezone Conversion ----------------
    
    def _resolve_time_window_ist_to_utc(self, query: str, user_id: str = None, query_type: str = None) -> Dict[str, Any]:
        """
        ADAPTIVE: Intelligent time window resolver with financial intelligence
        
        Uses query type classification to determine optimal time windows:
        - Credit Assessment: 6-12 months comprehensive view
        - Behavioral Analysis: 3-6 months pattern recognition
        - Risk Profiling: 6 months volatility assessment
        - Trend Analysis: Full year or comparative periods
        - Period Analysis: Specific timeframe with context
        """
        query_lower = query.lower()
        
        # Use adaptive intelligence if query type is provided
        if query_type:
            logger.info(f"ðŸ•’ ADAPTIVE TIME WINDOW: Using {query_type} optimization")
        now_ist = datetime.now(IST)
        
        # INTELLIGENT: DYNAMIC pattern extraction (NO HARDCODING)
        specific_month_match = None
        
        # SMART MONTH MAPPING (dynamic, works for any year)
        month_names = {
            'january': 1, 'jan': 1,
            'february': 2, 'feb': 2,
            'march': 3, 'mar': 3,
            'april': 4, 'apr': 4,
            'may': 5,
            'june': 6, 'jun': 6,
            'july': 7, 'jul': 7,
            'august': 8, 'aug': 8,
            'september': 9, 'sep': 9, 'sept': 9,
            'october': 10, 'oct': 10,
            'november': 11, 'nov': 11,
            'december': 12, 'dec': 12
        }
        
        # DYNAMIC PATTERN MATCHING: Extract any "month year" combination
        import re
        
        # Pattern 1: "July 2024", "March 2023", "December 2026", etc.
        month_year_pattern = r'\b(' + '|'.join(month_names.keys()) + r')\s+(\d{4})\b'
        match = re.search(month_year_pattern, query_lower, re.IGNORECASE)
        
        if match:
            month_name = match.group(1).lower()
            year = int(match.group(2))
            month = month_names[month_name]
            specific_month_match = (year, month)
            logger.info(f"ðŸŽ¯ DYNAMIC PATTERN DETECTED: {month_name.title()} {year} â†’ ({year}, {month})")
        
        # Pattern 2: "2024 July", "2023 March" (year first)
        if not specific_month_match:
            year_month_pattern = r'\b(\d{4})\s+(' + '|'.join(month_names.keys()) + r')\b'
            match = re.search(year_month_pattern, query_lower, re.IGNORECASE)
            
            if match:
                year = int(match.group(1))
                month_name = match.group(2).lower()
                month = month_names[month_name]
                specific_month_match = (year, month)
                logger.info(f"ðŸŽ¯ DYNAMIC PATTERN DETECTED: {year} {month_name.title()} â†’ ({year}, {month})")
        
        # Pattern 3: "07/2024", "03/2023" (MM/YYYY format)
        if not specific_month_match:
            mm_yyyy_pattern = r'\b(\d{1,2})/(\d{4})\b'
            match = re.search(mm_yyyy_pattern, query_lower)
            
            if match:
                month = int(match.group(1))
                year = int(match.group(2))
                if 1 <= month <= 12:  # Valid month
                    specific_month_match = (year, month)
                    logger.info(f"ðŸŽ¯ DYNAMIC PATTERN DETECTED: {month:02d}/{year} â†’ ({year}, {month})")
        
        # PRIORITY 1: Respect specific period requests
        if specific_month_match:
            year, month = specific_month_match
            month_name = ["", "January", "February", "March", "April", "May", "June", 
                         "July", "August", "September", "October", "November", "December"][month]
            
            logger.info(f"ðŸ“… SPECIFIC PERIOD DETECTED: {month_name} {year} (user request takes priority)")
            
            # Calculate month boundaries
            start_ist = now_ist.replace(year=year, month=month, day=1, hour=0, minute=0, second=0, microsecond=0)
            
            # Calculate last day of month
            if month == 12:
                next_month_start = now_ist.replace(year=year+1, month=1, day=1, hour=0, minute=0, second=0, microsecond=0)
            else:
                next_month_start = now_ist.replace(year=year, month=month+1, day=1, hour=0, minute=0, second=0, microsecond=0)
            
            end_ist = next_month_start - timedelta(microseconds=1)
            
            # Add contextual note based on query type
            context_note = ""
            if query_type == "behavioral_analysis":
                context_note = " (with behavioral pattern focus)"
            elif query_type == "credit_assessment":
                context_note = " (NOTE: Credit assessment may need additional months for comprehensive analysis)"
            
            return {
                "start_utc": start_ist.astimezone(timezone.utc),
                "end_utc": end_ist.astimezone(timezone.utc),
                "label": f"{month_name} {year}{context_note}",
                "timezone": "Asia/Kolkata",
                "description": f"Specific period analysis for {month_name} {year} as requested by user"
            }
        
        # PRIORITY 2: ADAPTIVE TIME WINDOW SELECTION for non-specific queries
        elif query_type == "credit_assessment":
            # Credit assessment needs comprehensive view (6-12 months) - DYNAMIC YEAR
            logger.info("ðŸ’³ CREDIT ASSESSMENT: Using 12-month comprehensive window")
            
            # INTELLIGENT: Use current year or detect year from query context
            target_year = now_ist.year
            
            # Check if query mentions a specific year for context
            year_context_match = re.search(r'\b(20\d{2})\b', query_lower)
            if year_context_match:
                context_year = int(year_context_match.group(1))
                if 2020 <= context_year <= now_ist.year + 5:  # Reasonable year range
                    target_year = context_year
                    logger.info(f"ðŸŽ¯ CREDIT ASSESSMENT: Using year {target_year} from query context")
            
            start_ist = now_ist.replace(year=target_year, month=1, day=1, hour=0, minute=0, second=0, microsecond=0)
            end_ist = now_ist.replace(year=target_year, month=12, day=31, hour=23, minute=59, second=59, microsecond=999999)
            
            return {
                "start_utc": start_ist.astimezone(timezone.utc),
                "end_utc": end_ist.astimezone(timezone.utc),
                "label": f"Complete Year {target_year} (Credit Assessment)",
                "timezone": "Asia/Kolkata",
                "description": f"12-month comprehensive view for creditworthiness analysis ({target_year})"
            }
        
        elif query_type == "behavioral_analysis":
            # Behavioral analysis needs pattern recognition (3-6 months) - DYNAMIC YEAR
            logger.info("ðŸ§  BEHAVIORAL ANALYSIS: Using 6-month pattern recognition window")
            
            # INTELLIGENT: Use current year or detect year from query context
            target_year = now_ist.year
            
            # Check if query mentions a specific year for context
            year_context_match = re.search(r'\b(20\d{2})\b', query_lower)
            if year_context_match:
                context_year = int(year_context_match.group(1))
                if 2020 <= context_year <= now_ist.year + 5:  # Reasonable year range
                    target_year = context_year
                    logger.info(f"ðŸŽ¯ BEHAVIORAL ANALYSIS: Using year {target_year} from query context")
            
            # Dynamic 6-month window around target year
            start_month = max(1, now_ist.month - 3) if target_year == now_ist.year else 3
            end_month = min(12, start_month + 5)
            
            start_ist = now_ist.replace(year=target_year, month=start_month, day=1, hour=0, minute=0, second=0, microsecond=0)
            end_ist = now_ist.replace(year=target_year, month=end_month, day=31, hour=23, minute=59, second=59, microsecond=999999)
            
            start_month_name = ["", "January", "February", "March", "April", "May", "June", 
                               "July", "August", "September", "October", "November", "December"][start_month]
            end_month_name = ["", "January", "February", "March", "April", "May", "June", 
                             "July", "August", "September", "October", "November", "December"][end_month]
            
            return {
                "start_utc": start_ist.astimezone(timezone.utc),
                "end_utc": end_ist.astimezone(timezone.utc),
                "label": f"{start_month_name}-{end_month_name} {target_year} (Behavioral Pattern Recognition)",
                "timezone": "Asia/Kolkata",
                "description": f"6-month window for spending behavior analysis ({target_year})"
            }
        
        # Handle complete year requests (e.g., "complete year 2025", "year 2025", "salary trends over 2025")
        import re
        year_patterns = [r"complete year (\d{4})", r"entire year (\d{4})", r"full year (\d{4})", 
                        r"year (\d{4})", r"over.*(\d{4})", r"in (\d{4})", r"during (\d{4})"]
        
        for pattern in year_patterns:
            match = re.search(pattern, query_lower)
            if match:
                year = int(match.group(1))
                start_ist = IST.localize(datetime(year, 1, 1, 0, 0, 0))
                end_ist = IST.localize(datetime(year, 12, 31, 23, 59, 59))
                
                start_utc = start_ist.astimezone(UTC).astimezone(timezone.utc)
                end_utc = end_ist.astimezone(UTC).astimezone(timezone.utc)
                label = f"Complete year {year}"
                
                logger.info(f"ðŸ“… Year-based window: {label} ({start_utc.date()} to {end_utc.date()})")
                return {
                    'start_utc': start_utc,
                    'end_utc': end_utc,
                    'label': label,
                    'timezone': 'Asia/Kolkata'
                }
        
        if "last month" in query_lower:
            # Calculate last month in IST context
            if now_ist.month == 1:
                last_month_start = now_ist.replace(year=now_ist.year - 1, month=12, day=1, hour=0, minute=0, second=0, microsecond=0)
            else:
                last_month_start = now_ist.replace(month=now_ist.month - 1, day=1, hour=0, minute=0, second=0, microsecond=0)
            
            this_month_start = now_ist.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
            
            # Convert IST to UTC for MongoDB (timezone-aware)
            start_utc = last_month_start.astimezone(UTC).astimezone(timezone.utc)
            end_utc = this_month_start.astimezone(UTC).astimezone(timezone.utc)
            
            month_names = ["", "January", "February", "March", "April", "May", "June", 
                          "July", "August", "September", "October", "November", "December"]
            label = f"{month_names[last_month_start.month]} {last_month_start.year}"
            
        # Handle specific months in 2025 (ENHANCED with timezone-aware UTC)
        elif any(month in query_lower for month in ["january 2025", "february 2025", "march 2025", "april 2025", "may 2025", "june 2025", "july 2025", "august 2025", "september 2025", "october 2025", "november 2025", "december 2025"]):
            
            def ist_month_bounds(year: int, month: int):
                """Helper: Return timezone-aware UTC bounds for IST month"""
                start_ist = IST.localize(datetime(year, month, 1, 0, 0, 0))
                if month == 12:
                    end_ist = IST.localize(datetime(year + 1, 1, 1, 0, 0, 0))
                else:
                    end_ist = IST.localize(datetime(year, month + 1, 1, 0, 0, 0))
                # Return timezone-aware UTC datetimes
                return (
                    start_ist.astimezone(UTC).astimezone(timezone.utc),
                    end_ist.astimezone(UTC).astimezone(timezone.utc)
                )
            
            # Explicit month mapping
            months = {
                "january 2025": (2025, 1, "January 2025"), 
                "february 2025": (2025, 2, "February 2025"),
                "march 2025": (2025, 3, "March 2025"),
                "april 2025": (2025, 4, "April 2025"),
                "may 2025": (2025, 5, "May 2025"),
                "june 2025": (2025, 6, "June 2025"),
                "july 2025": (2025, 7, "July 2025"),
                "august 2025": (2025, 8, "August 2025"),
                "september 2025": (2025, 9, "September 2025"),
                "october 2025": (2025, 10, "October 2025"),
                "november 2025": (2025, 11, "November 2025"),
                "december 2025": (2025, 12, "December 2025")
            }
            
            # Find requested month
            for key, (year, month, label) in months.items():
                if key in query_lower:
                    start_utc, end_utc = ist_month_bounds(year, month)
                    break
            else:
                # Fallback to intelligent window with query type awareness
                return self._create_intelligent_time_window(query_lower, user_id, now_ist, query_type)
            
        # ENHANCED: Handle time-based queries intelligently
        elif any(period in query_lower for period in ["past year", "last year", "over the year", "yearly", "annual"]):
            # For yearly queries, use full year
            start_ist = now_ist - timedelta(days=365)
            start_utc = start_ist.astimezone(UTC).astimezone(timezone.utc)
            end_utc = now_ist.astimezone(UTC).astimezone(timezone.utc)
            label = "Past year"
            
        elif any(period in query_lower for period in ["past 6 months", "last 6 months", "six months", "half year"]):
            # For 6-month queries
            start_ist = now_ist - timedelta(days=180)
            start_utc = start_ist.astimezone(UTC).astimezone(timezone.utc)
            end_utc = now_ist.astimezone(UTC).astimezone(timezone.utc)
            label = "Past 6 months"
            
        elif any(period in query_lower for period in ["past 3 months", "last 3 months", "three months", "quarter"]):
            # For 3-month queries
            start_ist = now_ist - timedelta(days=90)
            start_utc = start_ist.astimezone(UTC).astimezone(timezone.utc)
            end_utc = now_ist.astimezone(UTC).astimezone(timezone.utc)
            label = "Past 3 months"
            
        else:
            # INTELLIGENT DEFAULT: Adapt to available data with query type awareness
            return self._create_intelligent_time_window(query_lower, user_id, now_ist, query_type)
        
        logger.info(f"Time window: {label} | UTC: {start_utc} to {end_utc}")
        
        return {
            'start_utc': start_utc,
            'end_utc': end_utc,
            'label': label,
            'timezone': 'Asia/Kolkata'
        }
    
    def _create_intelligent_time_window(self, query_lower: str, user_id: str, now_ist: datetime, query_type: str = None) -> Dict[str, Any]:
        """
        INTELLIGENT TIME WINDOW: Dynamically adapt to available data with QUERY-TYPE AWARENESS
        
        Strategy:
        1. Determine minimum required window based on query type
        2. Start from minimum requirement (not always 30 days)
        3. Expand if no data found
        4. Ensure LLM expectations align with actual data window
        """
        
        # Convert IST to UTC for queries
        end_utc = now_ist.astimezone(UTC).astimezone(timezone.utc)
        
        # ENHANCED: Query-type-aware minimum windows
        minimum_days = self._get_minimum_days_for_query_type(query_type, query_lower)
        
        # Define progressive time windows starting from minimum requirement
        base_windows = [
            (30, "Last 30 days"),
            (90, "Last 3 months"), 
            (180, "Last 6 months"),
            (365, "Past year"),
            (1825, "All available data")  # 5 years
        ]
        
        # Filter windows to only include those >= minimum requirement
        time_windows = [(days, label) for days, label in base_windows if days >= minimum_days]
        
        # If no windows meet minimum, add the minimum as first option
        if not time_windows or time_windows[0][0] > minimum_days:
            if minimum_days == 180:
                time_windows.insert(0, (180, "Last 6 months (required for risk profiling)"))
            elif minimum_days == 365:
                time_windows.insert(0, (365, "Past year (required for comprehensive analysis)"))
        
        logger.info(f"ðŸŽ¯ Query type '{query_type}' requires minimum {minimum_days} days")
        
        for days, label in time_windows:
            start_ist = now_ist - timedelta(days=days)
            start_utc = start_ist.astimezone(UTC).astimezone(timezone.utc)
            
            # Quick check if data exists in this window
            if user_id:
                try:
                    count = financial_collection.count_documents({
                        "user_id": user_id,
                        "transaction_date": {"$gte": start_utc, "$lt": end_utc}
                    }, limit=1)  # Just check if any data exists
                    
                    if count > 0:
                        logger.info(f"ðŸ“Š Intelligent window selected: {label} (found data)")
                        logger.info(f"Time window: {label} | UTC: {start_utc} to {end_utc}")
                        return {
                            'start_utc': start_utc,
                            'end_utc': end_utc,
                            'label': label,
                            'timezone': 'Asia/Kolkata'
                        }
                except Exception as e:
                    logger.warning(f"Error checking data availability: {e}")
        
        # Fallback: Use last 6 months regardless
        start_ist = now_ist - timedelta(days=180)
        start_utc = start_ist.astimezone(UTC).astimezone(timezone.utc)
        label = "Last 6 months (fallback)"
        
        logger.info(f"âš ï¸ Using fallback window: {label}")
        logger.info(f"Time window: {label} | UTC: {start_utc} to {end_utc}")
        
        return {
            'start_utc': start_utc,
            'end_utc': end_utc,
            'label': label,
            'timezone': 'Asia/Kolkata'
        }

    # -------------------------- Main Orchestrator ------------------------------

    async def analyze_query(self, user_id: str, query: str) -> Dict[str, Any]:
        """
        Main orchestrator implementing the complete production pipeline:
        1) Request validation & cache
        2) LLM#1 â†’ Intent & subqueries
        3) Time window resolver (IST â†’ UTC)
        4) LLM#2 â†’ MongoDB pipelines with fallbacks
        5) Validator â†’ Execute (parallel)
        6) Generate grounding context
        7) LLM#3 â†’ grounded response + post-fact check
        """
        start_time = datetime.now(timezone.utc)
        
        # ðŸŽ¯ STEP 1: Log initial request
        logger.info("="*80)
        logger.info("ðŸš€ STARTING FINANCIAL ANALYSIS PIPELINE")
        logger.info("="*80)
        logger.info(f"ðŸ‘¤ USER ID: {user_id}")
        logger.info(f"â“ USER QUERY: '{query}'")
        logger.info(f"â° START TIME: {start_time.isoformat()}")
        logger.info("="*80)
        
        try:
            # Step 1: Request validation & cache
            if not await self._validate_user_exists(user_id):
                logger.warning(f"User {user_id} not found, proceeding anyway")
            
            # Step 2: LLM#1 â†’ Intent & subqueries
            logger.info("ðŸ§  STEP 2: INTENT ANALYSIS & SUBQUERY GENERATION")
            logger.info("-" * 60)
            
            intent_analysis = await self._analyze_intent(query)
            logger.info(f"ðŸŽ¯ INTENT ANALYSIS: {json.dumps(intent_analysis, indent=2)}")
            
            query_hash = self._hash_query(query, intent_analysis)
            
            # Check cache for subqueries
            subq_cache_key = self._generate_cache_key("subqueries", user_id, query_hash)
            cached_subqueries = self._get_cache(subq_cache_key)
            
            if cached_subqueries:
                sub_queries = cached_subqueries["sub_queries"]
                logger.info(f"ðŸ“‹ Using cached subqueries for {user_id}")
            else:
                sub_queries = await self._generate_sub_queries(query, intent_analysis, user_id)
                self._set_cache(subq_cache_key, {"sub_queries": sub_queries}, CACHE_TTL_SUBQUERIES)
            
            logger.info(f"ðŸ“ GENERATED {len(sub_queries)} SUB-QUERIES:")
            for i, sq in enumerate(sub_queries, 1):
                logger.info(f"   {i:2d}. {sq}")
            logger.info("-" * 60)
            
            # Step 3: Time window resolver (IST â†’ UTC) with ADAPTIVE INTELLIGENCE
            logger.info("ðŸ•’ STEP 3: TIME WINDOW RESOLUTION")
            logger.info("-" * 60)
            
            # INTELLIGENT: Classify query type for adaptive time window selection
            query_type = self._classify_financial_query_type(query, intent_analysis)
            time_window = self._resolve_time_window_ist_to_utc(query, user_id, query_type)
            logger.info(f"ðŸ“… TIME WINDOW: {time_window['label']}")
            logger.info(f"ðŸŒ TIMEZONE: {time_window['timezone']}")
            logger.info(f"â° START UTC: {time_window['start_utc']}")
            logger.info(f"â° END UTC: {time_window['end_utc']}")
            logger.info("-" * 60)
            
            # Step 4: LLM#2 â†’ MongoDB pipelines with robust fallbacks
            logger.info("ðŸ”§ STEP 4: MONGODB PIPELINE GENERATION")
            logger.info("-" * 60)
            
            mongo_queries = await self._generate_mongo_queries_with_exact_fallbacks(
                user_id, sub_queries, time_window, intent_analysis
            )
            
            logger.info(f"âœ… GENERATED {len(mongo_queries)} MONGODB PIPELINES")
            logger.info("-" * 60)
            
            # Step 5: Validator â†’ Execute (parallel 4-6)
            logger.info("âš¡ STEP 5: PARALLEL MONGODB EXECUTION")
            logger.info("-" * 60)
            
            aggregated_data = await self._execute_parallel_with_validation(mongo_queries)
            
            # Log execution results
            metadata = aggregated_data.get("_metadata", {})
            logger.info(f"ðŸ“Š EXECUTION SUMMARY:")
            logger.info(f"   â€¢ Total Queries: {metadata.get('total_queries', 0)}")
            logger.info(f"   â€¢ Successful: {metadata.get('successful_queries', 0)}")
            logger.info(f"   â€¢ Success Rate: {metadata.get('success_rate', 0)*100:.1f}%")
            logger.info(f"   â€¢ Total Results: {metadata.get('total_results', 0)}")
            logger.info(f"   â€¢ System Health: {metadata.get('system_health', 'unknown')}")
            logger.info("-" * 60)
            
            # Step 6: Generate grounding context
            logger.info("ðŸŽ¯ STEP 6: GROUNDING CONTEXT GENERATION")
            logger.info("-" * 60)
            
            grounding_context = self._create_grounding_context(
                aggregated_data, time_window, intent_analysis
            )
            
            logger.info(f"ðŸ“‹ GROUNDING CONTEXT SUMMARY:")
            logger.info(f"   â€¢ Time Period: {grounding_context.get('time_period')}")
            logger.info(f"   â€¢ Total Spending: â‚¹{grounding_context.get('total_spending', 0):,.2f}")
            logger.info(f"   â€¢ Total Transactions: {grounding_context.get('total_transactions', 0)}")
            logger.info(f"   â€¢ Categories: {len(grounding_context.get('categories', []))}")
            logger.info(f"   â€¢ Merchants: {len(grounding_context.get('merchants', []))}")
            logger.info("-" * 60)
            
            # Step 7: LLM#3 â†’ grounded response + post-fact check
            logger.info("ðŸ’¬ STEP 7: INITIAL RESPONSE GENERATION")
            logger.info("-" * 60)
            
            initial_response = await self._generate_grounded_response(
                query, intent_analysis, aggregated_data, grounding_context
            )
            
            # Step 8: FINAL LLM REFINEMENT â†’ Ultra-precise, query-specific response
            logger.info("ðŸŽ¯ STEP 8: FINAL REFINEMENT & PRECISION TUNING")
            logger.info("-" * 60)
            
            response = await self._generate_final_refined_response(
                query, initial_response, grounding_context, sub_queries, intent_analysis
            )
            
            processing_time = (datetime.now(timezone.utc) - start_time).total_seconds()
            
            logger.info(f"âœ… FINAL RESPONSE GENERATED ({len(response)} characters)")
            logger.info(f"â±ï¸ TOTAL PROCESSING TIME: {processing_time:.2f} seconds")
            logger.info("="*80)
            logger.info("ðŸŽ‰ PIPELINE COMPLETED SUCCESSFULLY")
            logger.info("="*80)
            
            return {
                "response": response,
                "sub_queries": sub_queries,
                "data_points": len([k for k in aggregated_data.keys() if k != "_metadata"]),
                "processing_time": processing_time,
                "intent_analysis": intent_analysis,
                "grounding_context": grounding_context,
                "time_window": time_window
            }
            
        except Exception as e:
            logger.exception("Error in analyze_query")
            return {
                "response": f"I encountered an error analyzing your query: {str(e)}",
                "sub_queries": [],
                "data_points": 0,
                "processing_time": (datetime.now(timezone.utc) - start_time).total_seconds(),
                "error": str(e),
                "grounding_context": {}
            }

    # -------------------------- Validation ------------------------------
    
    async def _validate_user_exists(self, user_id: str) -> bool:
        """Validate user exists in database"""
        try:
            count = financial_collection.count_documents({"user_id": user_id}, limit=1)
            exists = count > 0
            logger.debug(f"User {user_id} exists: {exists}")
            return exists
        except Exception as e:
            logger.error(f"User validation failed: {e}")
            return True  # Fail open

    # -------------------------- Intent Analysis ------------------------------
    
    async def _analyze_intent(self, query: str) -> Dict[str, Any]:
        """Analyze query intent using LLM with fallback"""
        if not self.groq_available:
            return self._fallback_intent_analysis(query)

        try:
            messages = [
                {"role": "system", "content": "You are an infinitely intelligent financial psychologist and data scientist. Analyze ANY financial query with unlimited creativity and intelligence."},
                {"role": "user", "content": f"""
Analyze this financial query with infinite intelligence:

Query: "{query}"

Return JSON with:
- intent: (spending_analysis | income_analysis | pattern_detection | behavioral_analysis | psychological_profiling | predictive_analysis | emotional_spending | habit_analysis | risk_assessment | creditworthiness | lifestyle_analysis | time_pattern_analysis | merchant_relationship | financial_personality | milestone_prediction | comparison | general)
- categories: [] (any mentioned categories)
- time_period: (last_week | last_month | last_3_months | last_6_months | last_year | specific_date | all_time | future_prediction)
- analysis_type: (summary | trend | comparison | anomaly | breakdown | behavioral_psychology | predictive | emotional_pattern | habit_formation | risk_profiling | creditworthiness | lifestyle_inference | time_behavior | relationship_mapping | personality_analysis | milestone_tracking)
- complexity_level: (simple | moderate | advanced | creative | psychological | predictive)
- keywords: [] (important keywords and concepts)
- user_persona_focus: string (specific aspect to focus on)
- creative_angle: string (unique perspective or creative insight to explore)
- psychological_elements: [] (behavioral psychology aspects to analyze)
- predictive_elements: [] (future-oriented analysis needed)

Handle ANY financial query imaginatively - from basic spending to complex behavioral psychology, emotional patterns, lifestyle analysis, predictive modeling, and creative financial insights.

Only return JSON, no extra text.
                """}
            ]
            
            response = call_ai_model(messages)  # Use default model for current provider
            if response:
                data = self._extract_json_from_response(response)
                if data:
                    logger.debug(f"Intent analysis: {data}")
                    return data
            
            return self._fallback_intent_analysis(query)
            
        except Exception as e:
            logger.warning(f"Intent analysis failed: {e}")
            return self._fallback_intent_analysis(query)
    
    def _fallback_intent_analysis(self, query: str) -> Dict[str, Any]:
        """Fallback intent analysis using keyword matching"""
        q = query.lower()
        
        if any(word in q for word in ["spending", "expense", "spent", "cost", "major expenses"]):
            intent = "spending_analysis"
        elif any(word in q for word in ["income", "salary", "earned", "credit"]):
            intent = "income_analysis"
        elif any(word in q for word in ["pattern", "trend", "habit", "behavior"]):
            intent = "pattern_detection"
        elif any(word in q for word in ["compare", "vs", "versus", "difference"]):
            intent = "comparison"
        else:
            intent = "general"
            
        return {
            "intent": intent,
            "categories": [],
            "time_period": "last_month" if "last month" in q else "specific_date" if "july" in q or "august" in q else "all_time",
            "analysis_type": "breakdown" if "breakdown" in q or "categories" in q else "summary",
            "keywords": query.split(),
            "user_persona_focus": "spending_habits" if intent == "spending_analysis" else "income_patterns"
        }

    def _extract_json_from_response(self, response_text: str) -> Dict[str, Any]:
        """Extract JSON from LLM responses with MongoDB shell syntax cleaning"""
        if not response_text:
            return {}
        try:
            cleaned = response_text.strip()
            
            # Extract JSON block
            json_content = cleaned
            if "```json" in cleaned:
                start = cleaned.find("```json") + 7
                end = cleaned.find("```", start)
                if end != -1:
                    json_content = cleaned[start:end].strip()
            
            # Clean MongoDB shell syntax that isn't valid JSON
            json_content = self._clean_mongodb_shell_syntax(json_content)
            
            return json.loads(json_content)
        except Exception as e:
            logger.warning(f"JSON extraction failed: {e}")
            return {}
    
    def _clean_mongodb_shell_syntax(self, json_str: str) -> str:
        """Clean MongoDB shell syntax to make valid JSON"""
        import re
        
        # Replace ISODate("...") with "..."
        json_str = re.sub(r'ISODate\("([^"]+)"\)', r'"\1"', json_str)
        
        # Replace ObjectId("...") with "..."
        json_str = re.sub(r'ObjectId\("([^"]+)"\)', r'"\1"', json_str)
        
        # Replace NumberLong(...) with number
        json_str = re.sub(r'NumberLong\((\d+)\)', r'\1', json_str)
        
        # Replace NumberDecimal("...") with number
        json_str = re.sub(r'NumberDecimal\("([^"]+)"\)', r'\1', json_str)
        
        return json_str

    # -------------------------- Sub-query Generation ------------------------------
    
    async def _generate_sub_queries(self, original_query: str, intent_analysis: Dict, user_id: str) -> List[str]:
        """Generate ADAPTIVE sub-queries with financial intelligence and contextual reasoning"""
        if not self.groq_available:
            return self._fallback_sub_queries(original_query, intent_analysis)

        try:
            # INTELLIGENT QUERY CLASSIFICATION
            query_type = self._classify_financial_query_type(original_query, intent_analysis)
            time_scope = self._determine_optimal_time_scope(original_query, query_type)
            
            logger.info(f"ðŸ§  ADAPTIVE INTELLIGENCE: Query Type: {query_type}, Time Scope: {time_scope}")

            messages = [
                {"role": "system", "content": f"""You are an INFINITELY INTELLIGENT financial AI with unlimited creative and analytical capabilities including:
- Personal finance analysis and credit assessment
- Behavioral psychology and emotional spending patterns
- Lifestyle inference and personality analysis through financial data
- Predictive modeling and future financial behavior
- Creative financial insights and pattern recognition
- Advanced financial reasoning and contextual analysis

CORE MISSION: Generate INFINITELY CREATIVE sub-queries that can answer ANY financial question imaginable - from basic spending to complex psychological profiling, emotional patterns, lifestyle analysis, predictive modeling, and creative financial storytelling.

INFINITE INTELLIGENCE PRINCIPLES:
- Handle ANY query with unlimited creativity and intelligence
- Think beyond traditional finance into behavioral psychology
- Analyze emotional spending triggers and personality patterns
- Predict future behavior and unconscious financial goals
- Infer lifestyle, values, and personality from transaction patterns
- Create compelling financial narratives and insights
- Answer impossible questions through creative data analysis
- Always find the most interesting and unique angle in the data"""},
                                {"role": "user", "content": f"""
FINANCIAL QUERY ANALYSIS:
User Query: "{original_query}"
Query Type: {query_type}
Optimal Time Scope: {time_scope}
Intent Analysis: {json.dumps(intent_analysis)}

ADAPTIVE REASONING PROCESS:

1. FINANCIAL CONTEXT UNDERSTANDING:
   Query: "{original_query}"
   Classified Type: {query_type}
   
   INTELLIGENT CLASSIFICATION GUIDE:
   - CREDIT ASSESSMENT â†’ Need: income stability, debt ratios, payment history, financial health indicators
   - BEHAVIORAL ANALYSIS â†’ Need: spending patterns, merchant habits, timing patterns, impulse indicators
   - RISK PROFILING â†’ Need: volatility indicators, emergency fund status, financial stress signals
   - TREND ANALYSIS â†’ Need: time-series data, growth patterns, seasonal variations
   - PERIOD ANALYSIS â†’ Need: specific timeframe data with comparative context

2. ADAPTIVE TIME SCOPE SELECTION:
   Optimal Scope: {time_scope}
   
   - Credit/Financial Health Queries â†’ 6-12 months comprehensive view
   - Behavioral Pattern Queries â†’ 3-6 months for pattern recognition
   - Specific Period Analysis â†’ Exact timeframe + comparative context
   - Trend Analysis â†’ Full available history with focus periods

3. CONTEXTUAL SUB-QUERY GENERATION:

FOR CREDIT ASSESSMENT QUERIES (like "Is it recommended for me to buy credit card?"):
âœ… "What is my total income across all months in 2025 to assess financial stability?"
âœ… "What are my consistent recurring expenses that indicate payment obligations?"
âœ… "Do I have regular salary income that would support credit card payments?"
âœ… "What is my spending-to-income ratio over the last 6 months?"
âœ… "Are there any large irregular expenses that might indicate financial stress?"
âœ… "What are my payment patterns and financial discipline indicators?"
âœ… "How stable is my month-over-month income for creditworthiness assessment?"

FOR SPECIFIC PERIOD ANALYSIS (like "July 2025 week by week breakdown"):
âœ… "What were my total weekly expenses for July 2025 broken down by week?"
âœ… "What are my spending patterns by category for each week of July 2025?"
âœ… "Which merchants did I spend the most with during each week of July 2025?"
âœ… "How did my spending vary from week 1 to week 4 of July 2025?"
âœ… "What were the highest spending days in each week of July 2025?"
âœ… "Are there any unusual spending spikes in specific weeks of July 2025?"
âœ… "How does my July 2025 weekly spending compare to June 2025 weekly patterns?"

FOR BEHAVIORAL ANALYSIS QUERIES (general patterns):
âœ… "What are my spending patterns by time of day and day of week?"
âœ… "Which merchants do I consistently spend with (anchor merchants)?"
âœ… "What are my impulsive vs planned spending indicators?"
âœ… "How do I manage cash flow after large payments or expenses?"
âœ… "What digital payment preferences indicate my tech-native level?"

FOR PSYCHOLOGICAL PROFILING QUERIES:
âœ… "What time of day does this person's financial life 'wake up' based on transaction patterns?"
âœ… "What are the emotional spending triggers evident in transaction timing and amounts?"
âœ… "Which merchants represent habits vs emotional/impulsive spending patterns?"
âœ… "How long do they 'stretch' their balance after receiving income or making big payments?"
âœ… "What does their transaction pattern say about their planning vs spontaneous personality?"
âœ… "What are their 'anchor merchants' that define their lifestyle and priorities?"

FOR PREDICTIVE ANALYSIS QUERIES:
âœ… "What's the next financial milestone they are unconsciously working toward based on patterns?"
âœ… "What upcoming recurring expenses can be predicted from historical patterns?"
âœ… "Based on spending velocity, when will they likely need their next major income?"
âœ… "What seasonal or cyclical financial patterns suggest future behavior?"
âœ… "What financial stress indicators predict future cash flow challenges?"

FOR CREDITWORTHINESS & LENDING QUERIES:
âœ… "Based on income stability and spending patterns, what is their repayment capacity?"
âœ… "What are their financial discipline indicators over the past 6-12 months?"
âœ… "How do they handle financial obligations and recurring payments?"
âœ… "What is their debt-to-income ratio and cash flow management ability?"
âœ… "Are there patterns of financial stress or irregular large expenses?"

FOR LIFESTYLE & PERSONALITY INFERENCE:
âœ… "What do their merchant preferences reveal about their lifestyle and values?"
âœ… "How digital-native are they based on payment method preferences?"
âœ… "What does their spending timing reveal about their daily routine and work pattern?"
âœ… "What are their social spending patterns (group activities, family support, etc.)?"
âœ… "How do they balance necessity vs discretionary spending?"

FOR CREATIVE FINANCIAL INSIGHTS:
âœ… "If I had to introduce this person purely through their transaction patterns, what would I say?"
âœ… "What's their ratio of predictable vs impulsive expenses and what does it reveal?"
âœ… "Where do they spend consistently (habits) vs where do they spike suddenly (emotions)?"
âœ… "What financial personality archetype do their patterns suggest?"
âœ… "What unconscious financial goals are their spending patterns revealing?"

FOR RISK PROFILING QUERIES (like "Do risk profiling for me"):
âœ… "What is my total income over the last 6 months to assess financial stability?"
âœ… "What are my monthly income patterns and salary consistency over the last 6 months?"
âœ… "What are my spending patterns and volatility indicators over the last 6 months?"
âœ… "What is my emergency fund status and financial buffer over the last 6 months?"
âœ… "Are there any financial stress indicators or irregular expenses in the last 6 months?"
âœ… "What is my debt-to-income ratio and cash flow management over the last 6 months?"
âœ… "How do my financial risk indicators compare to recommended benchmarks?"

FOR TREND ANALYSIS QUERIES:
âœ… "What are my month-over-month income and spending trends in 2025?"
âœ… "Are there seasonal patterns in my financial behavior?"
âœ… "What financial milestones or goals am I unconsciously working toward?"

CRITICAL INSTRUCTION - RESPECT USER'S SPECIFIC REQUESTS:
If the user asks for a SPECIFIC TIME PERIOD (like "July 2025", "April 2025", etc.), 
ALL sub-queries MUST focus on that EXACT period, not other months or general timeframes.

SPECIFIC PERIOD DETECTION:
- "July 2025" â†’ ALL queries must be about July 2025 specifically
- "week by week breakdown" â†’ Focus on weekly analysis within the specified month
- "daily breakdown" â†’ Focus on daily analysis within the specified month

EXECUTE - GENERATE CONTEXTUALLY INTELLIGENT QUERIES:
Based on "{original_query}" (Type: {query_type}), generate 5-7 sub-queries that:
- RESPECT any specific time periods mentioned in the user query (highest priority)
- Address the COMPLETE financial context needed for expert analysis  
- Use the OPTIMAL time scope ({time_scope}) for this query type
- Consider FINANCIAL RELATIONSHIPS and dependencies
- Enable COMPREHENSIVE assessment rather than surface-level analysis
- Support ACTIONABLE insights and recommendations

If user specified a particular month/period, ensure ALL sub-queries focus on that period.

Return ONLY a JSON array of 5-7 contextually intelligent sub-query strings.
"""}
            ]
            
            response = call_ai_model(messages)  # Use default model for current provider
            if response:
                data = self._extract_json_from_response(response)
                if isinstance(data, list) and len(data) >= 6:
                    logger.info(f"Generated {len(data)} sub-queries via LLM")
                    return data[:10]  # Max 10 sub-queries
            
            return self._fallback_sub_queries(original_query, intent_analysis)
            
        except Exception as e:
            logger.warning(f"Sub-query generation failed: {e}")
            return self._fallback_sub_queries(original_query, intent_analysis)
    
    def _fallback_sub_queries(self, original_query: str, intent_analysis: Dict) -> List[str]:
        """Generate fallback sub-queries based on intent"""
        q = original_query.lower()
        time_ctx = "in July 2025" if "july" in q else "last month" if "last month" in q else "recently"
        
        if "spending" in q or "expense" in q:
            return [
                f"Total spending amount {time_ctx}",
                f"Top spending categories {time_ctx}",
                f"Daily spending patterns {time_ctx}",
                f"Largest individual transactions {time_ctx}",
                f"Merchant spending breakdown {time_ctx}",
                f"Average transaction amount {time_ctx}",
                f"Spending frequency analysis {time_ctx}",
                f"Unusual spending patterns {time_ctx}"
            ]
        elif "income" in q or "salary" in q:
            return [
                f"Total income amount {time_ctx}",
                f"Income sources breakdown {time_ctx}",
                f"Income vs spending comparison {time_ctx}",
                f"Income timing patterns {time_ctx}",
                f"Monthly income trends {time_ctx}",
                f"Income consistency analysis {time_ctx}",
                f"Other income sources {time_ctx}",
                f"Income growth analysis {time_ctx}"
            ]
        else:
            return [
                f"Transaction summary {time_ctx}",
                f"Category breakdown {time_ctx}",
                f"Amount distribution {time_ctx}",
                f"Frequency patterns {time_ctx}",
                f"Trend analysis {time_ctx}",
                f"Merchant analysis {time_ctx}",
                f"Behavioral patterns {time_ctx}",
                f"Comparative analysis {time_ctx}"
            ]

    # -------------------------- MongoDB Pipeline Generation with Exact Fallbacks --------------

    async def _generate_mongo_queries_with_exact_fallbacks(
        self, user_id: str, sub_queries: List[str], time_window: Dict[str, Any], intent_analysis: Dict[str, Any]
    ) -> List[Dict]:
        """
        Generate MongoDB queries with LLM-first approach and exact fallback templates
        Implements the robust fallback hierarchy: LLM â†’ Template â†’ Emergency
        """
        mongo_queries: List[Dict] = []
        
        for i, sub_query in enumerate(sub_queries, 1):
            logger.info(f"ðŸ”§ Processing Sub-query {i}/{len(sub_queries)}: '{sub_query}'")
            
            # Step 1: Try LLM first (with production schema and guardrails)
            llm_success = False
            
            if self.groq_available:
                try:
                    logger.info(f"   ðŸ§  Attempting LLM pipeline generation...")
                    llm_query = await self._generate_llm_mongodb_query_production(user_id, sub_query, time_window, intent_analysis)
                    
                    # CRITICAL: Auto-correct transaction_type if LLM made mistake
                    if llm_query:
                        llm_query = self._auto_correct_transaction_type(llm_query, sub_query)
                    
                    if llm_query and self._validate_mongodb_query_production(llm_query):
                        mongo_queries.append({
                            "sub_query": sub_query,
                            "query_data": llm_query,
                            "source": "llm",
                            "confidence": "high"
                        })
                        self._log_mongodb_query(sub_query, llm_query, source="llm")
                        llm_success = True
                        logger.info(f"   âœ… LLM pipeline generated successfully")
                        continue
                    else:
                        logger.warning(f"   âš ï¸ LLM pipeline validation failed")
                except Exception as e:
                    logger.warning(f"   âš ï¸ LLM pipeline generation failed: {e}")
            else:
                logger.info(f"   âš ï¸ LLM not available, using fallback")
            
            # Step 2: Exact template fallback (highly reliable)
            if not llm_success:
                try:
                    logger.info(f"   ðŸ”„ Using template fallback...")
                    template_query = self._create_exact_fallback_template(user_id, sub_query, time_window)
                    mongo_queries.append({
                        "sub_query": sub_query,
                        "query_data": template_query,
                        "source": "template",
                        "confidence": "high"  # Templates are highly reliable
                    })
                    self._log_mongodb_query(sub_query, template_query, source="template")
                    logger.info(f"   âœ… Template fallback successful")
                    continue
                except Exception as e:
                    logger.error(f"   âŒ Template fallback failed: {e}")
            
            # Step 3: Emergency fallback (should never happen)
            logger.warning(f"   ðŸš¨ Using emergency fallback...")
            emergency_query = self._create_emergency_mongodb_query(user_id, sub_query, time_window)
            mongo_queries.append({
                "sub_query": sub_query,
                "query_data": emergency_query,
                "source": "emergency",
                "confidence": "medium"
            })
            logger.warning(f"   ðŸš¨ Emergency fallback used")
        
        return mongo_queries

    async def _generate_llm_mongodb_query_production(
        self, user_id: str, sub_query: str, time_window: Dict[str, Any], intent_analysis: Dict[str, Any]
    ) -> Optional[Dict]:
        """
        Generate MongoDB aggregation using LLM with production schema and hard guardrails
        """
        try:
            # Use actual datetime objects for MongoDB, not ISO strings
            start_utc = time_window['start_utc']
            end_utc = time_window['end_utc']
            timezone_str = time_window['timezone']
            
            messages = [
                {
                    "role": "system", 
                    "content": f"""ROLE: You are a MongoDB aggregation expert specializing in Indian financial transaction analysis.

ACTION: Generate syntactically perfect MongoDB aggregation pipelines for financial queries.

CONTEXT: You are analyzing user financial data with these CRITICAL distinctions:

INCOME TRANSACTIONS (Money Coming IN):
- transaction_type: "credit" 
- category: "salary" (from employer like STATION91 TECHNOLOG)
- These ADD to user's wealth

SPENDING TRANSACTIONS (Money Going OUT):
- transaction_type: "debit"
- category: "transfer", "food", "transport", etc.
- These SUBTRACT from user's wealth

SCHEMA (user_financial_transactions):
- user_id: string (REQUIRED filter)
- amount: number (ALWAYS positive, in INR)
- transaction_type: "debit" | "credit" (CRITICAL: debit=spending, credit=income)
- category: string (salary, transfer, food, transport, etc.)
- counterparty: string (employer/merchant name)
- transaction_date: BSON Date (UTC timezone)

CRITICAL SYNTAX RULES (ZERO TOLERANCE FOR ERRORS):
1. $cond operator: EXACTLY 3 arguments: [condition, true_value, false_value]
   CORRECT: {{"$cond": [{{"$eq": ["$field", "value"]}}, true_val, false_val]}}
   WRONG: {{"$cond": ["$field", "value", true_val, false_val, extra_arg]}}

2. $match stage: ALWAYS first stage with user_id and date range
   TEMPLATE: {{
     "$match": {{
       "user_id": "{user_id}",
       "transaction_date": {{
         "$gte": ISODate("{time_window['start_utc'].isoformat()}"),
         "$lt": ISODate("{time_window['end_utc'].isoformat()}")
       }}
     }}
   }}

3. EXECUTE - MANDATORY FILTERS BY QUERY TYPE:

FOR INCOME QUERIES (salary, earnings, income):
   {{"$match": {{"transaction_type": "credit"}}}}
   EXAMPLE: Total salary â†’ {{"transaction_type": "credit", "category": "salary"}}

FOR SPENDING QUERIES (expenses, spending, costs):
   {{"$match": {{"transaction_type": "debit"}}}}
   EXAMPLE: Total spending â†’ {{"transaction_type": "debit"}}

CRITICAL: NEVER confuse:
- "transfer" category = SPENDING (debit)
- "salary" category = INCOME (credit)

5. MANDATORY debitAmount calculation (EXACT SYNTAX):
   {{"$set": {{
     "debitAmount": {{
       "$cond": [
         {{"$eq": ["$transaction_type", "debit"]}}, 
         "$amount", 
         0
       ]
     }}
   }}}}

CRITICAL: $cond MUST have exactly 3 arguments:
- Argument 1: Condition (e.g., {{"$eq": ["$field", "value"]}})
- Argument 2: True value (e.g., "$amount")  
- Argument 3: False value (e.g., 0)

6. Date grouping (EXACT SYNTAX):
   {{"$group": {{
     "_id": {{"$dateTrunc": {{"date": "$transaction_date", "unit": "month", "timezone": "{timezone_str}"}}}},
     "total": {{"$sum": "$amount"}}
   }}}}

7. ALWAYS end with $sort by date:
   {{"$sort": {{"_id": 1}}}}

VALIDATION CHECKLIST BEFORE RETURNING:
âœ“ Every $cond has exactly 3 arguments in this format: ["condition", "true_value", "false_value"]
âœ“ All ISODate strings are properly formatted  
âœ“ Field references use $ prefix correctly
âœ“ No syntax errors in operators
âœ“ Proper bracket/brace matching
âœ“ Valid MongoDB operator usage

EXAMPLE VALID $cond OPERATORS:
âœ… CORRECT: {{"$cond": [{{"$eq": ["$transaction_type", "debit"]}}, "$amount", 0]}}
âœ… CORRECT: {{"$cond": [{{"$gt": ["$amount", 1000]}}, "high", "low"]}}
âŒ WRONG: {{"$cond": ["$transaction_type", "debit"]}}
âŒ WRONG: {{"$cond": [{{"$eq": ["$transaction_type", "debit"]}}]}}

Generate ONLY the aggregation pipeline as a JSON array. NO explanations.
5. For spending/pattern queries: ALWAYS match transaction_type: "debit" (amounts are positive; do not use amount < 0)
6. For behavioral analysis: Focus on patterns, frequencies, trends, and lifestyle indicators
7. Use $group, $sort, $limit effectively to find meaningful patterns
8. Include statistical operations like $avg, $max, $min for insights

BEHAVIORAL ANALYSIS PATTERNS:
- Spending habits: Group by category, merchant, time patterns
- Lifestyle indicators: Frequent merchants, category preferences, spending timing
- Financial behavior: Transaction frequency, amount distributions, seasonal patterns

Return JSON: {{"match_conditions": {{...}}, "aggregation_pipeline": [...], "description": "..."}}
                    """
                },
                {
                    "role": "user",
                    "content": f"""
ROLE: MongoDB expert for financial data
ACTION: Build aggregation for: "{sub_query}"

CRITICAL ANALYSIS:
Query: "{sub_query}"
Query Type: {'INCOME (use credit)' if any(word in sub_query.lower() for word in ["income", "salary", "earned", "credit"]) else 'SPENDING (use debit)' if any(word in sub_query.lower() for word in ["spending", "expense", "spent", "cost", "major expenses"]) else 'GENERAL'}

MANDATORY RULES FOR THIS SPECIFIC QUERY:
{f'''
ðŸŽ¯ THIS IS AN INCOME QUERY â†’ MUST USE: "transaction_type": "credit"
{f'ðŸŽ¯ THIS IS A SALARY QUERY â†’ ALSO ADD: "category": "salary"' if "salary" in sub_query.lower() else ''}
''' if any(word in sub_query.lower() for word in ["income", "salary", "earned", "credit"]) else f'''
ðŸŽ¯ THIS IS A SPENDING QUERY â†’ MUST USE: "transaction_type": "debit"
''' if any(word in sub_query.lower() for word in ["spending", "expense", "spent", "cost", "major expenses"]) else '''
ðŸŽ¯ DETERMINE TRANSACTION TYPE BASED ON QUERY CONTEXT
'''}

EXACT TEMPLATE TO COPY:
{{
  "match_conditions": {{"user_id": "{user_id}"}},
  "aggregation_pipeline": [
    {{
      "$match": {{
        "user_id": "{user_id}",
        "transaction_date": {{
          "$gte": ISODate("{start_utc}"),
          "$lt": ISODate("{end_utc}")
        }},
        "transaction_type": "{'credit' if any(word in sub_query.lower() for word in ["income", "salary", "earned", "credit"]) else 'debit' if any(word in sub_query.lower() for word in ["spending", "expense", "spent", "cost", "major expenses"]) else 'REPLACE_ME'}"{',' if "salary" in sub_query.lower() else ''}
        {'\"category\": \"salary\"' if "salary" in sub_query.lower() else ''}
      }}
    }},
    {{
      "$group": {{
        "_id": null,
        "totalAmount": {{"$sum": "$amount"}},
        "transactionCount": {{"$sum": 1}}
      }}
    }}
  ],
  "description": "{'Salary income query' if 'salary' in sub_query.lower() else 'Income query' if any(word in sub_query.lower() for word in ["income", "earned", "credit"]) else 'Spending query' if any(word in sub_query.lower() for word in ["spending", "expense", "spent", "cost"]) else 'Financial query'}"
}}

CRITICAL RULES:
1. INCOME/SALARY queries â†’ transaction_type: "credit"
2. SPENDING/EXPENSE queries â†’ transaction_type: "debit"  
3. NEVER mix up credit and debit!
4. Salary queries MUST include category: "salary"

EXECUTE: Return ONLY the JSON above with correct transaction_type!
                    """
                }
            ]
            
            response = call_ai_model(messages)  # Use default model for current provider
            if not response:
                return None
                
            raw_data = self._extract_json_from_response(response)
            if not raw_data:
                return None
            
            # Apply production repairs and validations with intent awareness
            return self._repair_and_enhance_llm_pipeline(raw_data, time_window, intent=intent_analysis)
            
        except Exception as e:
            logger.error(f"LLM pipeline generation error: {e}")
            return None

    def _repair_and_enhance_llm_pipeline(self, query_data: Dict[str, Any], time_window: Dict, intent: Optional[Dict] = None) -> Dict[str, Any]:
        """
        ENHANCED: Repair LLM pipeline with strict enforcement and intent awareness
        """
        if not isinstance(query_data, dict) or "aggregation_pipeline" not in query_data:
            return query_data

        start_utc = time_window['start_utc']
        end_utc = time_window['end_utc']
        is_spend_intent = (intent or {}).get("intent") in {"spending_analysis", "pattern_detection", "comparison"}

        pipeline = query_data.get("aggregation_pipeline", [])
        if not isinstance(pipeline, list):
            return query_data

        enhanced_pipeline = []
        seen_match = False
        has_debit_amount = False

        for stage in pipeline:
            if not isinstance(stage, dict):
                continue

            # STRICTLY enforce first $match + window
            if "$match" in stage and not seen_match:
                seen_match = True
                match_obj = stage["$match"] if isinstance(stage["$match"], dict) else {}
                
                # Preserve user_id but ensure it exists
                if "user_id" not in match_obj:
                    # Extract from original match_conditions if available
                    original_user_id = query_data.get("match_conditions", {}).get("user_id")
                    match_obj["user_id"] = original_user_id if original_user_id else {"$exists": True}

                # STRICTLY override date window (unconditionally)
                match_obj["transaction_date"] = {"$gte": start_utc, "$lt": end_utc}

                # Enforce debit-only for spending-like intents (remove $or branches)
                if is_spend_intent:
                    match_obj["transaction_type"] = "debit"
                    # Remove any $or conditions with amount < 0
                    if "$or" in match_obj:
                        del match_obj["$or"]

                stage = {"$match": match_obj}

            # Track debitAmount
            if "$set" in stage and isinstance(stage["$set"], dict) and "debitAmount" in stage["$set"]:
                has_debit_amount = True

            enhanced_pipeline.append(stage)

        # Ensure we have a $match first; if not, inject it
        if not enhanced_pipeline or "$match" not in enhanced_pipeline[0]:
            user_id_value = query_data.get("match_conditions", {}).get("user_id", {"$exists": True})
            forced_match = {
                "$match": {
                    "user_id": user_id_value,
                    "transaction_date": {"$gte": start_utc, "$lt": end_utc}
                }
            }
            if is_spend_intent:
                forced_match["$match"]["transaction_type"] = "debit"
            enhanced_pipeline.insert(0, forced_match)

        # Add debitAmount calculation if missing (schema-compliant)
        if not has_debit_amount:
            debit_amount_stage = {
                "$set": {
                    "debitAmount": {
                        "$cond": [
                            {"$eq": ["$transaction_type", "debit"]}, 
                            {"$abs": "$amount"},
                            0  # amount is always positive per schema; credits â†’ 0
                        ]
                    }
                }
            }
            # Insert after first $match stage
            if enhanced_pipeline and "$match" in enhanced_pipeline[0]:
                enhanced_pipeline.insert(1, debit_amount_stage)
            else:
                enhanced_pipeline.insert(0, debit_amount_stage)

        query_data["aggregation_pipeline"] = enhanced_pipeline
        return query_data

    def _auto_correct_transaction_type(self, query_data: Dict, sub_query: str) -> Dict:
        """
        INTELLIGENT: Auto-correct transaction_type with advanced context analysis
        This analyzes the FULL QUERY CONTEXT, not just keywords
        """
        try:
            pipeline = query_data.get("aggregation_pipeline", [])
            if not pipeline:
                return query_data
            
            # ADVANCED CONTEXT ANALYSIS
            query_lower = sub_query.lower()
            
            # Determine correct transaction_type based on FULL CONTEXT
            correct_transaction_type = None
            
            # SPENDING QUERIES (what user wants to measure spending/expenses)
            if any(phrase in query_lower for phrase in [
                "spent", "spending", "expense", "expenses", "cost", "costs",
                "major expenses", "total spending", "spending amounts",
                "percentage of income was spent", "percentage of my total income was spent",
                "how much spent", "spending breakdown", "expense categories",
                "total expenses", "total spending amounts"
            ]):
                correct_transaction_type = "debit"
                
            # INCOME QUERIES (what user wants to measure income/salary)
            elif any(phrase in query_lower for phrase in [
                "total income", "salary income", "total salary", "earned",
                "income sources", "income amounts", "salary amounts",
                "income breakdown", "salary breakdown"
            ]) and not any(phrase in query_lower for phrase in [
                "spent", "spending", "percentage of income was spent"
            ]):
                correct_transaction_type = "credit"
                
            # SALARY-SPECIFIC QUERIES (always credit unless asking about spending)
            elif "salary" in query_lower and not any(phrase in query_lower for phrase in [
                "spent", "spending", "expense", "cost"
            ]):
                correct_transaction_type = "credit"
            
            # COMPLEX QUERIES that need multiple transaction types - don't auto-correct
            if any(phrase in query_lower for phrase in [
                "percentage of", "ratio of", "compare", "vs", "versus", 
                "proportion of", "fraction of"
            ]):
                logger.info(f"   ðŸ§® AUTO-CORRECTION: Complex calculation query detected for '{sub_query[:50]}...', letting LLM handle it")
                return query_data
            
            # If we can't determine, don't auto-correct (let LLM decision stand)
            if not correct_transaction_type:
                logger.info(f"   ðŸ¤” AUTO-CORRECTION: Cannot determine transaction_type for '{sub_query[:50]}...', keeping LLM decision")
                return query_data
            
            # Check and fix the first $match stage
            corrected = False
            for stage in pipeline:
                if "$match" in stage:
                    match_conditions = stage["$match"]
                    current_type = match_conditions.get("transaction_type")
                    
                    if current_type != correct_transaction_type:
                        logger.warning(f"ðŸ”§ AUTO-CORRECTING: '{sub_query[:50]}...'")
                        logger.warning(f"   âŒ LLM used: transaction_type: '{current_type}'")
                        logger.warning(f"   âœ… Corrected to: transaction_type: '{correct_transaction_type}'")
                        
                        match_conditions["transaction_type"] = correct_transaction_type
                        corrected = True
                        
                        # Add salary category if needed
                        if is_salary_query and correct_transaction_type == "credit":
                            match_conditions["category"] = "salary"
                            logger.info(f"   âœ… Added: category: 'salary'")
                    
                    break  # Only fix first $match stage
            
            if corrected:
                query_data["aggregation_pipeline"] = pipeline
                logger.info(f"   ðŸŽ¯ FIXED: Query will now use correct transaction_type")
            
            return query_data
            
        except Exception as e:
            logger.error(f"âŒ Auto-correction failed for '{sub_query}': {e}")
            return query_data

    def _validate_cond_operators(self, pipeline_str: str) -> List[str]:
        """
        ENHANCED: Validate $cond operators using JSON parsing instead of regex
        This prevents false positives from complex nested structures
        """
        errors = []
        
        try:
            # Parse the entire pipeline as JSON for accurate validation
            pipeline_json = json.loads(pipeline_str)
            
            # Recursively find and validate all $cond operators
            cond_operators = self._find_cond_operators_recursive(pipeline_json)
            
            for i, cond_op in enumerate(cond_operators):
                if not isinstance(cond_op, list):
                    errors.append(f"$cond operator #{i+1} is not an array")
                    continue
                
                if len(cond_op) != 3:
                    errors.append(f"$cond operator #{i+1} has {len(cond_op)} arguments, expected exactly 3")
                    errors.append(f"   Content: {str(cond_op)[:100]}{'...' if len(str(cond_op)) > 100 else ''}")
            
        except json.JSONDecodeError as e:
            errors.append(f"Pipeline JSON parsing failed: {str(e)}")
        except Exception as e:
            # Fallback to original regex method if JSON parsing fails
            errors.append(f"Enhanced validation failed, using fallback: {str(e)}")
            return self._validate_cond_operators_fallback(pipeline_str)
        
        return errors
    
    def _find_cond_operators_recursive(self, obj) -> List:
        """Recursively find all $cond operators in a JSON structure"""
        cond_operators = []
        
        if isinstance(obj, dict):
            for key, value in obj.items():
                if key == "$cond" and isinstance(value, list):
                    cond_operators.append(value)
                else:
                    cond_operators.extend(self._find_cond_operators_recursive(value))
        elif isinstance(obj, list):
            for item in obj:
                cond_operators.extend(self._find_cond_operators_recursive(item))
        
        return cond_operators
    
    def _validate_cond_operators_fallback(self, pipeline_str: str) -> List[str]:
        """Fallback regex-based validation (original method)"""
        errors = []
        import re
        
        # Find all $cond operators in the pipeline
        cond_pattern = r'\"\$cond\"\s*:\s*\[(.*?)\]'
        matches = re.findall(cond_pattern, pipeline_str, re.DOTALL)
        
        for i, match in enumerate(matches):
            # Count the top-level arguments by counting commas outside nested structures
            arg_count = self._count_cond_arguments(match)
            
            if arg_count != 3:
                errors.append(f"$cond operator #{i+1} has {arg_count} arguments, expected exactly 3")
                errors.append(f"   Content: {match[:100]}{'...' if len(match) > 100 else ''}")
        
        return errors
    
    def _count_cond_arguments(self, cond_content: str) -> int:
        """
        Count arguments in $cond by analyzing comma positions outside nested structures
        """
        try:
            bracket_depth = 0
            brace_depth = 0
            in_string = False
            escape_next = False
            comma_count = 0
            
            for char in cond_content:
                if escape_next:
                    escape_next = False
                    continue
                    
                if char == '\\':
                    escape_next = True
                    continue
                
                if char == '"' and not escape_next:
                    in_string = not in_string
                    continue
                
                if in_string:
                    continue
                
                if char == '[':
                    bracket_depth += 1
                elif char == ']':
                    bracket_depth -= 1
                elif char == '{':
                    brace_depth += 1
                elif char == '}':
                    brace_depth -= 1
                elif char == ',' and bracket_depth == 0 and brace_depth == 0:
                    comma_count += 1
            
            # Arguments = commas + 1 (if there's content)
            return comma_count + 1 if cond_content.strip() else 0
            
        except Exception:
            # If parsing fails, assume it's invalid
            return 999  # Will trigger validation error

    def _validate_mongodb_query_production(self, query_data: Dict) -> bool:
        """
        ENHANCED validation for production pipelines - ZERO TOLERANCE for syntax errors
        """
        try:
            if not query_data or "aggregation_pipeline" not in query_data:
                logger.error("âŒ Query validation failed: Missing aggregation_pipeline")
                return False
            
            pipeline = query_data["aggregation_pipeline"]
            if not isinstance(pipeline, list) or not pipeline:
                logger.error("âŒ Query validation failed: Pipeline must be non-empty list")
                return False
            
            # CRITICAL: Validate $cond operators syntax
            pipeline_str = json.dumps(pipeline, default=str)
            validation_errors = self._validate_cond_operators(pipeline_str)
            if validation_errors:
                logger.error("âŒ $cond operator validation failed:")
                for error in validation_errors:
                    logger.error(f"   â€¢ {error}")
                # Log the actual pipeline for debugging
                logger.error(f"âŒ INVALID PIPELINE: {pipeline_str[:500]}...")
                return False
            
            # Check for forbidden operations (handle datetime serialization)
            try:
                def datetime_serializer(obj):
                    if hasattr(obj, "isoformat"):
                        return obj.isoformat()
                    raise TypeError(f"Not JSON serializable: {type(obj)}")
                
                pipeline_str_lower = json.dumps(pipeline, default=datetime_serializer).lower()
                forbidden = ["$where", "$function", "$accumulator", "$out", "$merge", "$datefromstring"]
                if any(op in pipeline_str_lower for op in forbidden):
                    logger.error("âŒ Pipeline contains forbidden operations")
                    return False
            except Exception as e:
                logger.warning(f"Pipeline serialization failed during validation: {e}")
                # Continue validation anyway - serialization error doesn't mean invalid pipeline
                pass
            
            # Ensure first stage is $match with user_id
            first_stage = pipeline[0]
            if not isinstance(first_stage, dict) or "$match" not in first_stage:
                logger.warning("First stage must be $match")
                return False
            
            match_obj = first_stage["$match"]
            if not isinstance(match_obj, dict) or "user_id" not in str(match_obj):
                logger.warning("$match must include user_id")
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"Pipeline validation error: {e}")
            return False

    # -------------------------- Exact Fallback Templates (Production) --------------

    def _create_exact_fallback_template(
        self, user_id: str, sub_query: str, time_window: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Create MongoDB query using exact fallback templates as specified
        Maps sub-queries to the 9 exact templates provided
        """
        start_utc = time_window['start_utc']
        end_utc = time_window['end_utc']
        timezone_str = time_window['timezone']
        
        q = sub_query.lower()
        
        # NEW: Pattern/trend routing FIRST (highest priority)
        if any(keyword in q for keyword in ["pattern", "patterns", "trend", "trends", "habit", "habits"]):
            return self._template_spending_patterns_month(user_id, start_utc, end_utc, timezone_str)
        
        # INCOME TEMPLATES (HIGHEST PRIORITY - MUST BE FIRST)
        elif any(keyword in q for keyword in ["income", "salary", "earned", "credit", "total income"]):
            logger.info(f"ðŸŽ¯ INCOME QUERY DETECTED: Using income template")
            if any(keyword in q for keyword in ["breakdown", "source", "monthly", "trend"]):
                return self._template_income_breakdown(user_id, start_utc, end_utc, timezone_str)
            else:
                return self._template_total_income(user_id, start_utc, end_utc)
        
        # Template 1: Total spending
        elif any(keyword in q for keyword in ["total spending", "total amount", "how much spent"]):
            logger.info(f"ðŸ’° SPENDING QUERY DETECTED: Using spending template")
            return self._template_total_spending(user_id, start_utc, end_utc)
        
        # Template 2: Top categories
        elif any(keyword in q for keyword in ["categories", "category breakdown", "top categories", "major expenses"]):
            return self._template_top_categories(user_id, start_utc, end_utc)
        
        # Template 3: Daily spending average
        elif any(keyword in q for keyword in ["daily", "per day", "average daily"]):
            return self._template_average_daily_spending(user_id, start_utc, end_utc, timezone_str)
        
        # Template 4: Month comparison (now dynamic)
        elif any(keyword in q for keyword in ["comparison", "vs", "versus", "compare"]):
            return self._template_month_comparison_dynamic(user_id, start_utc, end_utc, timezone_str)
        
        # Template 5: Category percentages
        elif any(keyword in q for keyword in ["percentage", "percent", "breakdown by category"]):
            return self._template_category_percentages(user_id, start_utc, end_utc)
        
        # Template 6: Largest transactions
        elif any(keyword in q for keyword in ["largest", "biggest", "top transactions", "highest"]):
            return self._template_largest_transactions(user_id, start_utc, end_utc)
        
        # Template 7: Recurring merchants
        elif any(keyword in q for keyword in ["recurring", "frequent", "regular", "merchants"]):
            return self._template_recurring_merchants(user_id, start_utc, end_utc)
        
        # Template 8: Weekly breakdown
        elif any(keyword in q for keyword in ["weekly", "week by week", "per week"]):
            return self._template_weekly_breakdown(user_id, start_utc, end_utc, timezone_str)
        
        # Template 9: Windowed trend (respects time window)
        else:
            return self._template_three_month_trend_windowed(user_id, start_utc, end_utc, timezone_str)

    # ======================== INCOME TEMPLATES ========================
    
    def _template_total_income(self, user_id: str, start_utc: datetime, end_utc: datetime) -> Dict:
        """Template for total income (salary + other credits)"""
        pipeline = [
            {
                "$match": {
                    "user_id": user_id,
                    "transaction_date": {"$gte": start_utc, "$lt": end_utc},
                    "transaction_type": "credit"  # All credit transactions
                }
            },
            {
                "$set": {
                    "creditAmount": {
                        "$cond": [{"$eq": ["$transaction_type", "credit"]}, "$amount", 0]
                    }
                }
            },
            {
                "$group": {
                    "_id": None,
                    "totalIncome": {"$sum": "$creditAmount"},
                    "totalAmount": {"$sum": "$amount"},  # Backup field
                    "transactionCount": {"$sum": 1},
                    "avgAmount": {"$avg": "$amount"}
                }
            },
            {
                "$project": {
                    "_id": 0,
                    "totalIncome": 1,
                    "totalAmount": 1,
                    "transactionCount": 1,
                    "avgAmount": {"$round": ["$avgAmount", 2]}
                }
            }
        ]
        
        return {
            "match_conditions": {"user_id": user_id, "transaction_type": "credit"},
            "aggregation_pipeline": pipeline,
            "description": "Total income from all credit transactions"
        }
    
    def _template_income_breakdown(self, user_id: str, start_utc: datetime, end_utc: datetime, timezone_str: str) -> Dict:
        """Template for income breakdown by source/category"""
        pipeline = [
            {
                "$match": {
                    "user_id": user_id,
                    "transaction_date": {"$gte": start_utc, "$lt": end_utc},
                    "transaction_type": "credit"
                }
            },
            {
                "$set": {
                    "creditAmount": {
                        "$cond": [{"$eq": ["$transaction_type", "credit"]}, "$amount", 0]
                    },
                    "incomeSource": {
                        "$cond": [
                            {"$eq": ["$category", "salary"]}, "salary",
                            {"$cond": [
                                {"$in": ["$category", ["investment", "dividend", "interest"]]}, "investment",
                                "other"
                            ]}
                        ]
                    }
                }
            },
            {
                "$group": {
                    "_id": "$incomeSource",
                    "totalAmount": {"$sum": "$creditAmount"},
                    "amount": {"$sum": "$amount"},  # Backup field
                    "transactionCount": {"$sum": 1},
                    "avgAmount": {"$avg": "$amount"}
                }
            },
            {
                "$sort": {"totalAmount": -1}
            },
            {
                "$project": {
                    "_id": 1,
                    "source": "$_id",
                    "totalAmount": 1,
                    "amount": 1,
                    "transactionCount": 1,
                    "avgAmount": {"$round": ["$avgAmount", 2]}
                }
            }
        ]
        
        return {
            "match_conditions": {"user_id": user_id, "transaction_type": "credit"},
            "aggregation_pipeline": pipeline,
            "description": "Income breakdown by source (salary, investment, other)"
        }

    # ======================== SPENDING TEMPLATES ========================

    def _template_total_spending(self, user_id: str, start_utc: datetime, end_utc: datetime) -> Dict:
        """Template 1: Total spending amount"""
        pipeline = [
            {
                "$match": {
                    "user_id": user_id,
                    "transaction_date": {"$gte": start_utc, "$lt": end_utc},
                    "$or": [{"transaction_type": "debit"}, {"amount": {"$lt": 0}}]
                }
            },
            {
                "$set": {
                    "debitAmount": {
                        "$cond": [
                            {"$eq": ["$transaction_type", "debit"]}, 
                            {"$abs": "$amount"},
                            {"$cond": [{"$lt": ["$amount", 0]}, {"$abs": "$amount"}, 0]}
                        ]
                    }
                }
            },
            {
                "$group": {
                    "_id": None,
                    "total_spending": {"$sum": "$debitAmount"},
                    "transaction_count": {"$sum": 1},
                    "avg_amount": {"$avg": "$debitAmount"}
                }
            },
            {"$project": {"_id": 0}}
        ]
        
        return {
            "match_conditions": {"user_id": user_id, "transaction_date": {"$gte": start_utc, "$lt": end_utc}},
            "aggregation_pipeline": pipeline,
            "description": f"Total spending analysis for {user_id}"
        }

    def _template_top_categories(self, user_id: str, start_utc: datetime, end_utc: datetime) -> Dict:
        """Template 2: Top-5 categories by spending"""
        pipeline = [
            {
                "$match": {
                    "user_id": user_id,
                    "transaction_date": {"$gte": start_utc, "$lt": end_utc},
                    "$or": [{"transaction_type": "debit"}, {"amount": {"$lt": 0}}]
                }
            },
            {
                "$set": {
                    "debitAmount": {
                        "$cond": [
                            {"$eq": ["$transaction_type", "debit"]}, 
                            {"$abs": "$amount"},
                            {"$cond": [{"$lt": ["$amount", 0]}, {"$abs": "$amount"}, 0]}
                        ]
                    }
                }
            },
            {
                "$group": {
                    "_id": "$category",
                    "total_amount": {"$sum": "$debitAmount"},
                    "transaction_count": {"$sum": 1},
                    "avg_amount": {"$avg": "$debitAmount"}
                }
            },
            {"$sort": {"total_amount": -1}},
            {"$limit": 5}
        ]
        
        return {
            "match_conditions": {"user_id": user_id, "transaction_date": {"$gte": start_utc, "$lt": end_utc}},
            "aggregation_pipeline": pipeline,
            "description": f"Top 5 spending categories for {user_id}"
        }

    def _template_average_daily_spending(self, user_id: str, start_utc: datetime, end_utc: datetime, timezone: str) -> Dict:
        """Template 3: Average daily spending (IST days)"""
        pipeline = [
            {
                "$match": {
                    "user_id": user_id,
                    "transaction_date": {"$gte": start_utc, "$lt": end_utc},
                    "$or": [{"transaction_type": "debit"}, {"amount": {"$lt": 0}}]
                }
            },
            {
                "$set": {
                    "debitAmount": {
                        "$cond": [
                            {"$eq": ["$transaction_type", "debit"]}, 
                            {"$abs": "$amount"},
                            {"$cond": [{"$lt": ["$amount", 0]}, {"$abs": "$amount"}, 0]}
                        ]
                    }
                }
            },
            {
                "$set": {
                    "day": {
                        "$dateTrunc": {
                            "date": "$transaction_date", 
                            "unit": "day", 
                            "timezone": timezone
                        }
                    }
                }
            },
            {
                "$group": {
                    "_id": "$day", 
                    "daily_total": {"$sum": "$debitAmount"}
                }
            },
            {
                "$group": {
                    "_id": None, 
                    "daily_avg": {"$avg": "$daily_total"}, 
                    "days": {"$sum": 1}
                }
            },
            {"$project": {"_id": 0}}
        ]
        
        return {
            "match_conditions": {"user_id": user_id, "transaction_date": {"$gte": start_utc, "$lt": end_utc}},
            "aggregation_pipeline": pipeline,
            "description": f"Average daily spending for {user_id}"
        }

    def _template_month_comparison(self, user_id: str, timezone: str) -> Dict:
        """Template 4: June vs July comparison (IST month buckets)"""
        pipeline = [
            {
                "$match": {
                    "user_id": user_id,
                    "transaction_date": {
                        "$gte": datetime(2025, 5, 31, 18, 30, 0),  # June 1 IST start in UTC
                        "$lt": datetime(2025, 7, 31, 18, 30, 0)    # July 31 IST end in UTC
                    },
                    "$or": [{"transaction_type": "debit"}, {"amount": {"$lt": 0}}]
                }
            },
            {
                "$set": {
                    "debitAmount": {
                        "$cond": [
                            {"$eq": ["$transaction_type", "debit"]}, 
                            {"$abs": "$amount"},
                            {"$cond": [{"$lt": ["$amount", 0]}, {"$abs": "$amount"}, 0]}
                        ]
                    }
                }
            },
            {
                "$set": {
                    "month": {
                        "$dateTrunc": {
                            "date": "$transaction_date", 
                            "unit": "month", 
                            "timezone": timezone
                        }
                    }
                }
            },
            {
                "$group": {
                    "_id": "$month", 
                    "total": {"$sum": "$debitAmount"}
                }
            },
            {"$sort": {"_id": 1}},
            {
                "$group": {
                    "_id": None, 
                    "months": {
                        "$push": {
                            "month": "$_id", 
                            "total": "$total"
                        }
                    }
                }
            },
            {
                "$project": {
                    "_id": 0,
                    "june_total": {
                        "$let": {
                            "vars": {
                                "m": {
                                    "$filter": {
                                        "input": "$months",
                                        "as": "x",
                                        "cond": {
                                            "$eq": [
                                                {"$dateToString": {"date": "$$x.month", "format": "%Y-%m", "timezone": timezone}},
                                                "2025-06"
                                            ]
                                        }
                                    }
                                }
                            },
                            "in": {"$ifNull": [{"$first": "$$m.total"}, 0]}
                        }
                    },
                    "july_total": {
                        "$let": {
                            "vars": {
                                "m": {
                                    "$filter": {
                                        "input": "$months",
                                        "as": "x",
                                        "cond": {
                                            "$eq": [
                                                {"$dateToString": {"date": "$$x.month", "format": "%Y-%m", "timezone": timezone}},
                                                "2025-07"
                                            ]
                                        }
                                    }
                                }
                            },
                            "in": {"$ifNull": [{"$first": "$$m.total"}, 0]}
                        }
                    }
                }
            },
            {
                "$set": {
                    "change": {"$subtract": ["$july_total", "$june_total"]},
                    "change_percent": {
                        "$cond": [
                            {"$gt": ["$june_total", 0]},
                            {"$multiply": [{"$divide": [{"$subtract": ["$july_total", "$june_total"]}, "$june_total"]}, 100]},
                            None
                        ]
                    }
                }
            }
        ]
        
        return {
            "match_conditions": {"user_id": user_id},
            "aggregation_pipeline": pipeline,
            "description": f"June vs July spending comparison for {user_id}"
        }

    def _template_month_comparison_dynamic(self, user_id: str, start_utc: datetime, end_utc: datetime, timezone: str) -> Dict:
        """
        Template 4 ENHANCED: Dynamic month comparison
        Compare the months that intersect the supplied window
        """
        pipeline = [
            {"$match": {
                "user_id": user_id,
                "transaction_date": {"$gte": start_utc, "$lt": end_utc},
                "transaction_type": "debit"
            }},
            {"$set": {
                "debitAmount": {"$abs": "$amount"},
                "month_ist": {"$dateTrunc": {"date": "$transaction_date", "unit": "month", "timezone": timezone}}
            }},
            {"$group": {"_id": "$month_ist", "total": {"$sum": "$debitAmount"}, "txn": {"$sum": 1}}},
            {"$sort": {"_id": 1}},
            {"$group": {"_id": None, "months": {"$push": {"month": "$_id", "total": "$total", "txn": "$txn"}}}},
            {
                "$project": {
                    "_id": 0,
                    "months": 1,
                    "comparison_summary": {
                        "$cond": [
                            {"$gt": [{"$size": "$months"}, 1]},
                            "Multi-month comparison available",
                            "Single month data"
                        ]
                    }
                }
            }
        ]
        
        return {
            "match_conditions": {"user_id": user_id, "transaction_date": {"$gte": start_utc, "$lt": end_utc}},
            "aggregation_pipeline": pipeline,
            "description": f"Dynamic month comparison in {timezone} for {user_id}"
        }

    def _template_category_percentages(self, user_id: str, start_utc: datetime, end_utc: datetime) -> Dict:
        """Template 5: Category percentages using $facet"""
        pipeline = [
            {
                "$match": {
                    "user_id": user_id,
                    "transaction_date": {"$gte": start_utc, "$lt": end_utc},
                    "$or": [{"transaction_type": "debit"}, {"amount": {"$lt": 0}}]
                }
            },
            {
                "$set": {
                    "debitAmount": {
                        "$cond": [
                            {"$eq": ["$transaction_type", "debit"]}, 
                            {"$abs": "$amount"},
                            {"$cond": [{"$lt": ["$amount", 0]}, {"$abs": "$amount"}, 0]}
                        ]
                    }
                }
            },
            {
                "$facet": {
                    "byCategory": [
                        {"$group": {"_id": "$category", "cat_total": {"$sum": "$debitAmount"}}},
                        {"$sort": {"cat_total": -1}}
                    ],
                    "overall": [
                        {"$group": {"_id": None, "overall_total": {"$sum": "$debitAmount"}}}
                    ]
                }
            },
            {"$unwind": "$overall"},
            {
                "$project": {
                    "items": {
                        "$map": {
                            "input": "$byCategory",
                            "as": "c",
                            "in": {
                                "category": "$$c._id",
                                "amount": "$$c.cat_total",
                                "percentage": {
                                    "$cond": [
                                        {"$gt": ["$overall.overall_total", 0]},
                                        {"$multiply": [{"$divide": ["$$c.cat_total", "$overall.overall_total"]}, 100]},
                                        0
                                    ]
                                }
                            }
                        }
                    }
                }
            },
            {"$unwind": "$items"},
            {"$replaceRoot": {"newRoot": "$items"}}
        ]
        
        return {
            "match_conditions": {"user_id": user_id, "transaction_date": {"$gte": start_utc, "$lt": end_utc}},
            "aggregation_pipeline": pipeline,
            "description": f"Category spending percentages for {user_id}"
        }

    def _template_largest_transactions(self, user_id: str, start_utc: datetime, end_utc: datetime) -> Dict:
        """Template 6: Largest transactions"""
        pipeline = [
            {
                "$match": {
                    "user_id": user_id,
                    "transaction_date": {"$gte": start_utc, "$lt": end_utc},
                    "$or": [{"transaction_type": "debit"}, {"amount": {"$lt": 0}}]
                }
            },
            {
                "$set": {
                    "debitAmount": {
                        "$cond": [
                            {"$eq": ["$transaction_type", "debit"]}, 
                            {"$abs": "$amount"},
                            {"$cond": [{"$lt": ["$amount", 0]}, {"$abs": "$amount"}, 0]}
                        ]
                    }
                }
            },
            {"$sort": {"debitAmount": -1}},
            {"$limit": 10},
            {
                "$project": {
                    "_id": 0,
                    "amount": "$debitAmount",
                    "counterparty": 1,
                    "category": 1,
                    "transaction_date": 1,
                    "summary": 1
                }
            }
        ]
        
        return {
            "match_conditions": {"user_id": user_id, "transaction_date": {"$gte": start_utc, "$lt": end_utc}},
            "aggregation_pipeline": pipeline,
            "description": f"Top 10 largest transactions for {user_id}"
        }

    def _template_recurring_merchants(self, user_id: str, start_utc: datetime, end_utc: datetime) -> Dict:
        """Template 7: Recurring merchants"""
        pipeline = [
            {
                "$match": {
                    "user_id": user_id,
                    "transaction_date": {"$gte": start_utc, "$lt": end_utc},
                    "$or": [{"transaction_type": "debit"}, {"amount": {"$lt": 0}}]
                }
            },
            {
                "$set": {
                    "debitAmount": {
                        "$cond": [
                            {"$eq": ["$transaction_type", "debit"]}, 
                            {"$abs": "$amount"},
                            {"$cond": [{"$lt": ["$amount", 0]}, {"$abs": "$amount"}, 0]}
                        ]
                    }
                }
            },
            {
                "$group": {
                    "_id": "$counterparty",
                    "total_amount": {"$sum": "$debitAmount"},
                    "transaction_count": {"$sum": 1},
                    "avg_amount": {"$avg": "$debitAmount"},
                    "transactions": {
                        "$push": {
                            "amount": "$debitAmount",
                            "date": "$transaction_date",
                            "category": "$category"
                        }
                    }
                }
            },
            {"$match": {"transaction_count": {"$gte": 2}}},  # Only recurring (2+ transactions)
            {"$sort": {"total_amount": -1}},
            {"$limit": 15}
        ]
        
        return {
            "match_conditions": {"user_id": user_id, "transaction_date": {"$gte": start_utc, "$lt": end_utc}},
            "aggregation_pipeline": pipeline,
            "description": f"Recurring merchants analysis for {user_id}"
        }

    def _template_weekly_breakdown(self, user_id: str, start_utc: datetime, end_utc: datetime, timezone: str) -> Dict:
        """Template 8: Week-by-week spending breakdown"""
        pipeline = [
            {
                "$match": {
                    "user_id": user_id,
                    "transaction_date": {"$gte": start_utc, "$lt": end_utc},
                    "$or": [{"transaction_type": "debit"}, {"amount": {"$lt": 0}}]
                }
            },
            {
                "$set": {
                    "debitAmount": {
                        "$cond": [
                            {"$eq": ["$transaction_type", "debit"]}, 
                            {"$abs": "$amount"},
                            {"$cond": [{"$lt": ["$amount", 0]}, {"$abs": "$amount"}, 0]}
                        ]
                    }
                }
            },
            {
                "$set": {
                    "week": {
                        "$dateTrunc": {
                            "date": "$transaction_date", 
                            "unit": "week", 
                            "timezone": timezone
                        }
                    }
                }
            },
            {
                "$group": {
                    "_id": "$week",
                    "weekly_total": {"$sum": "$debitAmount"},
                    "transaction_count": {"$sum": 1},
                    "avg_transaction": {"$avg": "$debitAmount"}
                }
            },
            {"$sort": {"_id": 1}},
            {
                "$project": {
                    "_id": 0,
                    "week_start": "$_id",
                    "total_spending": "$weekly_total",
                    "transaction_count": 1,
                    "avg_transaction": {"$round": ["$avg_transaction", 2]}
                }
            }
        ]
        
        return {
            "match_conditions": {"user_id": user_id, "transaction_date": {"$gte": start_utc, "$lt": end_utc}},
            "aggregation_pipeline": pipeline,
            "description": f"Weekly spending breakdown for {user_id}"
        }

    def _template_spending_patterns_month(self, user_id: str, start_utc: datetime, end_utc: datetime, timezone: str) -> Dict:
        """
        NEW TEMPLATE: Patterns within a single month window
        - IST-aware daily totals
        - stats: avg/max/min day
        - weekday split (1=Sun â€¦ 7=Sat)
        """
        pipeline = [
            {
                "$match": {
                    "user_id": user_id,
                    "transaction_date": {"$gte": start_utc, "$lt": end_utc},
                    "transaction_type": "debit"  # spending only, per schema
                }
            },
            {
                "$set": {
                    "debitAmount": {"$abs": "$amount"},  # amounts are positive; keep guardrail uniform
                    "day_ist": {
                        "$dateTrunc": {"date": "$transaction_date", "unit": "day", "timezone": timezone}
                    },
                    "weekday_ist": {"$dayOfWeek": {"date": "$transaction_date", "timezone": timezone}}
                }
            },
            # Daily totals
            {
                "$group": {
                    "_id": "$day_ist",
                    "daily_total": {"$sum": "$debitAmount"},
                    "txn_count": {"$sum": 1}
                }
            },
            {"$sort": {"_id": 1}},
            # Aggregate stats over days
            {
                "$group": {
                    "_id": None,
                    "daily": {"$push": {"day": "$_id", "total": "$daily_total", "txn_count": "$txn_count"}},
                    "avg_daily": {"$avg": "$daily_total"},
                    "max_daily": {"$max": "$daily_total"},
                    "min_daily": {"$min": "$daily_total"}
                }
            },
            {
                "$set": {
                    "avg_daily": {"$round": ["$avg_daily", 2]}
                }
            },
            # Weekday split using $lookup
            {
                "$lookup": {
                    "from": "user_financial_transactions",
                    "let": {"start": start_utc, "end": end_utc, "uid": user_id, "tz": timezone},
                    "pipeline": [
                        {"$match": {
                            "$expr": {"$and": [
                                {"$eq": ["$user_id", "$$uid"]},
                                {"$eq": ["$transaction_type", "debit"]},
                                {"$gte": ["$transaction_date", "$$start"]},
                                {"$lt":  ["$transaction_date", "$$end"]}
                            ]}
                        }},
                        {"$set": {
                            "weekday_ist": {"$dayOfWeek": {"date": "$transaction_date", "timezone": timezone}},
                            "debitAmount": {"$abs": "$amount"}
                        }},
                        {"$group": {
                            "_id": "$weekday_ist",
                            "weekday_total": {"$sum": "$debitAmount"},
                            "weekday_txn": {"$sum": 1}
                        }},
                        {"$sort": {"_id": 1}}
                    ],
                    "as": "weekday_breakdown"
                }
            },
            {
                "$project": {
                    "_id": 0,
                    "daily": 1,
                    "avg_daily": 1,
                    "max_daily": 1,
                    "min_daily": 1,
                    "weekday_breakdown": 1
                }
            }
        ]

        return {
            "match_conditions": {"user_id": user_id, "transaction_date": {"$gte": start_utc, "$lt": end_utc}},
            "aggregation_pipeline": pipeline,
            "description": f"IST-aware daily/weekday spending patterns for {user_id}"
        }

    def _template_three_month_trend(self, user_id: str, timezone: str) -> Dict:
        """Template 9: 3-month spending trend"""
        # Calculate 3 months back from now in IST
        now_ist = datetime.now(IST)
        three_months_ago = now_ist - timedelta(days=90)
        
        start_utc = three_months_ago.astimezone(UTC).replace(tzinfo=None)
        end_utc = now_ist.astimezone(UTC).replace(tzinfo=None)
        
        pipeline = [
            {
                "$match": {
                    "user_id": user_id,
                    "transaction_date": {"$gte": start_utc, "$lt": end_utc},
                    "$or": [{"transaction_type": "debit"}, {"amount": {"$lt": 0}}]
                }
            },
            {
                "$set": {
                    "debitAmount": {
                        "$cond": [
                            {"$eq": ["$transaction_type", "debit"]}, 
                            {"$abs": "$amount"},
                            {"$cond": [{"$lt": ["$amount", 0]}, {"$abs": "$amount"}, 0]}
                        ]
                    }
                }
            },
            {
                "$set": {
                    "month": {
                        "$dateTrunc": {
                            "date": "$transaction_date", 
                            "unit": "month", 
                            "timezone": timezone
                        }
                    }
                }
            },
            {
                "$group": {
                    "_id": "$month",
                    "monthly_total": {"$sum": "$debitAmount"},
                    "transaction_count": {"$sum": 1},
                    "avg_transaction": {"$avg": "$debitAmount"}
                }
            },
            {"$sort": {"_id": 1}},
            {
                "$project": {
                    "_id": 0,
                    "month": {"$dateToString": {"date": "$_id", "format": "%Y-%m", "timezone": timezone}},
                    "total_spending": "$monthly_total",
                    "transaction_count": 1,
                    "avg_transaction": {"$round": ["$avg_transaction", 2]}
                }
            }
        ]
        
        return {
            "match_conditions": {"user_id": user_id},
            "aggregation_pipeline": pipeline,
            "description": f"3-month spending trend for {user_id}"
        }

    def _template_three_month_trend_windowed(self, user_id: str, start_utc: datetime, end_utc: datetime, timezone: str) -> Dict:
        """Template 9 ENHANCED: Windowed trend (respects provided time window)"""
        pipeline = [
            {
                "$match": {
                    "user_id": user_id,
                    "transaction_date": {"$gte": start_utc, "$lt": end_utc},
                    "transaction_type": "debit"
                }
            },
            {
                "$set": {
                    "debitAmount": {"$abs": "$amount"},
                    "month": {"$dateTrunc": {"date": "$transaction_date", "unit": "month", "timezone": timezone}}
                }
            },
            {
                "$group": {
                    "_id": "$month",
                    "monthly_total": {"$sum": "$debitAmount"},
                    "transaction_count": {"$sum": 1},
                    "avg_transaction": {"$avg": "$debitAmount"}
                }
            },
            {"$sort": {"_id": 1}},
            {
                "$project": {
                    "_id": 0,
                    "month": {"$dateToString": {"date": "$_id", "format": "%Y-%m", "timezone": timezone}},
                    "total_spending": "$monthly_total",
                    "transaction_count": 1,
                    "avg_transaction": {"$round": ["$avg_transaction", 2]}
                }
            }
        ]
        
        return {
            "match_conditions": {"user_id": user_id, "transaction_date": {"$gte": start_utc, "$lt": end_utc}},
            "aggregation_pipeline": pipeline,
            "description": f"Trend over requested window for {user_id}"
        }

    def _create_emergency_mongodb_query(
        self, user_id: str, sub_query: str, time_window: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Emergency fallback - basic query that always works"""
        start_utc = time_window['start_utc']
        end_utc = time_window['end_utc']
        
        pipeline = [
            {
                "$match": {
                    "user_id": user_id,
                    "transaction_date": {"$gte": start_utc, "$lt": end_utc}
                }
            },
            {"$limit": 100},
            {
                "$project": {
                    "_id": 0,
                    "amount": 1,
                    "transaction_type": 1,
                    "category": 1,
                    "counterparty": 1,
                    "transaction_date": 1,
                    "summary": 1
                }
            }
        ]
        
        return {
            "match_conditions": {"user_id": user_id, "transaction_date": {"$gte": start_utc, "$lt": end_utc}},
            "aggregation_pipeline": pipeline,
            "description": f"Emergency query for: {sub_query}",
            "emergency": True
        }

    def _log_mongodb_query(self, sub_query: str, query_data: Dict[str, Any], source: str = "llm") -> None:
        """Log pipeline in Compass-friendly way"""
        try:
            pipeline = query_data.get("aggregation_pipeline")
            if pipeline:
                def _ser(obj):
                    if hasattr(obj, "isoformat"):
                        return obj.isoformat()
                    raise TypeError(f"Not JSON serializable: {type(obj)}")
                
                logger.info(f"   ðŸ“ MongoDB Pipeline ({source.upper()}):")
                logger.info(f"   ðŸ“‹ Query: '{sub_query}'")
                logger.info(f"   ðŸ”§ Pipeline ({len(pipeline)} stages):")
                
                # Log each stage separately for better readability
                for i, stage in enumerate(pipeline, 1):
                    stage_name = list(stage.keys())[0] if stage else "unknown"
                    logger.info(f"      Stage {i}: {stage_name}")
                
                # Log full pipeline for Compass copy-paste
                logger.info(f"   ðŸ’¾ Full Pipeline (Compass-ready):")
                pipeline_json = json.dumps(pipeline, ensure_ascii=False, default=_ser, indent=2)
                for line in pipeline_json.split('\n'):
                    logger.info(f"      {line}")
                
        except Exception as e:
            logger.warning(f"   âš ï¸ Failed to log pipeline for '{sub_query}': {e}")

    # -------------------------- Parallel Execution ------------------------------

    async def _execute_parallel_with_validation(self, mongo_queries: List[Dict]) -> Dict[str, Any]:
        """
        Execute MongoDB queries in parallel (4-6 concurrent) with enhanced validation
        """
        
        async def execute_single_query(query_info: Dict) -> tuple:
            """Execute a single MongoDB query with validation"""
            sub_query = query_info["sub_query"]
            query_data = query_info["query_data"]
            source = query_info.get("source", "unknown")
            
            logger.info(f"âš¡ Executing: '{sub_query}' (source: {source})")
            
            try:
                pipeline = query_data.get("aggregation_pipeline", [])
                if not pipeline:
                    logger.warning(f"   âŒ Empty pipeline for: {sub_query}")
                    return sub_query, {"results": [], "count": 0, "error": "Empty pipeline"}
                
                # Execute the aggregation
                logger.info(f"   ðŸ” Running MongoDB aggregation...")
                results = list(financial_collection.aggregate(pipeline))
                
                # Validate and clean results
                cleaned_results = self._validate_and_clean_results(results, sub_query)
                
                logger.info(f"   âœ… Got {len(cleaned_results)} results")
                if cleaned_results:
                    logger.info(f"   ðŸ“Š Sample result: {str(cleaned_results[0])[:100]}...")
                
                return sub_query, {
                    "results": cleaned_results,
                    "count": len(cleaned_results),
                    "description": query_data.get("description", ""),
                    "source": source,
                    "confidence": query_info.get("confidence", "medium"),
                    "data_quality_score": self._calculate_data_quality_score(cleaned_results),
                    "execution_success": True
                }
                
            except Exception as e:
                logger.error(f"   âŒ Query execution failed for '{sub_query}': {e}")
                return sub_query, {
                    "results": [],
                    "count": 0,
                    "error": str(e),
                    "description": query_data.get("description", ""),
                    "source": source,
                    "confidence": "low",
                    "data_quality_score": 0.0,
                    "execution_success": False
                }
        
        # Execute all queries in parallel
        tasks = [execute_single_query(query_info) for query_info in mongo_queries]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Aggregate results
        aggregated_data = {}
        successful_queries = 0
        total_results = 0
        
        for result in results:
            if isinstance(result, tuple):
                sub_query, data = result
                aggregated_data[sub_query] = data
                if data.get("execution_success", False):
                    successful_queries += 1
                    total_results += data.get("count", 0)
            else:
                logger.error(f"Query execution exception: {result}")
        
        # Add metadata
        aggregated_data["_metadata"] = {
            "total_queries": len(mongo_queries),
            "successful_queries": successful_queries,
            "total_results": total_results,
            "success_rate": successful_queries / len(mongo_queries) if mongo_queries else 0,
            "sources": {k: v.get("source", "unknown") for k, v in aggregated_data.items() if k != "_metadata"},
            "system_health": "excellent" if successful_queries / len(mongo_queries) > 0.9 else 
                           "good" if successful_queries / len(mongo_queries) > 0.7 else "needs_attention"
        }
        
        logger.info(f"ðŸ“‹ DETAILED RESULTS SUMMARY:")
        for sub_query, data in aggregated_data.items():
            if sub_query == "_metadata":
                continue
            count = data.get("count", 0)
            source = data.get("source", "unknown")
            success = "âœ…" if data.get("execution_success", False) else "âŒ"
            logger.info(f"   {success} '{sub_query}': {count} results (source: {source})")
        
        logger.info(f"ðŸ Parallel execution completed: {successful_queries}/{len(mongo_queries)} successful")
        return aggregated_data

    def _validate_and_clean_results(self, results: List[Dict], sub_query: str) -> List[Dict]:
        """Enhanced result validation and cleaning"""
        cleaned_results = []
        
        for result in results:
            if not isinstance(result, dict):
                continue
            
            cleaned_result = {}
            
            # Handle _id field
            if "_id" in result:
                if result["_id"] is None:
                    cleaned_result["_id"] = "total"
                elif isinstance(result["_id"], dict):
                    # Handle complex _id objects (from grouping)
                    cleaned_result["_id"] = str(result["_id"])
                else:
                    cleaned_result["_id"] = str(result["_id"])
            
            # Clean and validate other fields
            for key, value in result.items():
                if key == "_id":
                    continue
                
                if isinstance(value, (int, float)):
                    # Round monetary values
                    if key in ["amount", "total", "avg", "total_spending", "monthly_total", "weekly_total", "daily_total", "total_amount"]:
                        cleaned_result[key] = round(float(value), 2)
                    else:
                        cleaned_result[key] = value
                elif isinstance(value, str):
                    cleaned_result[key] = value.strip() if value else ""
                elif isinstance(value, list):
                    # Clean array fields
                    cleaned_result[key] = [item for item in value if item is not None]
                elif isinstance(value, datetime):
                    # Convert datetime to ISO string
                    cleaned_result[key] = value.isoformat()
                else:
                    cleaned_result[key] = value
            
            cleaned_results.append(cleaned_result)
        
        logger.debug(f"Cleaned {len(cleaned_results)} results for: {sub_query}")
        return cleaned_results

    def _calculate_data_quality_score(self, results: List[Dict]) -> float:
        """Calculate enhanced data quality score"""
        if not results:
            return 0.0
        
        total_fields = 0
        valid_fields = 0
        
        for result in results:
            for key, value in result.items():
                total_fields += 1
                
                # Check field validity
                if value is not None and value != "" and value != []:
                    if isinstance(value, (int, float)):
                        valid_fields += 1 if value != 0 else 0.5  # Zero values are partially valid
                    elif isinstance(value, str):
                        valid_fields += 1 if len(value.strip()) > 0 else 0
                    elif isinstance(value, list):
                        valid_fields += 1 if len(value) > 0 else 0
                    else:
                        valid_fields += 1
        
        return round(valid_fields / total_fields, 3) if total_fields > 0 else 0.0

    # -------------------------- Grounding Context & Validation --------------

    def _create_grounding_context(
        self, aggregated_data: Dict[str, Any], time_window: Dict[str, Any], intent_analysis: Dict
    ) -> Dict[str, Any]:
        """
        Create comprehensive grounding context with intelligent deduplication
        Prevents double-counting by identifying primary vs breakdown data sources
        """
        metadata = aggregated_data.get("_metadata", {})
        
        # Smart categorization of subqueries to avoid double-counting
        primary_metrics = self._extract_primary_metrics(aggregated_data)
        contextual_data = self._extract_contextual_data(aggregated_data)
        
        grounding_context = {
            "time_period": time_window["label"],
            "start_date": time_window["start_utc"].isoformat(),
            "end_date": time_window["end_utc"].isoformat(),
            "timezone": time_window["timezone"],
            "total_spending": primary_metrics["total_spending"],
            "total_income": primary_metrics.get("total_income", 0),
            "total_transactions": primary_metrics["total_transactions"],
            "categories": contextual_data["categories"][:10],  # Limit for response size
            "merchants": contextual_data["merchants"][:10],    # Limit for response size
            "top_categories": contextual_data["top_categories"][:5],
            "top_merchants": contextual_data["top_merchants"][:5],
            "query_success_rate": metadata.get("success_rate", 0),
            "data_sources": metadata.get("sources", {}),
            "system_health": metadata.get("system_health", "unknown"),
            "intent": intent_analysis.get("intent", "general"),
            "analysis_type": intent_analysis.get("analysis_type", "summary"),
            "data_quality": {
                "primary_source_confidence": primary_metrics.get("confidence", 1.0),
                "breakdown_consistency": self._validate_breakdown_consistency(aggregated_data, primary_metrics)
            }
        }
        
        logger.debug(f"ðŸ” Grounding Context Metrics:")
        logger.debug(f"   Primary Total: â‚¹{primary_metrics['total_spending']:,.2f}")
        logger.debug(f"   Source: {primary_metrics.get('source_query', 'unknown')}")
        logger.debug(f"   Confidence: {primary_metrics.get('confidence', 1.0):.2f}")
        
        return grounding_context
    
    def _extract_primary_metrics(self, aggregated_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Intelligently extract primary financial metrics without double-counting
        Priority: Direct totals > Aggregated sums > Inferred calculations
        """
        primary_metrics = {
            "total_spending": 0,
            "total_income": 0,
            "total_transactions": 0,
            "confidence": 0.0,
            "source_query": None
        }
        
        # Define query patterns in priority order (most authoritative first)
        query_patterns = [
            # Highest priority: Direct total queries
            {
                "patterns": ["total spending", "total spend", "total expense", "spending amount", "total cost"],
                "weight": 1.0,
                "type": "spending_total"
            },
            {
                "patterns": ["total income", "total salary", "income amount", "earnings"],
                "weight": 1.0,
                "type": "income_total"
            },
            # Medium priority: Comparative queries (often contain totals)
            {
                "patterns": ["compared to", "vs", "versus", "compare", "comparison"],
                "weight": 0.9,
                "type": "comparative_total"
            },
            # Lower priority: Breakdown queries (sum of parts)
            {
                "patterns": ["break down", "breakdown", "by category", "by merchant", "categories"],
                "weight": 0.7,
                "type": "breakdown_sum"
            }
        ]
        
        best_spending_source = None
        best_income_source = None
        
        # Analyze each subquery for primary metrics
        for sub_query, data in aggregated_data.items():
            if sub_query == "_metadata":
                continue
                
            query_lower = sub_query.lower()
            results = data.get("results", [])
            source_type = data.get("source", "unknown")
            
            # Determine query type and confidence
            query_confidence = 0.0
            query_type = "unknown"
            
            for pattern_group in query_patterns:
                if any(pattern in query_lower for pattern in pattern_group["patterns"]):
                    query_confidence = pattern_group["weight"]
                    query_type = pattern_group["type"]
                    break
            
            # ENHANCED: Intelligent query type separation with MIXED QUERY handling
            is_income_query = any(word in query_lower for word in ["income", "salary", "earned", "credit", "total income"])
            is_spending_query = any(word in query_lower for word in ["spending", "expense", "expenses", "spent", "cost", "expenditure", "total expenses"])
            
            # CRITICAL: Handle MIXED queries to prevent cross-contamination
            is_mixed_query = is_income_query and is_spending_query
            
            if is_mixed_query:
                logger.warning(f"ðŸ”„ MIXED QUERY DETECTED: '{sub_query[:60]}...'")
                logger.warning("   âš ï¸ Contains both income and spending keywords - applying intelligent separation")
                
                # INTELLIGENT: Determine PRIMARY intent of mixed query
                primary_intent = self._determine_primary_intent(sub_query)
                
                if primary_intent == "spending":
                    logger.info(f"   ðŸ’¸ PRIMARY INTENT: Spending - processing as spending query only")
                    is_income_query = False  # Override to prevent cross-contamination
                elif primary_intent == "income":
                    logger.info(f"   ðŸ’° PRIMARY INTENT: Income - processing as income query only")
                    is_spending_query = False  # Override to prevent cross-contamination
                else:
                    logger.warning(f"   ðŸš« AMBIGUOUS MIXED QUERY: Skipping to prevent data corruption")
                    continue  # Skip this query entirely to prevent contamination
            
            # Extract spending totals (ONLY from spending queries, excluding mixed)
            if is_spending_query and not is_mixed_query:
                spending_candidates = self._extract_spending_from_results(results, query_type)
                for candidate in spending_candidates:
                    if not best_spending_source or candidate["confidence"] * query_confidence > best_spending_source["confidence"]:
                        best_spending_source = {
                            "amount": candidate["amount"],
                            "confidence": candidate["confidence"] * query_confidence,
                            "source_query": sub_query,
                            "source_type": source_type,
                            "query_type": query_type
                        }
                        logger.info(f"ðŸ’° NEW BEST SPENDING: â‚¹{candidate['amount']:,.2f} from '{sub_query[:50]}...' (confidence: {candidate['confidence'] * query_confidence:.2f})")
            
            # Extract income totals (ONLY from income queries, excluding mixed) with SMART prioritization
            if is_income_query and not is_mixed_query:
                income_candidates = self._extract_income_from_results(results, query_type)
                for candidate in income_candidates:
                    # BULLETPROOF: Salary queries ALWAYS win, then highest amount
                    is_salary_query = "salary" in sub_query.lower()
                    is_current_salary = best_income_source and "salary" in best_income_source.get("source_query", "").lower()
                
                    # RULE 1: Salary queries ALWAYS have priority
                    if is_salary_query and not is_current_salary:
                        should_replace = True  # Salary always beats non-salary
                    elif is_current_salary and not is_salary_query:
                        should_replace = False  # Keep salary over non-salary
                    elif is_salary_query and is_current_salary:
                        # Both are salary queries - choose higher amount
                        should_replace = candidate["amount"] > best_income_source["amount"]
                    else:
                        # Both are non-salary - use confidence and amount
                        candidate_score = candidate["confidence"] * query_confidence
                        current_score = best_income_source["confidence"] if best_income_source else 0
                        
                        # If confidence is similar, prefer higher amount
                        if abs(candidate_score - current_score) < 0.1:
                            should_replace = candidate["amount"] > best_income_source.get("amount", 0)
                        else:
                            should_replace = candidate_score > current_score
                    
                    if not best_income_source or should_replace:
                        final_confidence = candidate["confidence"] * query_confidence
                        best_income_source = {
                            "amount": candidate["amount"],
                            "confidence": final_confidence,
                            "source_query": sub_query,
                            "source_type": source_type,
                            "query_type": query_type,
                            "is_salary": is_salary_query
                        }
                        logger.info(f"ðŸ’µ NEW BEST INCOME: â‚¹{candidate['amount']:,.2f} from '{sub_query[:50]}...' (salary: {is_salary_query}, confidence: {final_confidence:.2f})")
            
            # Extract transaction counts (additive, but avoid double counting)
            tx_count = self._extract_transaction_count(results, query_type)
            if tx_count > primary_metrics["total_transactions"]:
                primary_metrics["total_transactions"] = tx_count
        
        # Set the best sources with detailed logging
        if best_spending_source:
            primary_metrics["total_spending"] = round(best_spending_source["amount"], 2)
            primary_metrics["confidence"] = best_spending_source["confidence"]
            primary_metrics["source_query"] = best_spending_source["source_query"]
            primary_metrics["source_type"] = best_spending_source.get("source_type", "unknown")
            logger.info(f"ðŸ’° SPENDING EXTRACTED: â‚¹{primary_metrics['total_spending']:,.2f} from '{best_spending_source['source_query'][:50]}...' (confidence: {best_spending_source['confidence']:.2f})")
        else:
            logger.warning("âš ï¸ NO SPENDING DATA FOUND - Check MongoDB aggregation results")
        
        if best_income_source:
            primary_metrics["total_income"] = round(best_income_source["amount"], 2)
            is_salary_source = best_income_source.get("is_salary", False)
            logger.info(f"ðŸ’µ FINAL INCOME SELECTED: â‚¹{primary_metrics['total_income']:,.2f} from '{best_income_source['source_query'][:50]}...' (salary: {is_salary_source}, confidence: {best_income_source['confidence']:.2f})")
        else:
            logger.warning("âš ï¸ NO INCOME DATA FOUND - Check MongoDB aggregation results")
        
        # Debug: Log all aggregated data for analysis
        logger.info("ðŸ” AGGREGATED DATA SUMMARY:")
        logger.info("ðŸ” INCOME CANDIDATE ANALYSIS:")
        for sub_query, data in aggregated_data.items():
            if sub_query != "_metadata":
                results = data.get("results", [])
                is_income_query = any(word in sub_query.lower() for word in ["income", "salary", "earned", "credit"])
                is_salary_query = "salary" in sub_query.lower()
                
                logger.info(f"   ðŸ“Š '{sub_query[:60]}...': {len(results)} results (income: {is_income_query}, salary: {is_salary_query})")
                
                if results and is_income_query:
                    # Extract and log income candidates for this query
                    query_type = data.get("source", "unknown") 
                    income_candidates = self._extract_income_from_results(results, query_type)
                    for i, candidate in enumerate(income_candidates):
                        logger.info(f"      ðŸ’µ Candidate {i+1}: â‚¹{candidate['amount']:,.2f} (confidence: {candidate['confidence']:.2f}, field: {candidate['field']})")
                
                if results:
                    # Show first result structure
                    first_result = results[0]
                    numeric_fields = {k: v for k, v in first_result.items() if isinstance(v, (int, float)) and v > 0}
                    logger.info(f"      ðŸ’¹ Numeric fields: {numeric_fields}")
        
        return primary_metrics
    
    def _detect_breakdown_query(self, results: List[Dict], query_type: str) -> bool:
        """
        ENHANCED: Detect if results represent a breakdown that needs aggregation
        """
        if not results or len(results) < 2:
            return False
        
        # Check if results have breakdown indicators
        breakdown_indicators = []
        total_amount = 0
        
        for result in results:
            _id = result.get("_id", "")
            
            # Breakdown indicators: Multiple distinct categories/merchants/periods
            if _id and _id not in ["total", "sum", "aggregate", None, ""]:
                breakdown_indicators.append(str(_id))
                
                # Also check if there are meaningful amounts
                for field in ["totalAmount", "amount", "total_amount"]:
                    if field in result and isinstance(result[field], (int, float)) and result[field] > 0:
                        total_amount += result[field]
                        break
        
        # Enhanced detection criteria
        unique_identifiers = len(set(breakdown_indicators))
        has_meaningful_amounts = total_amount > 0
        
        # Strong indicators of breakdown data
        breakdown_detected = False
        
        if unique_identifiers >= 2 and has_meaningful_amounts:
            breakdown_detected = True
            logger.info(f"ðŸ” BREAKDOWN DETECTED: {unique_identifiers} distinct items, total â‚¹{total_amount:,.2f}")
            logger.info(f"   ðŸ“‹ Items: {breakdown_indicators[:5]}{'...' if len(breakdown_indicators) > 5 else ''}")
        
        # Additional check: Look for merchant names, categories, or date patterns
        elif unique_identifiers >= 2:
            # Check if identifiers look like merchants, categories, or dates
            merchant_like = any(len(str(item)) > 3 and any(c.isalpha() for c in str(item)) 
                              for item in breakdown_indicators)
            if merchant_like:
                breakdown_detected = True
                logger.info(f"ðŸ” MERCHANT/CATEGORY BREAKDOWN DETECTED: {unique_identifiers} items")
        
        return breakdown_detected
    
    def _determine_primary_intent(self, query: str) -> str:
        """
        INTELLIGENT: Determine the primary intent of a mixed query to prevent cross-contamination
        """
        query_lower = query.lower()
        
        # Count keyword density and context
        spending_indicators = ["expense", "expenses", "spending", "spent", "cost", "expenditure"]
        income_indicators = ["income", "salary", "earned", "earnings", "credit"]
        
        spending_score = sum(1 for indicator in spending_indicators if indicator in query_lower)
        income_score = sum(1 for indicator in income_indicators if indicator in query_lower)
        
        # Context-based scoring (what's the query PRIMARILY about?)
        if "compare" in query_lower and "expenses" in query_lower:
            # "compare expenses to income" = PRIMARY focus on expenses
            spending_score += 2
        
        if "fluctuations" in query_lower:
            # "fluctuations in income or expenses" = ambiguous, but often about spending volatility
            spending_score += 1
            
        if query_lower.startswith("what are my") and "expenses" in query_lower:
            # "What are my expenses..." = clearly about spending
            spending_score += 3
            
        if "monthly expenses" in query_lower:
            # "monthly expenses" = clearly spending focus
            spending_score += 2
        
        # CRITICAL: Handle risk profiling and financial assessment queries
        if any(phrase in query_lower for phrase in ["financial stability", "risk", "assessment", "profiling", "stability", "correlate"]):
            # Risk profiling needs BOTH income and spending data - treat as dual-purpose
            # But prioritize income since financial stability depends on income
            income_score += 3
            logger.info(f"ðŸŽ¯ RISK/ASSESSMENT QUERY: Prioritizing income extraction for financial stability analysis")
        
        if ("total income and expenses" in query_lower or 
            "income and expenses" in query_lower or 
            "income stability" in query_lower or
            "income" in query_lower and "spending" in query_lower):
            # When both are explicitly mentioned together, prioritize income for stability analysis
            income_score += 2
            logger.info(f"ðŸŽ¯ DUAL QUERY: Prioritizing income for comprehensive financial analysis")
        
        # SPECIFIC: Handle income stability queries (common in risk profiling)
        if "income stability" in query_lower or "monthly income" in query_lower:
            income_score += 4  # Strong income focus
            logger.info(f"ðŸŽ¯ INCOME STABILITY QUERY: Strong income prioritization")
        
        # Determine primary intent
        if spending_score > income_score:
            return "spending"
        elif income_score > spending_score:
            return "income"
        else:
            return "ambiguous"  # Skip to prevent contamination
    
    def _get_minimum_days_for_query_type(self, query_type: str, query_lower: str) -> int:
        """
        INTELLIGENT: Determine minimum required days based on query type for accurate analysis
        """
        if not query_type:
            return 30  # Default
            
        # Risk profiling needs volatility assessment - minimum 6 months
        if query_type == "risk_profiling":
            return 180
            
        # Credit assessment needs comprehensive income stability - minimum 6 months
        if query_type == "credit_assessment":
            return 180
            
        # Behavioral analysis needs pattern recognition - minimum 3 months
        if query_type == "behavioral_analysis":
            return 90
            
        # Trend analysis needs historical comparison - minimum 6 months
        if query_type == "trend_analysis":
            return 180
            
        # Period analysis - check for specific periods
        if query_type == "period_analysis":
            if "2025" in query_lower:
                return 365  # Full year analysis
            return 90  # Default for period comparisons
            
        # Default for income/spending analysis
        return 30
    
    def _extract_spending_from_results(self, results: List[Dict], query_type: str) -> List[Dict]:
        """Extract spending amounts with INTELLIGENT AGGREGATION - handles both totals and breakdowns"""
        candidates = []
        
        if not results:
            return candidates
        
        # INTELLIGENT: Detect if this is a breakdown query that needs aggregation
        needs_aggregation = self._detect_breakdown_query(results, query_type)
        
        if needs_aggregation:
            # SMART AGGREGATION: Sum all breakdown amounts
            total_amount = 0
            breakdown_count = 0
            
            for result in results:
                # Look for amount fields in breakdown results
                amount_fields = [
                    "totalAmount", "total_amount", "amount", "debitAmount", 
                    "spending", "expense", "cost", "sum"
                ]
                
                for field in amount_fields:
                    if field in result and isinstance(result[field], (int, float)) and result[field] > 0:
                        total_amount += result[field]
                        breakdown_count += 1
                        break
            
            if total_amount > 0:
                candidates.append({
                    "amount": total_amount,
                    "confidence": 0.95,  # High confidence for aggregated breakdown
                    "field": "aggregated_breakdown",
                    "type": "breakdown_sum",
                    "breakdown_count": breakdown_count
                })
                logger.info(f"ðŸ’¡ SMART AGGREGATION: Summed {breakdown_count} breakdown items = â‚¹{total_amount:,.2f}")
                return candidates  # Return aggregated result, don't process individual items
        
        for result in results:
            # High confidence: Direct total fields
            high_confidence_fields = [
                "totalSpending", "total_spending", "total_amount", "totalAmount",
                "debitAmount", "spending_total", "expense_total", "sum", "total"
            ]
            
            for field in high_confidence_fields:
                if field in result and isinstance(result[field], (int, float)) and result[field] > 0:
                    candidates.append({
                        "amount": result[field],
                        "confidence": 1.0,
                        "field": field,
                        "type": "direct_total"
                    })
            
            # Enhanced: Intelligent field detection for any numeric field
            for key, value in result.items():
                if isinstance(value, (int, float)) and value > 0:
                    key_lower = key.lower()
                    
                    # Skip if already processed in high confidence
                    if key in high_confidence_fields:
                        continue
                    
                    # CRITICAL: Skip average, weekly, daily, and ratio fields for total spending
                    skip_patterns = [
                        "average", "avg", "mean", "weekly", "daily", "monthly", 
                        "ratio", "rate", "percentage", "percent", "per"
                    ]
                    if any(pattern in key_lower for pattern in skip_patterns):
                        logger.debug(f"âš ï¸ SKIPPING field '{key}' with value {value} - contains average/ratio pattern")
                        continue
                    
                    # High confidence spending indicators
                    if any(indicator in key_lower for indicator in ["spend", "expense", "cost", "debit"]):
                        candidates.append({
                            "amount": value,
                            "confidence": 0.95,
                            "field": key,
                            "type": "spending_indicator"
                        })
                    
                    # Medium confidence: Amount-like fields
                    elif key_lower in ["amount", "value"] or "amount" in key_lower:
                        confidence = 0.8 if result.get("_id") == "total" else 0.6
                        candidates.append({
                            "amount": value,
                            "confidence": confidence,
                            "field": key,
                            "type": "amount_field"
                        })
        
        # CRITICAL: If breakdown query, sum up individual amounts
        if query_type == "breakdown_sum" and len(results) > 1:
            total_sum = 0
            valid_amounts = []
            
            for result in results:
                # Find the best amount field for this result
                best_amount = 0
                for key, value in result.items():
                    if isinstance(value, (int, float)) and value > 0:
                        if any(indicator in key.lower() for indicator in ["amount", "total", "sum", "spend", "expense", "debit"]):
                            best_amount = max(best_amount, value)  # Take the highest if multiple
                
                if best_amount > 0:
                    valid_amounts.append(best_amount)
                    total_sum += best_amount
            
            if total_sum > 0:
                candidates.append({
                    "amount": total_sum,
                    "confidence": 0.9,  # High confidence for breakdown sums
                    "field": "breakdown_calculated_total",
                    "type": "breakdown_sum_total"
                })
        
        return candidates
    
    def _extract_income_from_results(self, results: List[Dict], query_type: str) -> List[Dict]:
        """Extract income amounts with INTELLIGENT AGGREGATION - handles both totals and breakdowns"""
        candidates = []
        
        if not results:
            return candidates
        
        # INTELLIGENT: Detect if this is a breakdown query that needs aggregation
        needs_aggregation = self._detect_breakdown_query(results, query_type)
        
        if needs_aggregation:
            # SMART AGGREGATION: Sum all breakdown amounts
            total_amount = 0
            breakdown_count = 0
            
            for result in results:
                # Look for amount fields in breakdown results
                amount_fields = [
                    "totalAmount", "total_amount", "amount", "creditAmount", 
                    "income", "salary", "earnings", "sum"
                ]
                
                for field in amount_fields:
                    if field in result and isinstance(result[field], (int, float)) and result[field] > 0:
                        total_amount += result[field]
                        breakdown_count += 1
                        break
            
            if total_amount > 0:
                candidates.append({
                    "amount": total_amount,
                    "confidence": 0.95,  # High confidence for aggregated breakdown
                    "field": "aggregated_breakdown",
                    "type": "breakdown_sum",
                    "breakdown_count": breakdown_count
                })
                logger.info(f"ðŸ’° SMART INCOME AGGREGATION: Summed {breakdown_count} breakdown items = â‚¹{total_amount:,.2f}")
                return candidates  # Return aggregated result, don't process individual items
        
        for result in results:
            # High confidence: Direct income fields
            high_confidence_fields = [
                "totalIncome", "total_income", "salary_amount", "income_amount",
                "totalAmount", "total_amount", "creditAmount", "sum", "total"
            ]
            
            for field in high_confidence_fields:
                if field in result and isinstance(result[field], (int, float)) and result[field] > 0:
                    candidates.append({
                        "amount": result[field],
                        "confidence": 1.0,
                        "field": field,
                        "type": "direct_income"
                    })
            
            # Enhanced: Intelligent field detection for income
            for key, value in result.items():
                if isinstance(value, (int, float)) and value > 0:
                    key_lower = key.lower()
                    
                    # Skip if already processed
                    if key in high_confidence_fields:
                        continue
                    
                    # High confidence income indicators
                    if any(indicator in key_lower for indicator in ["income", "salary", "credit", "earn"]):
                        candidates.append({
                            "amount": value,
                            "confidence": 0.95,
                            "field": key,
                            "type": "income_indicator"
                        })
                    
                    # Medium confidence: Amount fields in credit contexts
                    elif key_lower in ["amount", "value"] or "amount" in key_lower:
                        # Higher confidence if this is clearly income/credit
                        confidence = 0.8 if (
                            result.get("transaction_type") == "credit" or 
                            any(keyword in str(result).lower() for keyword in ["salary", "income", "credit", "deposit"]) or
                            result.get("_id") == "total"
                        ) else 0.5
                        
                        candidates.append({
                            "amount": value,
                            "confidence": confidence,
                            "field": key,
                            "type": "amount_field"
                        })
        
        # CRITICAL: If breakdown query, sum up individual amounts
        if query_type == "breakdown_sum" and len(results) > 1:
            total_sum = 0
            valid_amounts = []
            
            for result in results:
                # Find the best amount field for this result
                best_amount = 0
                for key, value in result.items():
                    if isinstance(value, (int, float)) and value > 0:
                        if any(indicator in key.lower() for indicator in ["amount", "total", "sum", "income", "salary", "credit"]):
                            best_amount = max(best_amount, value)
                
                if best_amount > 0:
                    valid_amounts.append(best_amount)
                    total_sum += best_amount
            
            if total_sum > 0:
                candidates.append({
                    "amount": total_sum,
                    "confidence": 0.9,
                    "field": "breakdown_calculated_total", 
                    "type": "breakdown_sum_total"
                })
        
        return candidates
    
    def _extract_transaction_count(self, results: List[Dict], query_type: str) -> int:
        """Extract transaction count, avoiding double counting"""
        max_count = 0
        
        for result in results:
            # Look for transaction count fields
            for field in ["transaction_count", "txn_count", "count", "total_transactions"]:
                if field in result and isinstance(result[field], (int, float)):
                    count = int(result[field])
                    if count > max_count:
                        max_count = count
            
            # For breakdown queries, sum individual counts might be more accurate
            if query_type == "breakdown_sum" and "count" in result:
                max_count = max(max_count, int(result.get("count", 0)))
        
        return max_count
    
    def _extract_contextual_data(self, aggregated_data: Dict[str, Any]) -> Dict[str, Any]:
        """Extract categories, merchants, and other contextual data"""
        categories = set()
        merchants = set()
        category_amounts = {}
        merchant_amounts = {}
        
        for sub_query, data in aggregated_data.items():
            if sub_query == "_metadata":
                continue
            
            results = data.get("results", [])
            query_lower = sub_query.lower()
            
            for result in results:
                # Extract categories
                if "category" in result and result["category"]:
                    cat = str(result["category"])
                    categories.add(cat)
                    # Track amounts for ranking
                    if "amount" in result or "total_amount" in result:
                        amount = result.get("amount", result.get("total_amount", 0))
                        if isinstance(amount, (int, float)):
                            category_amounts[cat] = category_amounts.get(cat, 0) + amount
                
                # Extract merchants/counterparties
                for field in ["counterparty", "merchant", "merchant_canonical", "_id"]:
                    if field in result and result[field] and isinstance(result[field], str):
                        merchant = str(result[field])
                        # Skip generic IDs and dates
                        if not any(skip in merchant.lower() for skip in ["total", "unknown", "2024", "2025", ":"]):
                            merchants.add(merchant)
                            # Track amounts for ranking
                            if "amount" in result or "total_amount" in result or "totalSpent" in result:
                                amount = result.get("amount", result.get("total_amount", result.get("totalSpent", 0)))
                                if isinstance(amount, (int, float)):
                                    merchant_amounts[merchant] = merchant_amounts.get(merchant, 0) + amount
                
                # Extract from _id field if it looks like a category
                if "_id" in result and isinstance(result["_id"], str) and result["_id"] != "total":
                    id_val = result["_id"]
                    # If it's not a date/timestamp, treat as category
                    if not any(char in id_val for char in [":", "-", "T", "Z"]) and len(id_val) < 50:
                        categories.add(id_val)
        
        # Sort by amounts for top lists
        top_categories = sorted(category_amounts.items(), key=lambda x: x[1], reverse=True)
        top_merchants = sorted(merchant_amounts.items(), key=lambda x: x[1], reverse=True)
        
        return {
            "categories": list(categories),
            "merchants": list(merchants),
            "top_categories": [{"name": cat, "amount": amt} for cat, amt in top_categories],
            "top_merchants": [{"name": merchant, "amount": amt} for merchant, amt in top_merchants]
        }
    
    def _validate_breakdown_consistency(self, aggregated_data: Dict[str, Any], primary_metrics: Dict[str, Any]) -> float:
        """
        Validate that breakdown totals are consistent with primary totals
        Returns consistency score (0.0 to 1.0)
        """
        primary_total = primary_metrics.get("total_spending", 0)
        if primary_total <= 0:
            return 1.0  # No primary total to validate against
        
        breakdown_totals = []
        
        # Find breakdown queries and sum their components
        for sub_query, data in aggregated_data.items():
            if sub_query == "_metadata":
                continue
            
            query_lower = sub_query.lower()
            if any(pattern in query_lower for pattern in ["break down", "breakdown", "by category", "by merchant"]):
                results = data.get("results", [])
                breakdown_sum = 0
                
                for result in results:
                    for field in ["amount", "total_amount", "totalSpent"]:
                        if field in result and isinstance(result[field], (int, float)):
                            breakdown_sum += result[field]
                            break
                
                if breakdown_sum > 0:
                    breakdown_totals.append(breakdown_sum)
        
        if not breakdown_totals:
            return 1.0  # No breakdowns to validate
        
        # Calculate consistency score
        consistency_scores = []
        for breakdown_total in breakdown_totals:
            # Allow 5% variance for rounding/aggregation differences
            variance = abs(primary_total - breakdown_total) / primary_total
            consistency = max(0.0, 1.0 - (variance / 0.05))  # Perfect score within 5%
            consistency_scores.append(consistency)
        
        return sum(consistency_scores) / len(consistency_scores) if consistency_scores else 1.0

    # -------------------------- Response Generation with Validation --------------

    async def _generate_grounded_response(
        self, original_query: str, intent_analysis: Dict, 
        aggregated_data: Dict[str, Any], grounding_context: Dict[str, Any]
    ) -> str:
        """
        Generate response with post-fact validation against grounding context
        """
        if not self.groq_available:
            return self._generate_deterministic_response(original_query, aggregated_data, grounding_context)
        
        try:
            # Create comprehensive data summary for LLM
            data_summary = self._create_comprehensive_data_summary(aggregated_data)
            
            messages = [
                {
                    "role": "system",
                    "content": """You are a world-class financial intelligence analyst who reveals profound insights that blow users' minds. Combine engaging storytelling with deep analytical intelligence that makes users think "I never realized this about my money!"

INTELLIGENCE FOCUS:
- Reveal hidden patterns and financial DNA ðŸ§¬
- Provide specific, actionable insights ðŸ“Š
- Calculate meaningful ratios and benchmarks ðŸ“ˆ
- Identify behavioral patterns and triggers ðŸŽ¯
- Deliver mind-blowing realizations âœ¨
- Give clear financial recommendations ðŸ’¡

ANALYSIS DEPTH:
- Calculate percentiles vs average population
- Identify spending efficiency and optimization opportunities
- Reveal behavioral psychology patterns
- Provide specific next-step recommendations
- Include comparative benchmarks and context
- End with clear summary and action items

STRUCTURE:
# ðŸŽ¯ [Insight-Driven Title]

[Brief celebration + most surprising insight]

## ðŸ’¡ **Hidden Patterns Revealed**
[Deep financial behavior analysis with specific insights]

## ðŸ“Š **Key Insights**
â€¢ **[Category]**: [Specific insight with numbers and context]
â€¢ **[Pattern]**: [Behavioral insight with implications]
â€¢ **[Opportunity]**: [Actionable optimization recommendation]

## ðŸŽ¯ **Smart Moves**
[2-3 specific, actionable recommendations]

---
**Bottom Line**: [1-2 sentence powerful summary of their financial profile and biggest opportunity]

CRITICAL REQUIREMENTS:
- ALL amounts in INR (â‚¹)
- Use exact data from grounding_context
- Focus on INSIGHTS over entertainment
- Provide specific percentiles, ratios, and benchmarks
- 300-400 words max
- End with clear summary and actionable next steps"""
                },
                {
                    "role": "user",
                    "content": f"""
User Query: "{original_query}"

GROUNDING CONTEXT:
{json.dumps(grounding_context, indent=2)}

DATA SUMMARY:
{data_summary}

CRITICAL MATHEMATICAL AWARENESS:
- Time Period: {grounding_context.get('time_period', 'Unknown')}
- Total Income: â‚¹{grounding_context.get('total_income', 0):,.2f}
- Total Spending: â‚¹{grounding_context.get('total_spending', 0):,.2f}
- NEVER assume 6 months unless time_period explicitly states it
- Calculate monthly averages based on ACTUAL time period, not assumptions
- If time period is "Last 30 days", do NOT claim "6 months analysis"

CRITICAL CONTEXT AWARENESS & DATA INTELLIGENCE:
- If spending is unusually LOW (under â‚¹5,000/month), acknowledge this is atypical and explain possible reasons
- If spending is unusually HIGH (over â‚¹100,000/month), note this as significant
- Compare current period to user's typical patterns when possible
- Don't generalize from one unusual month to overall financial behavior
- If data shows dramatic changes between periods, highlight this pattern

RELEVANCE-BASED INFORMATION DISPLAY:
- ONLY mention income if the query is about income, salary, earnings, or financial health/stability
- If query is purely about spending patterns, categories, or merchants - DO NOT mention income at all
- If income = â‚¹0 and query isn't about income, completely ignore income data
- Focus ONLY on relevant metrics that answer the user's specific question

DATA QUALITY AWARENESS:
- If spending seems unrealistically low (under â‚¹10/day), explain this might be due to:
  * Limited data in the time period
  * Primarily cash-based spending not tracked
  * Unusual period with minimal activity
- If data seems incomplete, acknowledge limitations rather than making bold claims

Create a MIND-BLOWING financial intelligence analysis (300-400 words) that reveals profound insights:

ðŸ§  **INTELLIGENCE GOALS:**
1. Reveal the MOST surprising insight in their data
2. Calculate specific ratios, percentiles, and benchmarks
3. Identify behavioral patterns and financial DNA
4. Provide actionable optimization opportunities
5. Include comparative context and implications
6. End with clear summary and smart moves

ðŸ“Š **DEEP ANALYSIS:**
1. Calculate spending efficiency ratios and percentiles
2. Identify behavioral psychology patterns and triggers
3. Reveal hidden optimization opportunities
4. Provide specific benchmarks vs average population
5. Give precise next-step recommendations
6. Include clear financial profile assessment

ðŸ’¡ **INSIGHT REQUIREMENTS:**
- Use exact amounts from grounding_context
- Calculate meaningful ratios and comparisons
- Identify specific behavioral patterns
- Provide actionable recommendations with numbers
- Include percentile rankings and benchmarks
- End with powerful summary line

GOAL: User thinks "Holy shit, I never realized this about my finances!" and immediately wants to take action.
                    """
                }
            ]
            
            response = call_ai_model(messages, temperature=0.2)  # Use default model for current provider
            if response:
                return response.strip()
            else:
                return self._generate_deterministic_response(original_query, aggregated_data, grounding_context)
                
        except Exception as e:
            logger.error(f"Response generation failed: {e}")
            return self._generate_deterministic_response(original_query, aggregated_data, grounding_context)
    
    async def _generate_final_refined_response(
        self, 
        original_query: str, 
        initial_response: str, 
        grounding_context: Dict[str, Any], 
        sub_queries: List[str], 
        intent_analysis: Dict[str, Any]
    ) -> str:
        """
        FINAL REFINEMENT: Generate ultra-precise response based on complete context
        """
        try:
            # Create complete context JSON for final refinement
            complete_context = {
                "user_query": original_query,
                "initial_response": initial_response,
                "sub_queries": sub_queries,
                "grounding_context": grounding_context,
                "intent_analysis": intent_analysis
            }
            
            messages = [
                {
                    "role": "system",
                    "content": """You are an ULTRA-PRECISE financial intelligence system performing FINAL REFINEMENT.

Your mission: Transform the initial response into a PERFECTLY TARGETED answer that directly addresses the user's specific question.

CRITICAL REFINEMENT PRINCIPLES:
1. **Query-Specific Precision**: Tailor everything to the exact user question
2. **Perfect Bottom Line**: Create 1-2 sentence summary that DIRECTLY answers their question
3. **Insane Accuracy**: Use exact data from grounding context
4. **Zero Fluff**: Remove generic advice, focus on their specific situation
5. **Actionable Clarity**: Make recommendations crystal clear and specific

BOTTOM LINE REQUIREMENTS:
- Must DIRECTLY answer the user's question in 1-2 sentences
- Use specific numbers and percentages from the data
- Give clear YES/NO or specific recommendations when applicable
- Examples:
  * Loan Query â†’ "âœ… **RECOMMENDATION**: Yes, approve loan - income â‚¹X, spending â‚¹Y, repayment capacity Z%"
  * Spending Query â†’ "ðŸ’¡ **INSIGHT**: You spent â‚¹X in July, which is Y% higher than average - optimize Z category"
  * Risk Query â†’ "ðŸŽ¯ **RISK LEVEL**: Low risk - savings rate X%, emergency fund covers Y months"

REFINEMENT FOCUS:
- Keep the engaging tone and insights
- Sharpen precision and relevance
- Perfect the bottom line summary
- Ensure every sentence serves the user's specific question"""
                },
                {
                    "role": "user",
                    "content": f"""
COMPLETE CONTEXT FOR FINAL REFINEMENT:

{json.dumps(complete_context, indent=2)}

REFINEMENT TASK:
Transform the initial response into an ULTRA-PRECISE, QUERY-SPECIFIC analysis that:
1. Maintains the engaging insights and structure
2. Ensures every detail serves the user's exact question
3. Creates a PERFECT bottom line that directly answers their query
4. Uses precise data from grounding context
5. Provides crystal-clear recommendations

Focus on making this response feel like it was written SPECIFICALLY for their exact question, not a generic financial analysis.
                    """
                }
            ]
            
            refined_response = call_ai_model(messages, temperature=0.1)  # Low temperature for precision
            
            if refined_response:
                logger.info("âœ¨ FINAL REFINEMENT COMPLETED - Ultra-precise response generated")
                return refined_response.strip()
            else:
                logger.warning("âš ï¸ Final refinement failed - returning initial response")
                return initial_response
                
        except Exception as e:
            logger.error(f"Final refinement failed: {e}")
            logger.info("ðŸ”„ Fallback: Returning initial response")
            return initial_response

    def _create_comprehensive_data_summary(self, aggregated_data: Dict[str, Any]) -> str:
        """Create SMART data summary for LLM with token optimization"""
        summary_parts = []
        total_tokens = 0
        MAX_TOKENS = 50000  # Safe limit to prevent overflow
        
        for sub_query, data in aggregated_data.items():
            if sub_query == "_metadata":
                continue
            
            count = data.get("count", 0)
            if count > 0:
                query_summary = f"\n{sub_query}:"
                results = data.get("results", [])
                
                # SMART SUMMARIZATION: Extract only key financial metrics
                result_summaries = []
                for i, result in enumerate(results[:5]):  # Max 5 results per query
                    smart_summary = self._extract_key_metrics_from_result(result, i+1)
                    result_summaries.append(smart_summary)
                
                if len(results) > 5:
                    result_summaries.append(f"  ... and {len(results) - 5} more results")
                
                # Estimate tokens (roughly 4 chars per token)
                section_content = query_summary + "\n" + "\n".join(result_summaries)
                section_tokens = len(section_content) // 4
                
                # Only add if within token budget
                if total_tokens + section_tokens < MAX_TOKENS:
                    summary_parts.append(query_summary)
                    summary_parts.extend(result_summaries)
                    total_tokens += section_tokens
                else:
                    summary_parts.append(f"\n[TRUNCATED] {sub_query}: {count} results (token limit reached)")
                    break
        
        if total_tokens > MAX_TOKENS * 0.8:  # Warn if approaching limit
            summary_parts.insert(0, f"[TOKEN OPTIMIZED: {total_tokens:,} tokens]")
        
        return "\n".join(summary_parts) if summary_parts else "No significant data found."
    
    def _extract_key_metrics_from_result(self, result: Dict, index: int) -> str:
        """Extract only essential financial metrics from MongoDB result"""
        key_fields = []
        
        # Extract amounts (most important)
        for amount_field in ['totalAmount', 'amount', 'total_amount', 'debitAmount']:
            if amount_field in result and isinstance(result[amount_field], (int, float)):
                key_fields.append(f"â‚¹{result[amount_field]:,.2f}")
                break
        
        # Extract identifiers (category, merchant, date)
        identifier = None
        for id_field in ['_id', 'category', 'counterparty', 'merchant']:
            if id_field in result and result[id_field]:
                identifier = str(result[id_field])
                if len(identifier) > 30:  # Truncate long identifiers
                    identifier = identifier[:27] + "..."
                break
        
        # Extract transaction count
        count = result.get('transactionCount', result.get('count', ''))
        if count:
            key_fields.append(f"{count} txns")
        
        # Build compact summary
        if identifier and key_fields:
            return f"  {index}. {identifier}: {', '.join(key_fields)}"
        elif key_fields:
            return f"  {index}. {', '.join(key_fields)}"
        else:
            return f"  {index}. [No key metrics found]"

    def _generate_deterministic_response(
        self, original_query: str, aggregated_data: Dict[str, Any], grounding_context: Dict[str, Any]
    ) -> str:
        """
        Generate deterministic response when LLM is unavailable
        """
        time_period = grounding_context.get("time_period", "the specified period")
        total_spending = grounding_context.get("total_spending", 0)
        total_transactions = grounding_context.get("total_transactions", 0)
        
        response_parts = [
            f"ðŸ“Š **Financial Analysis for {time_period}**",
            f"",
            f"**Summary:**",
            f"â€¢ Total spending: â‚¹{total_spending:,.2f}",
            f"â€¢ Total transactions: {total_transactions}",
            f"â€¢ Average per transaction: â‚¹{(total_spending/total_transactions):,.2f}" if total_transactions > 0 else "",
            f"",
            f"**Detailed Breakdown:**"
        ]
        
        # Add details from each sub-query
        for sub_query, data in aggregated_data.items():
            if sub_query == "_metadata":
                continue
            
            count = data.get("count", 0)
            if count > 0:
                response_parts.append(f"â€¢ {sub_query}: {count} item(s)")
                
                # Show top results
                results = data.get("results", [])
                for result in results[:2]:
                    if "total_amount" in result:
                        response_parts.append(f"  - {result.get('_id', 'Item')}: â‚¹{result['total_amount']:,.2f}")
                    elif "amount" in result:
                        response_parts.append(f"  - {result.get('counterparty', 'Transaction')}: â‚¹{result['amount']:,.2f}")
        
        response_parts.extend([
            f"",
            f"**System Status:** {grounding_context.get('system_health', 'unknown').title()}",
            f"**Data Quality:** {grounding_context.get('query_success_rate', 0)*100:.1f}% queries successful"
        ])
        
        return "\n".join(response_parts)

    def _classify_financial_query_type(self, query: str, intent_analysis: Dict) -> str:
        """
        INTELLIGENT: Classify the type of financial query for adaptive processing
        """
        query_lower = query.lower()
        
        # Credit Assessment Queries
        if any(keyword in query_lower for keyword in [
            "credit card", "loan", "mortgage", "creditworthiness", "borrow", 
            "debt", "financial health", "recommended", "should i", "can i afford",
            "lend", "lending", "repay", "repayment", "return that money", "pay back",
            "interest", "6 percent", "6%", "able to return", "will it be able",
            "financial stability for", "loan repayment", "credit assessment"
        ]):
            return "credit_assessment"
        
        # FALLBACK: Check intent analysis for credit assessment
        intent = intent_analysis.get("intent", "")
        if intent in ["creditworthiness", "credit_assessment"]:
            return "credit_assessment"
        
        # Behavioral Analysis Queries  
        if any(keyword in query_lower for keyword in [
            "spending habits", "patterns", "behavior", "introduce this person", 
            "transaction patterns", "habits", "anchor merchants", "impulsive", 
            "digital-native", "time of day", "stretch balance", "milestone"
        ]):
            return "behavioral_analysis"
        
        # Risk Profiling Queries
        if any(keyword in query_lower for keyword in [
            "risk", "volatility", "emergency", "stress", "broke pattern",
            "ratio", "predictable", "sudden", "spike", "emotion"
        ]):
            return "risk_profiling"
        
        # Trend Analysis Queries
        if any(keyword in query_lower for keyword in [
            "trends", "over time", "growth", "change", "progression",
            "compare", "vs", "versus", "month to month", "year over year"
        ]):
            return "trend_analysis"
        
        # Specific Period Analysis
        if any(keyword in query_lower for keyword in [
            "july 2025", "april 2025", "last month", "this month",
            "breakdown", "top 5", "largest", "merchants"
        ]):
            return "period_analysis"
        
        # Income/Spending Analysis (default)
        return "income_spending_analysis"
    
    def _determine_optimal_time_scope(self, query: str, query_type: str) -> str:
        """
        ADAPTIVE: Determine the optimal time scope based on query type and financial intelligence
        """
        query_lower = query.lower()
        
        # Credit assessment needs comprehensive view
        if query_type == "credit_assessment":
            return "6-12 months comprehensive (income stability assessment)"
        
        # Behavioral analysis needs pattern recognition timeframe
        if query_type == "behavioral_analysis":
            return "3-6 months pattern recognition (behavioral consistency)"
        
        # Risk profiling needs volatility assessment period
        if query_type == "risk_profiling":
            return "6 months volatility assessment (financial stress indicators)"
        
        # Trend analysis needs historical comparison
        if query_type == "trend_analysis":
            if "2025" in query_lower:
                return "Full year 2025 with monthly breakdown"
            return "6-12 months trend analysis (comparative periods)"
        
        # Specific period analysis - extract from query
        if query_type == "period_analysis":
            if "july 2025" in query_lower:
                return "July 2025 with comparative context (June/August)"
            elif "april 2025" in query_lower:
                return "April 2025 with comparative context (March/May)"
            return "Specified period with comparative context"
        
        # Default for income/spending
        return "Last 3 months with current month focus"


# -----------------------------------------------------------------------------
# FastAPI Endpoints
# -----------------------------------------------------------------------------

# Initialize the production analyzer
production_analyzer = ProductionFinancialAnalyzer()

@app.get("/")
async def root():
    return {
        "message": "Financial Chat API - Production Ready âœ…",
        "version": "2.0.0",
        "features": [
            "IST â†” UTC timezone conversion",
            "BSON Date handling (no string conversion)",
            "debitAmount calculation in all pipelines", 
            "9 exact fallback templates",
            "Versioned caching system",
            "Parallel execution (4-6 concurrent)",
            "GroundingContext and post-fact validation",
            "Real schema: user_financial_transactions"
        ],
        "ai_model_status": ai_interface.get_status(),
        "mongodb": {"db": MONGODB_DB, "collection": MONGODB_COLLECTION},
        "cache_entries": len(cache_store)
    }

@app.post("/chat", response_model=ChatResponse)
async def chat_with_financial_data(request: ChatRequest):
    """
    Main chat endpoint implementing the complete production pipeline
    """
    try:
        logger.info(f"Processing query for user {request.user_id}: {request.query}")
        
        result = await production_analyzer.analyze_query(request.user_id, request.query)
        
        return ChatResponse(
            user_id=request.user_id,
            query=request.query,
            response=result["response"],
            sub_queries=result["sub_queries"],
            data_points=result["data_points"],
            processing_time=result["processing_time"],
            timestamp=datetime.now(timezone.utc).isoformat(),
            grounding_context=result.get("grounding_context", {})
        )
        
    except Exception as e:
        logger.exception("Chat endpoint error")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    """Enhanced health check with system metrics"""
    return {
        "status": "healthy",
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "services": {
            "ai_model": ai_interface.get_status(),
            "mongodb": True,
            "api": True,
            "cache_entries": len(cache_store)
        },
        "version": "2.0.0",
        "features_enabled": [
            "IST timezone support",
            "BSON Date handling", 
            "Exact fallback templates",
            "Parallel execution",
            "Grounding validation"
        ]
    }

@app.get("/cache/stats")
async def cache_stats():
    """Get cache statistics"""
    return {
        "total_entries": len(cache_store),
        "entries": {key: {"timestamp": timestamp, "age_seconds": time.time() - timestamp} 
                   for key, (data, timestamp) in cache_store.items()}
    }

@app.post("/cache/clear")
async def clear_cache():
    """Clear all cache entries"""
    cache_store.clear()
    return {"message": "Cache cleared", "entries_removed": len(cache_store)}

# AI Provider Management Endpoints
@app.get("/ai/priority")
async def get_ai_priority():
    """Get current AI provider priority information"""
    try:
        priority_info = ai_config.get_current_priority()
        return {
            "status": "success",
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "priority_info": priority_info
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get AI priority: {str(e)}")

@app.post("/ai/priority")
async def set_ai_priority(priority_request: dict):
    """Set AI provider priority order
    
    Example:
    {
        "priority": ["openai", "gemini", "groq"]
    }
    """
    try:
        if "priority" not in priority_request:
            raise HTTPException(status_code=400, detail="Missing 'priority' field in request")
        
        priority_list = priority_request["priority"]
        if not isinstance(priority_list, list):
            raise HTTPException(status_code=400, detail="'priority' must be a list of provider names")
        
        # Set the new priority
        result = ai_config.set_provider_priority(priority_list)
        
        if result["status"] == "error":
            raise HTTPException(status_code=400, detail=result["message"])
        
        return {
            "status": "success",
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "message": f"AI provider priority updated. Active: {result['active_provider']}",
            "result": result
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to set AI priority: {str(e)}")

@app.post("/ai/switch/{provider}")
async def switch_ai_provider(provider: str):
    """Switch to a specific AI provider
    
    Available providers: openai, gemini, groq, inhouse
    """
    try:
        success = ai_config.switch_provider(provider)
        if not success:
            raise HTTPException(status_code=400, detail=f"Failed to switch to provider: {provider}")
        
        return {
            "status": "success",
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "message": f"Switched to AI provider: {provider}",
            "current_priority": ai_config.get_current_priority()
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to switch AI provider: {str(e)}")

@app.get("/ai/providers")
async def get_available_providers():
    """Get information about all AI providers"""
    try:
        providers_info = {}
        for provider_name, provider_config in ai_config.PROVIDERS.items():
            api_key_available = bool(os.getenv(provider_config['api_key_env']))
            providers_info[provider_name] = {
                "name": provider_config['name'],
                "models": provider_config['models'],
                "default_model": provider_config['default_model'],
                "api_key_available": api_key_available,
                "rate_limit_handling": provider_config.get('rate_limit_handling', False),
                "fallback_support": provider_config.get('fallback_support', False)
            }
        
        return {
            "status": "success",
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "providers": providers_info,
            "current_active": ai_config.active_provider
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get providers info: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
