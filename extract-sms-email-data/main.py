#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Fixed Optimized SMS Processing Script for LifafaV0
=================================================

FIXES:
1. Improved JSON extraction to handle LLM reasoning text
2. Better error handling for API timeouts
3. Enhanced prompt clarity for structured responses
4. Robust response parsing with multiple fallback strategies
"""

import os
import re
import json
import time
import argparse
import asyncio
from typing import Any, Dict, Optional, List
import math
from datetime import datetime
from dotenv import load_dotenv
import statistics

import aiohttp
from tqdm import tqdm
from mongodb_operations import MongoDBOperations

# Load environment variables from .env file
load_dotenv()

API_URL = os.getenv("API_URL", "")
API_KEY = os.getenv("API_KEY", "")

# Debug: Show loaded environment variables
print(f"🔧 Environment Debug:")
print(f"   API_URL loaded: {'✅' if API_URL else '❌'} - '{API_URL}'")
print(f"   API_KEY loaded: {'✅' if API_KEY else '❌'} - {'***' if API_KEY else 'Not set'}")
print(f"   Current working directory: {os.getcwd()}")
print(f"   .env file exists: {'✅' if os.path.exists('.env') else '❌'}")

# Adaptive Rate Limiting Configuration
class AdaptiveRateLimiter:
    """Intelligent rate limiter that adapts to API performance"""
    
    def __init__(self, min_delay=0.5, max_delay=10.0, initial_delay=1.0):
        self.min_delay = min_delay
        self.max_delay = max_delay
        self.current_delay = initial_delay
        self.response_times = []
        self.error_count = 0
        self.success_count = 0
        self.max_response_time = 5.0  # Target max response time
    
    def update_delay(self, response_time: float, success: bool):
        """Update delay based on API performance"""
        if success:
            self.success_count += 1
            self.error_count = max(0, self.error_count - 1)
            
            # Add response time to rolling window (keep last 10)
            self.response_times.append(response_time)
            if len(self.response_times) > 10:
                self.response_times.pop(0)
            
            # Calculate average response time
            if self.response_times:
                avg_response_time = statistics.mean(self.response_times)
                
                # Adjust delay based on performance
                if avg_response_time > self.max_response_time:
                    # API is slow, increase delay
                    self.current_delay = min(self.current_delay * 1.2, self.max_delay)
                elif avg_response_time < self.max_response_time * 0.7:
                    # API is fast, decrease delay
                    self.current_delay = max(self.current_delay * 0.8, self.min_delay)
        else:
            self.error_count += 1
            # Increase delay on errors
            self.current_delay = min(self.current_delay * 1.5, self.max_delay)
    
    async def wait(self):
        """Wait for the calculated delay"""
        await asyncio.sleep(self.current_delay)
    
    def get_stats(self):
        """Get current rate limiter statistics"""
        return {
            "current_delay": self.current_delay,
            "avg_response_time": statistics.mean(self.response_times) if self.response_times else 0,
            "success_rate": self.success_count / (self.success_count + self.error_count) if (self.success_count + self.error_count) > 0 else 0,
            "error_count": self.error_count
        }

# Global rate limiter instance
rate_limiter = AdaptiveRateLimiter()

# Performance Monitoring
class PerformanceMonitor:
    """Comprehensive performance monitoring for the SMS processing system"""
    
    def __init__(self):
        self.start_time = time.time()
        self.api_calls = 0
        self.successful_calls = 0
        self.failed_calls = 0
        self.total_processing_time = 0
        self.batch_times = []
        self.api_response_times = []
        self.memory_usage = []
        
    def record_api_call(self, success: bool, response_time: float):
        """Record API call performance"""
        self.api_calls += 1
        if success:
            self.successful_calls += 1
        else:
            self.failed_calls += 1
        self.api_response_times.append(response_time)
    
    def record_batch_time(self, batch_size: int, processing_time: float):
        """Record batch processing performance"""
        self.batch_times.append({
            "batch_size": batch_size,
            "processing_time": processing_time,
            "sms_per_second": batch_size / processing_time
        })
    
    def get_performance_stats(self):
        """Get comprehensive performance statistics"""
        total_time = time.time() - self.start_time
        
        if self.api_response_times:
            avg_response_time = statistics.mean(self.api_response_times)
            min_response_time = min(self.api_response_times)
            max_response_time = max(self.api_response_times)
        else:
            avg_response_time = min_response_time = max_response_time = 0
        
        if self.batch_times:
            avg_sms_per_second = statistics.mean([b["sms_per_second"] for b in self.batch_times])
            total_sms_processed = sum([b["batch_size"] for b in self.batch_times])
        else:
            avg_sms_per_second = total_sms_processed = 0
        
        return {
            "total_processing_time": total_time,
            "total_api_calls": self.api_calls,
            "successful_api_calls": self.successful_calls,
            "failed_api_calls": self.failed_calls,
            "api_success_rate": self.successful_calls / self.api_calls if self.api_calls > 0 else 0,
            "avg_api_response_time": avg_response_time,
            "min_api_response_time": min_response_time,
            "max_api_response_time": max_response_time,
            "avg_sms_per_second": avg_sms_per_second,
            "total_sms_processed": total_sms_processed,
            "overall_throughput": total_sms_processed / total_time if total_time > 0 else 0,
            "rate_limiter_stats": rate_limiter.get_stats()
        }
    
    def print_performance_summary(self):
        """Print comprehensive performance summary"""
        stats = self.get_performance_stats()
        
        print(f"\n📊 PERFORMANCE SUMMARY:")
        print(f"   ⏱️  Total Processing Time: {stats['total_processing_time']:.2f}s")
        print(f"   🔗 API Calls: {stats['total_api_calls']} (Success: {stats['successful_api_calls']}, Failed: {stats['failed_api_calls']})")
        print(f"   ✅ API Success Rate: {stats['api_success_rate']:.1%}")
        print(f"   ⚡ Average API Response: {stats['avg_api_response_time']:.2f}s")
        print(f"   📱 Total SMS Processed: {stats['total_sms_processed']}")
        print(f"   🚀 Overall Throughput: {stats['overall_throughput']:.2f} SMS/second")
        print(f"   🔄 Rate Limiter Delay: {stats['rate_limiter_stats']['current_delay']:.2f}s")
        print(f"   📈 Rate Limiter Success Rate: {stats['rate_limiter_stats']['success_rate']:.1%}")

# Global performance monitor
performance_monitor = PerformanceMonitor()

# Enhanced Error Recovery System
class ErrorRecoveryManager:
    """Intelligent error recovery with retry logic and dead letter queue"""
    
    def __init__(self, max_retries=3, backoff_factor=2.0):
        self.max_retries = max_retries
        self.backoff_factor = backoff_factor
        self.retry_queue = {}  # SMS that need retry
        self.dead_letter_queue = []  # SMS that failed permanently
        self.retry_delays = {}  # Track retry delays per SMS
    
    def should_retry(self, sms_id: str, error_type: str) -> bool:
        """Determine if SMS should be retried based on error type and retry count"""
        if sms_id not in self.retry_queue:
            return True
        
        retry_count = self.retry_queue[sms_id]["retry_count"]
        
        # Don't retry certain error types
        permanent_errors = ["validation_error", "parsing_error", "missing_essential_fields"]
        if error_type in permanent_errors:
            return False
        
        # Check retry count
        return retry_count < self.max_retries
    
    def schedule_retry(self, sms_data: Dict[str, Any], error_type: str, error_message: str):
        """Schedule SMS for retry with exponential backoff"""
        sms_id = sms_data.get("_source_id") or sms_data.get("unique_id")
        
        if not self.should_retry(sms_id, error_type):
            # Move to dead letter queue
            self.add_to_dead_letter_queue(sms_data, error_type, error_message)
            return
        
        if sms_id not in self.retry_queue:
            self.retry_queue[sms_id] = {
                "sms_data": sms_data,
                "retry_count": 0,
                "error_type": error_type,
                "error_message": error_message,
                "first_error_time": datetime.now(),
                "last_error_time": datetime.now()
            }
            self.retry_delays[sms_id] = 1.0  # Start with 1 second delay
        else:
            # Increment retry count and update error info
            self.retry_queue[sms_id]["retry_count"] += 1
            self.retry_queue[sms_id]["last_error_time"] = datetime.now()
            self.retry_queue[sms_id]["error_type"] = error_type
            self.retry_queue[sms_id]["error_message"] = error_message
            
            # Exponential backoff
            self.retry_delays[sms_id] = min(
                self.retry_delays[sms_id] * self.backoff_factor, 
                60.0  # Max 60 second delay
            )
    
    def add_to_dead_letter_queue(self, sms_data: Dict[str, Any], error_type: str, error_message: str):
        """Add permanently failed SMS to dead letter queue"""
        dead_letter_entry = {
            "sms_data": sms_data,
            "error_type": error_type,
            "error_message": error_message,
            "failed_at": datetime.now(),
            "retry_count": self.retry_queue.get(sms_data.get("_source_id"), {}).get("retry_count", 0)
        }
        self.dead_letter_queue.append(dead_letter_entry)
        
        # Remove from retry queue
        sms_id = sms_data.get("_source_id") or sms_data.get("unique_id")
        if sms_id in self.retry_queue:
            del self.retry_queue[sms_id]
        if sms_id in self.retry_delays:
            del self.retry_delays[sms_id]
    
    def get_retry_stats(self):
        """Get retry queue statistics"""
        return {
            "retry_queue_size": len(self.retry_queue),
            "dead_letter_queue_size": len(self.dead_letter_queue),
            "total_retries": sum(entry["retry_count"] for entry in self.retry_queue.values()),
            "avg_retry_count": statistics.mean([entry["retry_count"] for entry in self.retry_queue.values()]) if self.retry_queue else 0
        }
    
    def save_dead_letter_queue(self, file_path: str):
        """Save dead letter queue to file for analysis"""
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(self.dead_letter_queue, f, indent=2, ensure_ascii=False, default=str)
            print(f"💾 Dead letter queue saved to: {file_path}")
        except Exception as e:
            print(f"❌ Failed to save dead letter queue: {e}")

# Global error recovery manager
error_recovery_manager = ErrorRecoveryManager()

# IMPROVED PROMPT - More explicit about JSON structure
UNIVERSAL_RULES = """You are an expert financial data parser for LifafaV0. 

CRITICAL INSTRUCTIONS:
1. Output ONLY valid JSON - no explanations, no markdown, no thinking process

MANDATORY FIELDS (must include):
- currency: "INR" (always required)
- message_intent: one of the specified values (always required)
- transaction_type: "credit" or "debit" (for actual transactions)
- amount: numeric value (for transactions)
- account: bank and account number details
2. Use the EXACT field names and structure shown below
3. If unsure about a field, omit it entirely rather than guess

INPUT: SMS message JSON
OUTPUT: Single JSON object with transaction details

REQUIRED OUTPUT STRUCTURE:
{
  "transaction_type": "credit | debit",
  "amount": number,
  "currency": "INR",
  "transaction_date": "ISO 8601 timestamp",
  "account": {
    "bank": "bank_name",
    "account_number": "account_number"
  },
  "counterparty": "merchant/person/organization",
  "balance": number,
  "category": "investment | transfer | atm-withdrawal | other",
  "tags": ["tag1", "tag2"],
  "summary": "brief description under 10 words",
  "confidence_score": 0.0-1.0,
  "message_intent": "transaction | payment_request | pending_confirmation | otp | promo | alert | other",
  "metadata": {
    "channel": "sms",
    "sender": "sender_name",
    "method": "UPI | IMPS | NEFT | RTGS | ATM | Card | MF | Other",
    "reference_id": "transaction_reference",
    "original_text": "complete_sms_body"
  }
}

CLASSIFICATION RULES:
- "credited/received/deposit" → transaction_type: "credit"
- "debited/withdrawn/paid" → transaction_type: "debit" 
- Payment requests → message_intent: "payment_request", omit transaction_type
- OTP messages → message_intent: "otp", omit transaction fields
- Promotional → message_intent: "promo", omit transaction fields
- Currency: Always "INR" for Indian SMS
- Extract amounts as numbers (remove Rs., commas)
- Preserve masked account formats (XXXXXXXX9855, A/cX9855)

EXAMPLES:
Credit SMS: "Your a/c XXXXXXXX9855 is credited by Rs.60000.00 on 02-07-25 by STATION91"
→ {"transaction_type": "credit", "amount": 60000.0, "currency": "INR", ...}

Debit SMS: "Rs.2000 withdrawn at ATM from A/cX9855"  
→ {"transaction_type": "debit", "amount": 2000, "currency": "INR", ...}

RESPOND WITH ONLY THE JSON OBJECT - NO OTHER TEXT."""

def clean_mongodb_document(doc: Dict[str, Any]) -> Dict[str, Any]:
    """Clean MongoDB document by converting ObjectId to strings and removing problematic fields"""
    if not isinstance(doc, dict):
        return doc
    
    cleaned = {}
    for key, value in doc.items():
        # Skip MongoDB internal fields
        if key in ['_id', '__v']:
            continue
            
        # Convert ObjectId to string
        if hasattr(value, '__class__') and value.__class__.__name__ == 'ObjectId':
            cleaned[key] = str(value)
        # Convert datetime to ISO string
        elif hasattr(value, '__class__') and value.__class__.__name__ == 'datetime':
            cleaned[key] = value.isoformat()
        # Convert date to ISO string
        elif hasattr(value, '__class__') and value.__class__.__name__ == 'date':
            cleaned[key] = value.isoformat()
        # Handle nested dictionaries
        elif isinstance(value, dict):
            cleaned[key] = clean_mongodb_document(value)
        # Handle lists
        elif isinstance(value, list):
            cleaned[key] = [clean_mongodb_document(item) if isinstance(item, dict) else item for item in value]
        # Keep other values as is
        else:
            cleaned[key] = value
    
    return cleaned

def build_prompt(input_msg: Dict[str, Any]) -> str:
    """Build the prompt for LLM processing"""
    # Clean the MongoDB document before sending to API
    cleaned_msg = clean_mongodb_document(input_msg)
    return UNIVERSAL_RULES + f"\n\nSMS TO PARSE:\n{json.dumps(cleaned_msg, ensure_ascii=False)}\n\nJSON OUTPUT:"

def extract_json_object(text: str) -> Optional[Dict[str, Any]]:
    """Enhanced JSON extraction with multiple fallback strategies"""
    if not text:
        return None
    
    text = text.strip()
    
    # Remove common LLM prefixes/suffixes
    text = re.sub(r'^```json\s*', '', text, flags=re.IGNORECASE)
    text = re.sub(r'\s*```$', '', text)
    text = re.sub(r'^json\s*', '', text, flags=re.IGNORECASE)
    
    # Remove thinking tags and content
    text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL | re.IGNORECASE)
    text = re.sub(r'<reasoning>.*?</reasoning>', '', text, flags=re.DOTALL | re.IGNORECASE)
    
    # Remove explanatory text before JSON
    text = re.sub(r'^[^{]*?(?=\{)', '', text)
    
    # Try direct JSON parsing first
    try:
        return json.loads(text)
    except Exception:
        pass
    
    # Strategy 1: Find complete JSON object with proper brace matching
    brace_count = 0
    start_idx = -1
    best_json = None
    
    for i, char in enumerate(text):
        if char == '{':
            if start_idx == -1:
                start_idx = i
            brace_count += 1
        elif char == '}':
            brace_count -= 1
            if brace_count == 0 and start_idx != -1:
                try:
                    candidate = text[start_idx:i+1]
                    parsed = json.loads(candidate)
                    if isinstance(parsed, dict) and len(parsed) > 1:  # Must have multiple fields
                        best_json = parsed
                        break
                except Exception:
                    continue
    
    if best_json:
        return best_json
    
    # Strategy 2: Find JSON with regex and fix common issues
    json_patterns = [
        r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}',  # Nested objects
        r'\{.*?\}',  # Simple objects
    ]
    
    for pattern in json_patterns:
        matches = re.findall(pattern, text, re.DOTALL)
        for match in matches:
            try:
                # Fix common JSON issues
                fixed_match = match.strip()
                # Fix trailing commas
                fixed_match = re.sub(r',(\s*[}\]])', r'\1', fixed_match)
                # Fix unquoted keys
                fixed_match = re.sub(r'(\w+):', r'"\1":', fixed_match)
                
                parsed = json.loads(fixed_match)
                if isinstance(parsed, dict) and len(parsed) > 1:
                    return parsed
            except Exception:
                continue
    
    # Strategy 3: Extract key-value pairs and construct JSON
    try:
        # Look for key-value patterns
        kv_pattern = r'"([^"]+)":\s*([^,}]+)'
        matches = re.findall(kv_pattern, text)
        
        if matches:
            result = {}
            for key, value in matches:
                try:
                    # Try to parse value as JSON
                    if value.strip().startswith('"') and value.strip().endswith('"'):
                        result[key] = json.loads(value.strip())
                    elif value.strip() in ['true', 'false']:
                        result[key] = json.loads(value.strip())
                    elif re.match(r'^-?\d+\.?\d*$', value.strip()):
                        result[key] = float(value.strip()) if '.' in value else int(value.strip())
                    else:
                        result[key] = value.strip().strip('"')
                except:
                    result[key] = value.strip().strip('"')
            
            if len(result) > 1:
                return result
    except Exception:
        pass
    
        return None

async def call_openai_style(session: aiohttp.ClientSession, model: str, prompt: str, 
                           temperature: float, max_tokens: int, top_p: float):
    """Enhanced API call with adaptive rate limiting and better error handling"""
    
    # Verify API_URL is loaded
    if not API_URL:
        print(f"  ❌ ERROR: API_URL is not set! Current value: '{API_URL}'")
        return None
    
    payload = {
        "model": model,
        "temperature": temperature,
        "top_p": top_p,
        "max_tokens": max_tokens,
        "messages": [{"role": "user", "content": prompt}]
    }
    headers = {"Content-Type": "application/json"}
    if API_KEY:
        headers["Authorization"] = f"Bearer {API_KEY}"

    print(f"  🔗 Calling API: {API_URL}")
    print(f"  📤 Payload: model={model}, max_tokens={max_tokens}")
    print(f"  ⏱️  Current rate limit delay: {rate_limiter.current_delay:.2f}s")

    start_time = time.time()
    
    for attempt in range(3):
        try:
            # Adaptive timeout based on current rate limiter state
            base_timeout = 60 if rate_limiter.current_delay < 2.0 else 90
            timeout = aiohttp.ClientTimeout(total=base_timeout, connect=20)
            print(f"  📡 Attempt {attempt + 1}/3: Making API call...")
            
            async with session.post(API_URL, json=payload, headers=headers, timeout=timeout, ssl=False) as resp:
                response_time = time.time() - start_time
                print(f"  📥 Response status: {resp.status} (took {response_time:.2f}s)")
                
                if resp.status == 200:
                    data = await resp.json()
                    print(f"  ✅ API call successful on attempt {attempt + 1}")
                    
                    # Update rate limiter with success
                    rate_limiter.update_delay(response_time, True)
                    print(f"  ⚡ Rate limiter updated - New delay: {rate_limiter.current_delay:.2f}s")
                    
                    # Record performance metrics
                    performance_monitor.record_api_call(True, response_time)
                    
                    return data
                elif resp.status in (429, 500, 502, 503, 504):
                    # Update rate limiter with failure
                    rate_limiter.update_delay(response_time, False)
                    
                    # Record performance metrics
                    performance_monitor.record_api_call(False, response_time)
                    
                    wait_time = min(30, 5 ** attempt)  # Exponential backoff
                    print(f"  ⏳ API rate limit/error {resp.status}, waiting {wait_time}s...")
                    print(f"  ⚡ Rate limiter updated - New delay: {rate_limiter.current_delay:.2f}s")
                    await asyncio.sleep(wait_time)
                    continue
                else:
                    error_text = await resp.text()
                    print(f"  ❌ API error {resp.status}: {error_text[:200]}")
                    
                    # Update rate limiter with failure
                    rate_limiter.update_delay(response_time, False)
                    
                    # Record performance metrics
                    performance_monitor.record_api_call(False, response_time)
                    
                    return None
                    
        except asyncio.TimeoutError:
            response_time = time.time() - start_time
            print(f"  ⏳ API timeout on attempt {attempt + 1} (took {response_time:.2f}s)")
            
            # Update rate limiter with failure
            rate_limiter.update_delay(response_time, False)
            
            await asyncio.sleep(min(10, 2 ** attempt))
        except aiohttp.ClientError as e:
            response_time = time.time() - start_time
            print(f"  🌐 Network error on attempt {attempt + 1}: {str(e)} (took {response_time:.2f}s)")
            print(f"  🔍 Error details: {type(e).__name__} - {e}")
            
            # Update rate limiter with failure
            rate_limiter.update_delay(response_time, False)
            
            await asyncio.sleep(min(5, 2 ** attempt))
        except Exception as e:
            response_time = time.time() - start_time
            print(f"  ❌ Unexpected error on attempt {attempt + 1}: {str(e)} (took {response_time:.2f}s)")
            print(f"  🔍 Error type: {type(e).__name__}")
            
            # Update rate limiter with failure
            rate_limiter.update_delay(response_time, False)
            
            await asyncio.sleep(min(5, 2 ** attempt))
    
    print(f"  ❌ All 3 API attempts failed")
    return None

def parse_response(data: Dict[str, Any], mode: str) -> Optional[Dict[str, Any]]:
    """Enhanced response parsing with better error handling"""
    content = None
    if not data:
        return None

    try:
        if mode == "openai":
            content = data["choices"][0]["message"]["content"]
        else:
            content = (
                data.get("text") or 
                data.get("output") or 
                data.get("generated_text") or 
                data.get("content")
            )
        
        if content:
            # Clean up the content before extraction
            content = content.strip()
            return extract_json_object(content)
    except Exception as e:
        print(f"  ❌ Response parsing error: {e}")
    
    return None

def safe_enrich(input_msg: Dict[str, Any], parsed: Dict[str, Any]) -> Dict[str, Any]:
    """Enhanced enrichment with validation"""
    try:
        # Ensure currency is always set
        if "currency" not in parsed:
            parsed["currency"] = "INR"
        
        # Only enrich if we have meaningful data
        if not parsed.get("message_intent"):
            return parsed
        
        # Set method based on content analysis
        if "metadata" not in parsed:
            parsed["metadata"] = {}
        
        if not parsed["metadata"].get("method"):
            body = input_msg.get("body", "").lower()
            method = "Other"
            if "imps" in body: method = "IMPS"
            elif "neft" in body: method = "NEFT"
            elif "rtgs" in body: method = "RTGS"
            elif "upi" in body: method = "UPI"
            elif "atm" in body: method = "ATM"
            elif "credit card" in body or "debit card" in body: method = "Card"
            elif "mutual fund" in body or "sip" in body: method = "MF"
            
            parsed["metadata"]["method"] = method
        
        # Ensure metadata has required fields
        if "channel" not in parsed["metadata"]:
            parsed["metadata"]["channel"] = "sms"
        if "sender" not in parsed["metadata"]:
            parsed["metadata"]["sender"] = input_msg.get("sender", "")
        if "original_text" not in parsed["metadata"]:
            parsed["metadata"]["original_text"] = input_msg.get("body", "")
        
        # CRITICAL: Preserve user_id and email_id from input SMS for MongoDB storage
        if "user_id" in input_msg:
            parsed["user_id"] = input_msg["user_id"]
        if "email_id" in input_msg:
            parsed["email_id"] = input_msg["email_id"]
        
        # CRITICAL: Ensure unique_id is set for MongoDB storage
        if "unique_id" in input_msg:
            # Use the original unique_id but make it unique for this transaction
            base_id = input_msg["unique_id"]
            parsed["unique_id"] = f"{base_id}_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}"
        elif "_source_id" in input_msg:
            # Use _source_id but make it unique for this transaction
            base_id = input_msg["_source_id"]
            parsed["unique_id"] = f"{base_id}_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}"
        else:
            # Generate a completely unique ID
            parsed["unique_id"] = f"txn_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}"
        
        return parsed
    except Exception as e:
        print(f"  ⚠️  Enrichment error: {e}")
        return parsed

def mark_sms_as_processed(input_path: str, source_id: str, success: bool = True):
    """Mark SMS as processed in the input file for resume capability"""
    try:
        # Read the input file
        with open(input_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # Find and update the SMS
        if isinstance(data, dict) and 'financial_sms' in data:
            sms_list = data['financial_sms']
        elif isinstance(data, list):
            sms_list = data
        else:
            return
        
        # Find SMS by unique_id - this is the ONLY reliable way
        sms_found = False
        
        for sms in sms_list:
            if sms.get('unique_id') == source_id:
                sms['isprocessed'] = True
                sms['processing_timestamp'] = datetime.now().isoformat()
                sms['processing_status'] = 'success' if success else 'failed'
                print(f"  🔄 Marked SMS {source_id} as processed in input file")
                sms_found = True
                break

        if not sms_found:
            print(f"  ❌ ERROR: Could not find SMS {source_id} by unique_id in input file")
            print(f"  🔍 Available unique_ids: {[sms.get('unique_id', 'NO_ID') for sms in sms_list[:3]]}...")
        
        # Write back to file
        with open(input_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
            
    except Exception as e:
        print(f"⚠️  Warning: Could not update input file for SMS {source_id}: {e}")

def update_input_file_progress(input_path: str, processed_sms: List[Dict[str, Any]], failed_sms: List[Dict[str, Any]]):
    """Update input file with processing progress for all SMS in current batch"""
    try:
        # Read the input file
        with open(input_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # Find and update the SMS
        if isinstance(data, dict) and 'financial_sms' in data:
            sms_list = data['financial_sms']
        elif isinstance(data, list):
            sms_list = data
        else:
            return
        
        # Mark successful SMS as processed
        for sms in processed_sms:
            source_id = sms.get('_source_id')
            if source_id:
                # Find by unique_id - this is the ONLY reliable way
                sms_found = False
                for input_sms in sms_list:
                    if input_sms.get('unique_id') == source_id:
                        input_sms['isprocessed'] = True
                        input_sms['processing_timestamp'] = datetime.now().isoformat()
                        input_sms['processing_status'] = 'success'
                        sms_found = True
                        break
                
                if not sms_found:
                    print(f"  ❌ ERROR: Could not find SMS {source_id} by unique_id in batch progress update")
        
        # Mark failed SMS as processed (so they won't be retried)
        for sms in failed_sms:
            source_id = sms.get('_source_id')
            if source_id:
                # Find by unique_id - this is the ONLY reliable way
                sms_found = False
                for input_sms in sms_list:
                    if input_sms.get('unique_id') == source_id:
                        input_sms['isprocessed'] = True
                        input_sms['processing_timestamp'] = datetime.now().isoformat()
                        input_sms['processing_status'] = 'failed'
                        sms_found = True
                        break
                
                if not sms_found:
                    print(f"  ❌ ERROR: Could not find SMS {source_id} by unique_id in batch progress update")
        
        # Write back to file
        with open(input_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
            
    except Exception as e:
        print(f"⚠️  Warning: Could not update input file progress: {e}")

def write_results_real_time(output_path: str, results: List[Dict[str, Any]], mode: str = "append"):
    """Write results to output file in real-time"""
    try:
        if mode == "append" and os.path.exists(output_path):
            # Read existing results and append new ones
            with open(output_path, 'r', encoding='utf-8') as f:
                existing_results = json.load(f)
            existing_results.extend(results)
            results_to_write = existing_results
            print(f"  💾 Real-time: Appended {len(results)} new results to existing {len(existing_results)} results")
        else:
            # Write all results (overwrite mode)
            results_to_write = results
            print(f"  💾 Real-time: Wrote {len(results)} results (overwrite mode)")
        
        # Write results with pretty formatting
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results_to_write, f, indent=2, ensure_ascii=False)
        
    except Exception as e:
        print(f"  ❌ Error writing results to {output_path}: {e}")

def write_failures_real_time(failures_path: str, failures: List[Dict[str, Any]], mode: str = "append"):
    """Write failures to failures file in real-time"""
    try:
        if mode == "append" and os.path.exists(failures_path):
            # Read existing failures and append new ones
            existing_failures = []
            try:
                with open(failures_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            existing_failures.append(json.loads(line.strip()))
            except:
                pass  # File might be empty or corrupted
            
            existing_failures.extend(failures)
            failures_to_write = existing_failures
        else:
            # Write all failures (overwrite mode)
            failures_to_write = failures
        
        # Write failures in NDJSON format
        with open(failures_path, 'w', encoding='utf-8') as f:
            for failure in failures_to_write:
                f.write(json.dumps(failure, ensure_ascii=False) + "\n")
        
        print(f"  💾 Real-time: Updated {failures_path} with {len(failures)} new failures")
        
    except Exception as e:
        print(f"  ❌ Error writing failures to {failures_path}: {e}")

def cleanup_empty_failures_file(failures_path: str):
    """Delete failures file if it's empty (all SMS processed successfully)"""
    try:
        if os.path.exists(failures_path):
            with open(failures_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()
            
            if not content:
                os.remove(failures_path)
                print(f"  🗑️  Deleted empty failures file: {failures_path}")
                print(f"  ✅ All SMS processed successfully!")
            else:
                print(f"  📊 Failures file contains {len(content.split(chr(10)))} failure records")
                
    except Exception as e:
        print(f"  ⚠️  Could not check/cleanup failures file: {e}")

async def process_sms_batch_parallel(sms_batch: List[Dict[str, Any]], batch_id: int, 
                                    session: aiohttp.ClientSession, model: str, mode: str,
                                    temperature: float, max_tokens: int, top_p: float, 
                                    enrich_mode: str, pbar: tqdm, input_path: str) -> tuple:
    """Process SMS batch with true parallel processing for maximum efficiency"""
    batch_start_time = time.time()
    results = []
    failures = []
    
    print(f"🔄 Processing Batch {batch_id} ({len(sms_batch)} SMS) - PARALLEL MODE")
    
    # Apply rate limiting at batch level before starting parallel processing
    await rate_limiter.wait()
    
    # Process all SMS in the batch concurrently
    async def process_single_sms(sms_data):
        src_id = sms_data.get("_source_id")  # Use _source_id (unique_id) instead of id
        input_msg = sms_data
        
        try:
            # Check cache first for similar SMS patterns
            cached_result = intelligent_cache.get_cached_result(input_msg)
            if cached_result:
                # Use cached result with source ID
                cached_result["_source_id"] = src_id
                
                # Cache hit event removed - not needed for production
                
                intent = cached_result.get('message_intent', 'unknown')
                amount = cached_result.get('amount', 'N/A')
                print(f"  🎯 SMS {src_id}: {intent} (₹{amount}) [CACHED]")
                
                # Success event removed - not needed for production
                
                # Real-time persistence: Mark as processed in input file
                mark_sms_as_processed(input_path, src_id, success=True)
                
                return {"type": "success", "data": cached_result, "source_id": src_id}
            
            # SMS processing start event removed - not needed for production
            
            # No cache hit, process through API
            prompt = build_prompt(input_msg)

            if mode == "openai":
                data = await call_openai_style(session, model, prompt, temperature, max_tokens, top_p)
            else:
                data = None

            parsed = parse_response(data, mode)
            
            # Enhanced validation
            if parsed and isinstance(parsed, dict) and len(parsed) > 1:
                # Validate that we have essential fields
                essential_fields = ['currency', 'message_intent']
                has_essential = all(field in parsed for field in essential_fields)
                
                if has_essential:
                    # Safe enrichment
                    if enrich_mode == "safe":
                        parsed = safe_enrich(input_msg, parsed)
                    
                    # CRITICAL: Add _source_id to successful results for status tracking
                    parsed["_source_id"] = src_id
                    
                    # Cache the result for future similar SMS
                    intelligent_cache.cache_result(input_msg, parsed)
                    
                    intent = parsed.get('message_intent', 'unknown')
                    amount = parsed.get('amount', 'N/A')
                    print(f"  ✅ SMS {src_id}: {intent} (₹{amount})")
                    
                    # Real-time persistence: Mark as processed in input file
                    mark_sms_as_processed(input_path, src_id, success=True)
                    
                    return {"type": "success", "data": parsed, "source_id": src_id}
                else:
                    # Missing essential fields - treat as failure
                    print(f"  ⚠️  SMS {src_id}: Missing essential fields")
                    failure_info = {
                        "_source_id": src_id,
                        "batch_id": batch_id,
                        "input": input_msg,
                        "parsing_error": "Missing essential fields (currency, message_intent)",
                        "partial_result": parsed
                    }
                    
                    # Schedule for retry if appropriate
                    error_recovery_manager.schedule_retry(
                        input_msg, 
                        "missing_essential_fields", 
                        "Missing essential fields (currency, message_intent)"
                    )
                    
                    # Real-time persistence: Mark as processed in input file
                    mark_sms_as_processed(input_path, src_id, success=False)
                    
                    return {"type": "failure", "data": failure_info, "source_id": src_id}
            else:
                # Enhanced failure logging
                raw_text = None
                if data and mode == "openai":
                    try:
                        raw_text = data["choices"][0]["message"]["content"]
                    except:
                        raw_text = str(data)
                
                failure_info = {
                    "_source_id": src_id,
                    "batch_id": batch_id,
                    "input": input_msg,
                    "raw_response": raw_text[:500] if raw_text else None,  # Truncate long responses
                    "parsing_error": "Failed to extract valid JSON" if raw_text else "No API response"
                }
                print(f"  ❌ SMS {src_id}: Processing failed")
                
                # Schedule for retry if appropriate
                error_recovery_manager.schedule_retry(
                    input_msg, 
                    "parsing_error", 
                    "Failed to extract valid JSON"
                )
                
                # Real-time persistence: Mark as processed in input file
                mark_sms_as_processed(input_path, src_id, success=False)
                
                return {"type": "failure", "data": failure_info, "source_id": src_id}
            
        except Exception as e:
            print(f"  ❌ SMS {src_id}: Exception - {str(e)[:50]}")
            failure_info = {
                "_source_id": src_id,
                "batch_id": batch_id,
                "input": input_msg,
                "error": str(e)
            }
            
            # Schedule for retry if appropriate
            error_recovery_manager.schedule_retry(
                input_msg, 
                "exception", 
                str(e)
            )
            
            return {"type": "failure", "data": failure_info, "source_id": src_id}
    
    # Create concurrent tasks for all SMS in the batch
    tasks = [process_single_sms(sms) for sms in sms_batch]
    
    # Execute all tasks concurrently
    batch_results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # Process results
    for result in batch_results:
        if isinstance(result, Exception):
            print(f"  ❌ SMS processing exception: {result}")
            continue
        
        if result["type"] == "success":
            results.append(result["data"])
        else:
            failures.append(result["data"])
        
        # Update progress bar
        pbar.update(1)
    
    # Record batch performance metrics
    batch_processing_time = time.time() - batch_start_time
    performance_monitor.record_batch_time(len(sms_batch), batch_processing_time)
    
    success_count = len(results)
    failure_count = len(failures)
    print(f"✅ Batch {batch_id} completed: {success_count} success, {failure_count} failed in {batch_processing_time:.2f}s")
    print(f"   🚀 Batch throughput: {len(sms_batch)/batch_processing_time:.2f} SMS/second")
    
    return results, failures

async def process_sms_batch(sms_batch: List[Dict[str, Any]], batch_id: int, 
                           session: aiohttp.ClientSession, model: str, mode: str,
                           temperature: float, max_tokens: int, top_p: float, 
                           enrich_mode: str, pbar: tqdm, input_path: str) -> tuple:
    """Enhanced batch processing with better error handling"""
    results = []
    failures = []
    
    print(f"🔄 Processing Batch {batch_id} ({len(sms_batch)} SMS)")
    
    for sms_data in sms_batch:
        src_id = sms_data.get("_source_id")  # Use _source_id (unique_id) instead of id
        input_msg = sms_data
        
        try:
            prompt = build_prompt(input_msg)

            if mode == "openai":
                data = await call_openai_style(session, model, prompt, temperature, max_tokens, top_p)
            else:
                data = None

            parsed = parse_response(data, mode)
            
            # Enhanced validation
            if parsed and isinstance(parsed, dict) and len(parsed) > 1:
                # Validate that we have essential fields
                essential_fields = ['currency', 'message_intent']
                has_essential = all(field in parsed for field in essential_fields)
                
                if has_essential:
                    # Safe enrichment
                    if enrich_mode == "safe":
                        parsed = safe_enrich(input_msg, parsed)
                    
                    # CRITICAL: Add _source_id to successful results for status tracking
                    parsed["_source_id"] = src_id
                    
                    results.append(parsed)
                    intent = parsed.get('message_intent', 'unknown')
                    amount = parsed.get('amount', 'N/A')
                    print(f"  ✅ SMS {src_id}: {intent} (₹{amount})")
                    
                    # Real-time persistence: Mark as processed in input file
                    mark_sms_as_processed(input_path, src_id, success=True)
                else:
                    # Missing essential fields - treat as failure
                    print(f"  ⚠️  SMS {src_id}: Missing essential fields")
                    failure_info = {
                        "_source_id": src_id,
                        "batch_id": batch_id,
                        "input": input_msg,
                        "parsing_error": "Missing essential fields (currency, message_intent)",
                        "partial_result": parsed
                    }
                    failures.append(failure_info)
                    
                    # Real-time persistence: Mark as processed in input file
                    mark_sms_as_processed(input_path, src_id, success=False)
            else:
                # Enhanced failure logging
                raw_text = None
                if data and mode == "openai":
                    try:
                        raw_text = data["choices"][0]["message"]["content"]
                    except:
                        raw_text = str(data)
                
                failure_info = {
                    "_source_id": src_id,
                    "batch_id": batch_id,
                    "input": input_msg,
                    "raw_response": raw_text[:500] if raw_text else None,  # Truncate long responses
                    "parsing_error": "Failed to extract valid JSON" if raw_text else "No API response"
                }
                failures.append(failure_info)
                print(f"  ❌ SMS {src_id}: Processing failed")
                
                # Real-time persistence: Mark as processed in input file
                mark_sms_as_processed(input_path, src_id, success=False)
            
        except Exception as e:
            print(f"  ❌ SMS {src_id}: Exception - {str(e)[:50]}")
            failures.append({
                "_source_id": src_id,
                "batch_id": batch_id,
                "input": input_msg,
                "error": str(e)
            })
        
        # Update progress
        pbar.update(1)
        
        # Use adaptive rate limiting instead of fixed delay
        await rate_limiter.wait()
    
    success_count = len(results)
    failure_count = len(failures)
    print(f"✅ Batch {batch_id} completed: {success_count} success, {failure_count} failed")
    
    return results, failures

def load_sms_data(path: str) -> List[Dict[str, Any]]:
    """Load SMS data from JSON file with resume capability"""
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    # Handle different JSON structures
    if isinstance(data, dict):
        if 'financial_sms' in data:
            sms_list = data['financial_sms']
        elif 'sms' in data:
            sms_list = data['sms']
        else:
            sms_list = list(data.values())
    elif isinstance(data, list):
        sms_list = data
    else:
        raise ValueError("Invalid JSON format. Expected list or dict with 'sms' key.")
    
    # Filter only unprocessed SMS for resume capability
    unprocessed_sms = [sms for sms in sms_list if not sms.get('isprocessed', False)]
    
    if len(unprocessed_sms) < len(sms_list):
        print(f"🔄 Resume mode: {len(sms_list) - len(unprocessed_sms)} SMS already processed, continuing with {len(unprocessed_sms)} unprocessed SMS")
    
    # Normalize each SMS to expected format while preserving original structure
    normalized_sms = []
    for i, sms in enumerate(unprocessed_sms):
        # Create a copy of the original SMS to preserve all fields
        normalized = sms.copy()
        
        # Add required fields if missing
        if "id" not in normalized:
            normalized["id"] = str(i + 1)
        if "channel" not in normalized:
            normalized["channel"] = "sms"
        if "subject" not in normalized:
            normalized["subject"] = None
        if "type" not in normalized:
            normalized["type"] = "received"
        
        # Ensure _source_id is set for persistence (ALWAYS use unique_id if available)
        if "unique_id" in normalized:
            normalized["_source_id"] = normalized["unique_id"]
        elif "_source_id" not in normalized:
            normalized["_source_id"] = normalized.get("id", str(i + 1))
        
        normalized_sms.append(normalized)
    
    return normalized_sms

def create_adaptive_batches(sms_list: List[Dict[str, Any]], base_batch_size: int, rate_limiter: AdaptiveRateLimiter) -> List[List[Dict[str, Any]]]:
    """Create adaptive batches based on current API performance"""
    if not sms_list:
        return []
    
    # Get rate limiter stats
    stats = rate_limiter.get_stats()
    current_delay = rate_limiter.current_delay
    success_rate = stats["success_rate"]
    
    # Only adjust batch size if we have enough data to make informed decisions
    # Check if we have any API calls made (success_rate will be 0 if no calls yet)
    if success_rate > 0 or current_delay > rate_limiter.min_delay:
        # Calculate optimal batch size based on performance
        if success_rate > 0.9 and current_delay < 1.5:
            # API performing well - increase batch size
            optimal_batch_size = min(base_batch_size * 2, 20)
            print(f"🚀 High performance mode: batch size {optimal_batch_size}")
        elif success_rate > 0.7 and current_delay < 3.0:
            # API performing moderately - use base batch size
            optimal_batch_size = base_batch_size
            print(f"⚡ Normal performance mode: batch size {optimal_batch_size}")
        else:
            # API struggling - reduce batch size but respect minimum
            optimal_batch_size = max(base_batch_size // 2, 1)
            print(f"🐌 Conservative mode: batch size {optimal_batch_size}")
    else:
        # Not enough data yet - use requested batch size
        optimal_batch_size = base_batch_size
        print(f"📊 Initial mode: using requested batch size {optimal_batch_size}")
    
    # Create batches
    batches = []
    for i in range(0, len(sms_list), optimal_batch_size):
        batch = sms_list[i:i + optimal_batch_size]
        batches.append(batch)
    
    return batches

async def process_all_batches(input_path: str, output_path: str, model: str, mode: str,
                             batch_size: int, max_parallel_batches: int,
                             temperature: float, max_tokens: int, top_p: float, 
                             failures_path: Optional[str], enrich_mode: str, 
                             use_mongodb: bool = False, user_id: str = None):
    """Enhanced batch processing with better progress tracking"""
    
    # Initialize MongoDB if requested
    mongo_ops = None
    if use_mongodb:
        try:
            mongo_ops = MongoDBOperations()
            print(f"🗄️  MongoDB connected: {mongo_ops.db_name}")
        except Exception as e:
            print(f"❌ MongoDB connection failed: {e}")
            print(f"🔄 Falling back to file-based processing")
            use_mongodb = False
    
    # Load SMS data from input file (this contains the filtered financial SMS)
    print(f"📱 Loading SMS data from: {input_path}")
    sms_data = load_sms_data(input_path)
    
    total_sms = len(sms_data)
    print(f"📊 Total SMS to process: {total_sms}")
    
    # Verify we're processing the correct number of SMS
    if total_sms > 1000:
        print(f"⚠️  WARNING: Processing {total_sms} SMS - this seems high!")
        print(f"🔍 Expected: ~1400-1500 financial SMS from filtering")
        print(f"📁 Input file: {input_path}")
        print(f"🔄 Continue anyway? (Ctrl+C to stop)")
        try:
            await asyncio.sleep(3)  # Give user time to stop
        except asyncio.CancelledError:
            print("🛑 Processing cancelled by user")
            return
    
    # Initialize output files for real-time updates (only if not using MongoDB)
    if not use_mongodb:
        if failures_path:
            # Create empty failures file
            with open(failures_path, 'w', encoding='utf-8') as f:
                pass  # Create empty file
            print(f"📝 Initialized failures file: {failures_path}")
        
        # Initialize results file - PRESERVE existing results if file exists
        if os.path.exists(output_path):
            try:
                with open(output_path, 'r', encoding='utf-8') as f:
                    existing_results = json.load(f)
                print(f"📝 Preserved existing results file: {output_path} with {len(existing_results)} previous results")
            except:
                # If file is corrupted, start fresh
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump([], f)
                print(f"📝 Started fresh results file: {output_path}")
        else:
            # Create new results file
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump([], f)
            print(f"📝 Created new results file: {output_path}")
    
    batches = create_adaptive_batches(sms_data, batch_size, rate_limiter)
    total_batches = len(batches)
    print(f"📦 Created {total_batches} batches of {batch_size} SMS each")
    print(f"🔄 Processing {max_parallel_batches} batches in parallel")
    
    all_results = []
    all_failures = []
    
    # Enhanced progress bar
    pbar = tqdm(
        total=total_sms, 
        desc="Processing SMS", 
        unit="msg",
        bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}] {postfix}'
    )
    
    async with aiohttp.ClientSession() as session:
        # Initialize checkpoint for resume capability
        if use_mongodb and mongo_ops:
            checkpoint_created = mongo_ops.create_processing_checkpoint(
                user_id=user_id or "all_users",
                batch_id=1,
                total_sms=total_sms,
                processed_sms=0
            )
            if checkpoint_created:
                print(f"💾 Created processing checkpoint for resume capability")
        
        # Process batches with parallel execution
        for i in range(0, total_batches, max_parallel_batches):
            batch_group_start = time.time()
            print(f"🚀 Processing batch group {i//max_parallel_batches + 1}/{total_batches//max_parallel_batches + 1}")
            
            # Create batch group checkpoint
            if use_mongodb and mongo_ops:
                mongo_ops.create_processing_checkpoint(
                    user_id=user_id or "all_users",
                    batch_id=i//max_parallel_batches + 1,
                    total_sms=total_sms,
                    processed_sms=len(all_results)
                )
            
            batch_group = batches[i:i + max_parallel_batches]
            
            # Process batches in parallel
            tasks = []
            for j, batch in enumerate(batch_group):
                batch_id = i + j + 1
                task = process_sms_batch_parallel(
                    batch, batch_id, session, model, mode,
                    temperature, max_tokens, top_p, enrich_mode, pbar, input_path
                )
                tasks.append(task)
            
            # Wait for batch group completion
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # Collect results
            batch_results_collected = []
            batch_failures_collected = []
            
            for result in batch_results:
                if isinstance(result, Exception):
                    print(f"❌ Batch failed: {result}")
                    continue
                
                results, failures = result
                batch_results_collected.extend(results)
                batch_failures_collected.extend(failures)
                all_results.extend(results)
                all_failures.extend(failures)
            
            # Real-time persistence: Update input file or MongoDB with batch progress
            if batch_results_collected or batch_failures_collected:
                if use_mongodb and mongo_ops:
                    # Update MongoDB
                    for sms in batch_results_collected:
                        source_id = sms.get('_source_id')
                        if source_id:
                            mongo_ops.mark_sms_as_processed(source_id, "success")
                    
                    for sms in batch_failures_collected:
                        source_id = sms.get('_source_id')
                        if source_id:
                            mongo_ops.mark_sms_as_processed(source_id, "failed")
                    
                    # Store transactions in MongoDB IMMEDIATELY after each batch
                    if use_mongodb and mongo_ops and batch_results_collected:
                        try:
                            print(f"  🔍 DEBUG: Attempting to store {len(batch_results_collected)} transactions in MongoDB...")
                            print(f"  🔍 DEBUG: MongoDB connection status: {mongo_ops.db is not None}")
                            
                            # Verify MongoDB connection
                            if mongo_ops.db is None:
                                print(f"  ❌ ERROR: MongoDB connection is None!")
                                raise Exception("MongoDB connection is None")
                            
                            stored_count = mongo_ops.store_financial_transactions_batch(batch_results_collected)
                            print(f"  💾 MongoDB: Stored {stored_count} transactions IMMEDIATELY")
                            
                            if stored_count != len(batch_results_collected):
                                print(f"  ⚠️  WARNING: Expected to store {len(batch_results_collected)}, but stored {stored_count}")
                            
                            # Mark SMS as processed IMMEDIATELY after storage
                            success_count = 0
                            for sms in batch_results_collected:
                                source_id = sms.get('_source_id')
                                if source_id:
                                    print(f"  🔍 DEBUG: Marking SMS {source_id} as processed...")
                                    success = mongo_ops.mark_financial_sms_as_processed(source_id, "success")
                                    if success:
                                        success_count += 1
                                        print(f"  ✅ Marked SMS {source_id} as processed IMMEDIATELY")
                                    else:
                                        print(f"  ❌ Failed to mark SMS {source_id} as processed")
                                else:
                                    print(f"  ⚠️  Result missing _source_id: {sms.get('unique_id', 'NO_ID')}")
                            
                            print(f"  ✅ IMMEDIATE Status Update: {success_count}/{len(batch_results_collected)} SMS marked as processed")
                            
                            # Update checkpoint IMMEDIATELY after each batch
                            if user_id:
                                print(f"  🔍 DEBUG: Updating checkpoint for user {user_id}...")
                                checkpoint_updated = mongo_ops.update_processing_checkpoint(
                                    user_id=user_id,
                                    batch_id=1,  # For now, using batch 1
                                    processed_sms=len(all_results),
                                    last_processed_id=batch_results_collected[-1].get('_source_id') if batch_results_collected else None
                                )
                                if checkpoint_updated:
                                    print(f"  💾 Checkpoint updated IMMEDIATELY: {len(all_results)} SMS processed")
                                else:
                                    print(f"  ❌ Failed to update checkpoint")
                            
                        except Exception as e:
                            print(f"  ❌ CRITICAL ERROR in IMMEDIATE storage: {e}")
                            print(f"  🔍 Error type: {type(e).__name__}")
                            import traceback
                            print(f"  🔍 Full traceback: {traceback.format_exc()}")
                            # Continue processing even if storage fails
                    else:
                        print(f"  🔍 DEBUG: Skipping MongoDB storage - use_mongodb: {use_mongodb}, mongo_ops: {mongo_ops is not None}, batch_results: {len(batch_results_collected) if batch_results_collected else 0}")
                        # Update input file
                        update_input_file_progress(input_path, batch_results_collected, batch_failures_collected)
            
            # Real-time file updates: Write results and failures immediately (only if not using MongoDB)
            if not use_mongodb:
                if batch_results_collected:
                    write_results_real_time(output_path, batch_results_collected, mode="append")
                if batch_failures_collected:
                    write_failures_real_time(failures_path, batch_failures_collected, mode="append")
            
            # Update checkpoint with batch completion
            if use_mongodb and mongo_ops:
                mongo_ops.update_processing_checkpoint(
                    user_id=user_id or "all_users",
                    batch_id=i//max_parallel_batches + 1,
                    processed_sms=len(all_results),
                    last_processed_id=batch_results_collected[-1].get('_source_id') if batch_results_collected else None
                )
            
            # Update progress bar postfix
            pbar.set_postfix_str(f"✅{len(all_results)} ❌{len(all_failures)}")
            
            # Longer pause between batch groups
            if i + max_parallel_batches < total_batches:
                await asyncio.sleep(5)
    
        pbar.close()

    # Final cleanup and summary
    if use_mongodb and mongo_ops:
        # Mark all checkpoints as completed
        if user_id:
            mongo_ops.mark_checkpoint_completed(user_id, 1)
        else:
            # Mark all checkpoints for all users
            mongo_ops.db.processing_checkpoints.update_many(
                {"status": "in_progress"},
                {"$set": {"status": "completed", "completion_timestamp": datetime.now()}}
            )
            print(f"✅ Marked all processing checkpoints as completed")
        
        # MongoDB summary
        stats = mongo_ops.get_processing_stats()
        print(f"\n🗄️  MongoDB Processing Summary:")
        print(f"   Database: {mongo_ops.db_name}")
        print(f"   Total SMS: {stats.get('total_sms', 0)}")
        print(f"   Processed SMS: {stats.get('processed_sms', 0)}")
        print(f"   Unprocessed SMS: {stats.get('unprocessed_sms', 0)}")
        print(f"   Total Transactions: {stats.get('total_transactions', 0)}")
        print(f"   Processing Progress: {stats.get('processing_percentage', 0)}%")
        
        # Close MongoDB connection
        mongo_ops.close_connection()
    else:
        # File-based summary
        print(f"\n💾 Final results file: {output_path}")
        print(f"📊 Total results written: {len(all_results)}")
        
        # Cleanup failures file if empty
        if failures_path:
            cleanup_empty_failures_file(failures_path)
    
    # Final summary
    success_rate = (len(all_results) / total_sms) * 100
    print(f"\n📊 PROCESSING SUMMARY:")
    print(f"   Total SMS: {total_sms}")
    print(f"   Successfully Processed: {len(all_results)} ({success_rate:.1f}%)")
    print(f"   Failed: {len(all_failures)} ({100-success_rate:.1f}%)")
    
    if all_results:
        # Intent breakdown
        intent_counts = {}
        for result in all_results:
            intent = result.get("message_intent", "unknown")
            intent_counts[intent] = intent_counts.get(intent, 0) + 1
        
        print(f"\n📋 Message Intent Breakdown:")
        for intent, count in sorted(intent_counts.items()):
            print(f"   {intent.title()}: {count}")
    
    # Print comprehensive performance summary
    performance_monitor.print_performance_summary()
    
    # Print cache performance summary
    cache_stats = intelligent_cache.get_cache_stats()
    if cache_stats["cache_hits"] > 0 or cache_stats["cache_misses"] > 0:
        print(f"\n🎯 CACHE PERFORMANCE SUMMARY:")
        print(f"   💾 Cache size: {cache_stats['cache_size']}/{cache_stats['max_cache_size']}")
        print(f"   🎯 Cache hits: {cache_stats['cache_hits']}")
        print(f"   ❌ Cache misses: {cache_stats['cache_misses']}")
        print(f"   📈 Hit rate: {cache_stats['hit_rate']:.1%}")
        print(f"   🧠 Memory usage: {cache_stats['memory_usage_mb']:.2f} MB")
        
        # Calculate API call savings
        total_requests = cache_stats["cache_hits"] + cache_stats["cache_misses"]
        if total_requests > 0:
            api_savings = cache_stats["cache_hits"] / total_requests * 100
            print(f"   🚀 API calls saved: {api_savings:.1f}%")
    
    # Print error recovery summary
    retry_stats = error_recovery_manager.get_retry_stats()
    if retry_stats["retry_queue_size"] > 0 or retry_stats["dead_letter_queue_size"] > 0:
        print(f"\n🔄 ERROR RECOVERY SUMMARY:")
        print(f"   📋 SMS in retry queue: {retry_stats['retry_queue_size']}")
        print(f"   💀 SMS in dead letter queue: {retry_stats['dead_letter_queue_size']}")
        print(f"   🔄 Total retry attempts: {retry_stats['total_retries']}")
        print(f"   📊 Average retry count: {retry_stats['avg_retry_count']:.1f}")
        
        # Save dead letter queue if any
        if retry_stats["dead_letter_queue_size"] > 0:
            dead_letter_path = f"dead_letter_queue_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            error_recovery_manager.save_dead_letter_queue(dead_letter_path)
            print(f"   💾 Dead letter queue saved to: {dead_letter_path}")
    
    # Event processing summary removed - not needed for production, just clutters codebase

# Intelligent Caching System
class IntelligentCache:
    """Smart caching system to reduce API calls and improve efficiency"""
    
    def __init__(self, max_cache_size=10000, ttl_hours=24):
        self.cache = {}
        self.max_cache_size = max_cache_size
        self.ttl_seconds = ttl_hours * 3600
        self.cache_hits = 0
        self.cache_misses = 0
        self.pattern_recognition = {}
        
    def generate_cache_key(self, sms_data: Dict[str, Any]) -> str:
        """Generate intelligent cache key based on SMS content patterns"""
        # Extract key identifying features
        sender = sms_data.get('sender', '').lower()
        body = sms_data.get('body', '').lower()
        
        # Create pattern-based key
        if 'upi' in body or 'transaction' in body:
            # Financial transaction pattern
            amount_pattern = self._extract_amount_pattern(body)
            transaction_type = self._classify_transaction_type(body)
            key = f"financial_{transaction_type}_{amount_pattern}_{sender}"
        elif 'otp' in body or 'verification' in body:
            # OTP/Verification pattern
            key = f"otp_{sender}"
        elif 'balance' in body or 'account' in body:
            # Balance/Account pattern
            key = f"balance_{sender}"
        else:
            # Generic pattern based on content similarity
            content_hash = self._content_similarity_hash(body)
            key = f"generic_{content_hash}_{sender}"
        
        return key
    
    def _extract_amount_pattern(self, body: str) -> str:
        """Extract amount pattern for caching similar transactions"""
        import re
        
        # Look for amount patterns
        amount_patterns = [
            r'rs\.?\s*(\d+(?:,\d+)*(?:\.\d+)?)',  # Rs. 1,000.00
            r'₹\s*(\d+(?:,\d+)*(?:\.\d+)?)',      # ₹ 1,000.00
            r'(\d+(?:,\d+)*(?:\.\d+)?)\s*rs',     # 1,000.00 Rs
            r'(\d+(?:,\d+)*(?:\.\d+)?)\s*rupees'  # 1,000.00 rupees
        ]
        
        for pattern in amount_patterns:
            match = re.search(pattern, body, re.IGNORECASE)
            if match:
                amount = match.group(1)
                # Categorize amount ranges for better caching
                try:
                    num_amount = float(amount.replace(',', ''))
                    if num_amount < 100:
                        return "small"
                    elif num_amount < 1000:
                        return "medium"
                    elif num_amount < 10000:
                        return "large"
                    else:
                        return "xlarge"
                except ValueError:
                    return "unknown"
        
        return "no_amount"
    
    def _classify_transaction_type(self, body: str) -> str:
        """Classify transaction type for pattern-based caching"""
        body_lower = body.lower()
        
        if any(word in body_lower for word in ['debit', 'deducted', 'spent', 'paid']):
            return "debit"
        elif any(word in body_lower for word in ['credit', 'credited', 'received', 'added']):
            return "credit"
        elif any(word in body_lower for word in ['transfer', 'sent', 'moved']):
            return "transfer"
        else:
            return "unknown"
    
    def _content_similarity_hash(self, body: str) -> str:
        """Generate similarity hash for content-based caching"""
        import hashlib
        
        # Normalize content for better matching
        normalized = re.sub(r'\s+', ' ', body.lower().strip())
        normalized = re.sub(r'[^\w\s]', '', normalized)
        
        # Create hash of normalized content
        return hashlib.md5(normalized.encode()).hexdigest()[:8]
    
    def get_cached_result(self, sms_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Get cached result if available and valid"""
        cache_key = self.generate_cache_key(sms_data)
        
        if cache_key in self.cache:
            cached_item = self.cache[cache_key]
            
            # Check if cache is still valid
            if time.time() - cached_item['timestamp'] < self.ttl_seconds:
                self.cache_hits += 1
                print(f"  🎯 Cache HIT: Using cached result for pattern '{cache_key}'")
                return cached_item['result']
            else:
                # Expired cache entry
                del self.cache[cache_key]
                print(f"  ⏰ Cache EXPIRED: Removing expired entry '{cache_key}'")
        
        self.cache_misses += 1
        return None
    
    def cache_result(self, sms_data: Dict[str, Any], result: Dict[str, Any]):
        """Cache processing result for future use"""
        cache_key = self.generate_cache_key(sms_data)
        
        # Manage cache size
        if len(self.cache) >= self.max_cache_size:
            self._evict_oldest_entries()
        
        # Store result with timestamp
        self.cache[cache_key] = {
            'result': result,
            'timestamp': time.time(),
            'pattern': cache_key
        }
        
        print(f"  💾 Cached result for pattern '{cache_key}'")
    
    def _evict_oldest_entries(self):
        """Evict oldest cache entries when size limit is reached"""
        if not self.cache:
            return
        
        # Sort by timestamp and remove oldest 20%
        sorted_entries = sorted(self.cache.items(), key=lambda x: x[1]['timestamp'])
        entries_to_remove = int(len(sorted_entries) * 0.2)
        
        for i in range(entries_to_remove):
            key = sorted_entries[i][0]
            del self.cache[key]
        
        print(f"  🗑️  Cache cleanup: Removed {entries_to_remove} oldest entries")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """Get cache performance statistics"""
        return {
            "cache_size": len(self.cache),
            "max_cache_size": self.max_cache_size,
            "cache_hits": self.cache_hits,
            "cache_misses": self.cache_misses,
            "hit_rate": self.cache_hits / (self.cache_hits + self.cache_misses) if (self.cache_hits + self.cache_misses) > 0 else 0,
            "memory_usage_mb": len(self.cache) * 0.001  # Rough estimate
        }
    
    def clear_cache(self):
        """Clear all cached data"""
        self.cache.clear()
        self.cache_hits = 0
        self.cache_misses = 0
        print("  🧹 Cache cleared")

# Global intelligent cache
intelligent_cache = IntelligentCache()

# Streaming and Memory Management
class StreamingProcessor:
    """Streaming processor for unlimited dataset handling"""
    
    def __init__(self, chunk_size=1000):
        self.chunk_size = chunk_size
        self.processed_count = 0
        self.memory_usage = 0
        
    async def process_sms_stream(self, input_path: str, output_path: str, 
                          session: aiohttp.ClientSession, model: str, mode: str,
                          temperature: float, max_tokens: int, top_p: float,
                          enrich_mode: str, batch_size: int) -> Dict[str, Any]:
        """Process SMS data in streaming mode for unlimited scalability"""
        
        print(f"🌊 STREAMING MODE: Processing unlimited dataset in chunks of {self.chunk_size}")
        
        # Initialize tracking
        total_processed = 0
        total_success = 0
        total_failures = 0
        chunk_results = []
        
        # Process in chunks to manage memory
        chunk_number = 0
        
        try:
            # Stream process the dataset
            for chunk in self._read_sms_chunks(input_path):
                chunk_number += 1
                print(f"\n📦 Processing Chunk {chunk_number}: {len(chunk)} SMS")
                
                # Process this chunk
                chunk_success, chunk_failures = await self._process_chunk(
                    chunk, chunk_number, session, model, mode,
                    temperature, max_tokens, top_p, enrich_mode, batch_size
                )
                
                # Update totals
                total_processed += len(chunk)
                total_success += chunk_success
                total_failures += chunk_failures
                
                # Save chunk results immediately (streaming persistence)
                chunk_output = f"{output_path}_chunk_{chunk_number}.json"
                self._save_chunk_results(chunk_output, chunk_results)
                
                # Clear chunk results to free memory
                chunk_results.clear()
                
                # Memory management
                self._manage_memory()
                
                print(f"   ✅ Chunk {chunk_number} completed: {chunk_success} success, {chunk_failures} failed")
                print(f"   📊 Running totals: {total_success} success, {total_failures} failed")
                
        except Exception as e:
            print(f"❌ Streaming processing error: {e}")
            # Continue with partial results
        
        # Final summary
        final_results = {
            "total_processed": total_processed,
            "total_success": total_success,
            "total_failures": total_failures,
            "chunks_processed": chunk_number,
            "memory_usage_mb": self.memory_usage / (1024 * 1024)
        }
        
        print(f"\n🌊 STREAMING PROCESSING COMPLETED:")
        print(f"   📦 Total chunks: {chunk_number}")
        print(f"   📱 Total SMS: {total_processed}")
        print(f"   ✅ Success: {total_success}")
        print(f"   ❌ Failures: {total_failures}")
        print(f"   💾 Memory used: {self.memory_usage / (1024 * 1024):.2f} MB")
        
        return final_results
    
    def _read_sms_chunks(self, input_path: str):
        """Read SMS data in chunks to manage memory"""
        try:
            with open(input_path, 'r', encoding='utf-8') as f:
                # Read file line by line for JSONL or parse JSON in chunks
                if input_path.endswith('.ndjson') or input_path.endswith('.jsonl'):
                    # JSONL format - read line by line
                    chunk = []
                    for line_num, line in enumerate(f):
                        if line.strip():
                            try:
                                sms_data = json.loads(line.strip())
                                chunk.append(sms_data)
                                
                                if len(chunk) >= self.chunk_size:
                                    yield chunk
                                    chunk = []
                                    
                            except json.JSONDecodeError:
                                print(f"⚠️  Skipping invalid JSON at line {line_num + 1}")
                                continue
                    
                    # Yield remaining chunk
                    if chunk:
                        yield chunk
                        
                else:
                    # JSON format - load in chunks
                    data = json.load(f)
                    if isinstance(data, list):
                        for i in range(0, len(data), self.chunk_size):
                            yield data[i:i + self.chunk_size]
                    else:
                        # Single object or dict
                        yield [data]
                        
        except Exception as e:
            print(f"❌ Error reading SMS chunks: {e}")
            yield []
    
    async def _process_chunk(self, chunk: List[Dict[str, Any]], chunk_number: int,
                           session: aiohttp.ClientSession, model: str, mode: str,
                           temperature: float, max_tokens: int, top_p: float,
                           enrich_mode: str, batch_size: int) -> tuple:
        """Process a single chunk of SMS data"""
        
        # Create adaptive batches for this chunk
        batches = create_adaptive_batches(chunk, batch_size, rate_limiter)
        
        results = []
        failures = []
        
        # Process batches in parallel
        batch_tasks = []
        for i, batch in enumerate(batches):
            batch_id = f"{chunk_number}_{i+1}"
            task = process_sms_batch_parallel(
                batch, batch_id, session, model, mode,
                temperature, max_tokens, top_p, enrich_mode, 
                None, "streaming_mode"  # No progress bar in streaming mode
            )
            batch_tasks.append(task)
        
        # Execute all batches concurrently
        batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
        
        # Process results
        for result in batch_results:
            if isinstance(result, Exception):
                print(f"  ❌ Batch processing exception: {result}")
                continue
            
            batch_results, batch_failures = result
            results.extend(batch_results)
            failures.extend(batch_failures)
        
        return len(results), len(failures)
    
    def _save_chunk_results(self, output_path: str, results: List[Dict[str, Any]]):
        """Save chunk results immediately for streaming persistence"""
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False, default=str)
            print(f"   💾 Chunk results saved to: {output_path}")
        except Exception as e:
            print(f"   ❌ Failed to save chunk results: {e}")
    
    def _manage_memory(self):
        """Manage memory usage and cleanup"""
        import gc
        
        # Force garbage collection
        gc.collect()
        
        # Track memory usage
        try:
            import psutil
            process = psutil.Process()
            self.memory_usage = process.memory_info().rss
        except ImportError:
            # psutil not available, use basic tracking
            pass
        
        print(f"   🧠 Memory managed, current usage: {self.memory_usage / (1024 * 1024):.2f} MB")

# Global streaming processor
streaming_processor = StreamingProcessor()

def main():
    parser = argparse.ArgumentParser(description="Fixed Optimized SMS Processing")
    parser.add_argument("--input", required=True, help="SMS JSON file")
    parser.add_argument("--output", required=True, help="Output JSON file")
    parser.add_argument("--model", default="qwen3:8b", help="Model name")
    parser.add_argument("--mode", choices=["openai", "generic"], default="openai")
    parser.add_argument("--batch-size", type=int, default=1, help="SMS per batch")
    parser.add_argument("--parallel-batches", type=int, default=2, help="Parallel batches (reduced for stability)")
    parser.add_argument("--temperature", type=float, default=0.1, help="LLM temperature")
    parser.add_argument("--max_tokens", type=int, default=4096, help="Max tokens (optimized for complete responses)")
    parser.add_argument("--top_p", type=float, default=0.9, help="Top-p sampling")
    parser.add_argument("--failures", help="Failures log path (NDJSON)")
    parser.add_argument("--enrich", choices=["off", "safe"], default="safe")
    parser.add_argument("--mongodb", action="store_true", help="Use MongoDB instead of files")
    parser.add_argument("--user-id", help="Process SMS for specific user ID (when using MongoDB)")

    args = parser.parse_args()

    if not API_URL:
        raise SystemExit("❌ Set API_URL environment variable")

    print(f"🚀 Starting FIXED Optimized SMS Processing")
    print(f"   Endpoint: {API_URL}")
    print(f"   Model: {args.model}")
    print(f"   Batch Config: {args.parallel_batches} parallel × {args.batch_size} SMS")
    print(f"   Max Tokens: {args.max_tokens} (optimized)")

    asyncio.run(process_all_batches(
        input_path=args.input,
        output_path=args.output,
        model=args.model,
        mode=args.mode,
        batch_size=args.batch_size,
        max_parallel_batches=args.parallel_batches,
        temperature=args.temperature,
        max_tokens=args.max_tokens,
        top_p=args.top_p,
        failures_path=args.failures,
        enrich_mode=args.enrich,
        use_mongodb=args.mongodb,
        user_id=args.user_id,
    ))

if __name__ == "__main__":
    main()
